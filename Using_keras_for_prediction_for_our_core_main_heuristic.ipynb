{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Using_keras_for_prediction_for_our_core_main_heuristic.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPrZo00iCppBCoh/cx9+MsP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prakface/Practice/blob/master/Using_keras_for_prediction_for_our_core_main_heuristic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pGAg41tkbD3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.regularizers import L1L2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEs_sQBylrbX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "75442abc-5788-45e5-dbbd-be6471d76b4e"
      },
      "source": [
        "#Importing Data\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "#url='https://raw.githubusercontent.com/Prakface/Practice/master/One_mon_present_full.csv'\n",
        "#url2='https://raw.githubusercontent.com/Prakface/Practice/master/Final_one_month_prev_features.csv'\n",
        "\n",
        "\n",
        "#following is path data with user-IDs\n",
        "url='https://raw.githubusercontent.com/Prakface/Practice/master/OneMonth_Present_Final_both_classes.csv'\n",
        "\n",
        "url2='https://raw.githubusercontent.com/Prakface/Practice/master/OneMonthPrevFeatures2_USER_ID.csv'\n",
        "\n",
        "url_h='https://raw.githubusercontent.com/Prakface/Practice/master/heuristic_labels.csv'\n",
        "\n",
        "data = pd.read_csv(url) \n",
        "\n",
        "data2= pd.read_csv(url2)\n",
        "\n",
        "print(\"Data Shape:\", data.shape) \n",
        "\n",
        "#data=pd.read_csv(url)\n",
        "\n",
        "df=pd.DataFrame(data)\n",
        "print(data.head()) \n",
        "\n",
        "\n",
        "data_modified= data.dropna()\n",
        "\n",
        "data_modified.to_csv(\"modifiedData.csv\", index=False)\n",
        "\n",
        "\n",
        "df2=pd.read_csv(\"modifiedData.csv\")\n",
        "\n",
        "print(df2[0:6])\n",
        "\n",
        "print(df2['result'])\n",
        "\n",
        "df_main=df2[df2.columns[~df2.columns.isin(['text', 'user_name'])]]\n",
        "\n",
        "print(df_main.columns)\n",
        "\n",
        "print(len(df_main.columns))\n",
        "\n",
        "  \n",
        "# X_1, y_1 means rpesent tweets' data\n",
        "X_1=df_main[df_main.columns[~df_main.columns.isin(['time', 'tweet_id','result'])]] #removing result attribute as it is class label, hence we get 34 attributes\n",
        "X_1=X_1.iloc[:,1:len(X_1.columns)].values   #removing the unnamed attribute\n",
        "x_1=df_main[df_main.columns[~df_main.columns.isin(['time', 'tweet_id','result'])]]\n",
        "x_1=x_1.iloc[:,1:len(x_1.columns)].values \n",
        "y_1=df_main.loc[:, ['result']].values\n",
        "\n",
        "\n",
        "print(type(X_1), type(y_1), type(x_1), type(y_1))\n",
        "\n",
        "print(X_1.shape)\n",
        "print(y_1.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n the following is for previous data sets\")\n",
        "\n",
        "\n",
        "data2= pd.read_csv(url2)\n",
        "\n",
        "print(\"Data Shape:\", data2.shape) \n",
        "\n",
        "#data=pd.read_csv(url)\n",
        "\n",
        "df_prev=pd.DataFrame(data2)\n",
        "print(data2.head()) \n",
        "\n",
        "\n",
        "data2_modified= data2.dropna()\n",
        "\n",
        "data2_modified.to_csv(\"modifiedData2.csv\", index=False)\n",
        "\n",
        "\n",
        "df_2=pd.read_csv(\"modifiedData2.csv\")\n",
        "\n",
        "print(df_2[0:6])\n",
        "\n",
        "print(df_2['result'])\n",
        "\n",
        "df_main2=df_2[df_2.columns[~df_2.columns.isin(['text', 'user_name'])]]\n",
        "\n",
        "print(df_main2.columns)\n",
        "\n",
        "print(len(df_main2.columns))\n",
        "\n",
        "  \n",
        "\n",
        "X_2=df_main2[df_main2.columns[~df_main2.columns.isin(['time', 'tweet_id','result'])]] #removing result attribute as it is class label, hence we get 34 attributes\n",
        "X_2=X_2.iloc[:,1:len(X_2.columns)].values   #removing the unnamed attribute\n",
        "x_2=df_main2[df_main2.columns[~df_main2.columns.isin(['time', 'tweet_id','result'])]]\n",
        "x_2=x_2.iloc[:,1:len(x_2.columns)].values \n",
        "y_2=df_main2.loc[:, ['result']].values\n",
        "\n",
        "\n",
        "print(type(X_2), type(y_2), type(x_2), type(y_2))\n",
        "\n",
        "print(X_2.shape)\n",
        "print(y_2.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Shape: (1879, 40)\n",
            "     Users_ID cat1  cat10  ...      tweet_id  url      user_name\n",
            "0   205281515    0      0  ...  8.323790e+17  0.0  THEJEROMEOWEN\n",
            "1   571989630    0      0  ...  8.323786e+17  0.0       Acejinjo\n",
            "2   469678667    0      0  ...  8.323780e+17  0.0     RabRakha21\n",
            "3  2858633976    0      0  ...  8.323777e+17  0.0       RS_Aloha\n",
            "4   955893199    0      0  ...  8.323767e+17  0.0  preciselyizzy\n",
            "\n",
            "[5 rows x 40 columns]\n",
            "     Users_ID  cat1  cat10  ...      tweet_id  url        user_name\n",
            "0   205281515     0      0  ...  8.323790e+17  0.0    THEJEROMEOWEN\n",
            "1   571989630     0      0  ...  8.323786e+17  0.0         Acejinjo\n",
            "2   469678667     0      0  ...  8.323780e+17  0.0       RabRakha21\n",
            "3  2858633976     0      0  ...  8.323777e+17  0.0         RS_Aloha\n",
            "4   955893199     0      0  ...  8.323767e+17  0.0    preciselyizzy\n",
            "5   115498371     0      0  ...  8.323759e+17  0.0  thefireistarted\n",
            "\n",
            "[6 rows x 40 columns]\n",
            "0       1.0\n",
            "1       1.0\n",
            "2       1.0\n",
            "3       1.0\n",
            "4       1.0\n",
            "       ... \n",
            "1872    0.0\n",
            "1873    0.0\n",
            "1874    0.0\n",
            "1875    0.0\n",
            "1876    0.0\n",
            "Name: result, Length: 1877, dtype: float64\n",
            "Index(['Users_ID', 'cat1', 'cat10', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6',\n",
            "       'cat7', 'cat8', 'cat9', 'favorite_count', 'hour', 'image', 'level',\n",
            "       'nadj', 'nadv', 'nemoji', 'nlevel', 'nword', 'orginal', 'padj', 'padv',\n",
            "       'pemoji', 'plevel', 'pnoun', 'punc1', 'punc2', 'punc3', 'pword',\n",
            "       'question', 'result', 'retweets_count', 'sarcasm', 'sentiment', 'time',\n",
            "       'tweet_id', 'url'],\n",
            "      dtype='object')\n",
            "38\n",
            "<class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
            "(1877, 34)\n",
            "(1877, 1)\n",
            "\n",
            " the following is for previous data sets\n",
            "Data Shape: (3273, 40)\n",
            "              Users_ID  cat1  cat10  ...             tweet_id  url      user_name\n",
            "0            205281515     0      0  ...  1155575657402961920    1  THEJEROMEOWEN\n",
            "1            205281515     0      0  ...  1155459426243043328    0  THEJEROMEOWEN\n",
            "2  1126967948504125440     0      0  ...  1126969730307448832    0     rabrakha14\n",
            "3           2858633976     0      0  ...  1155277550794338304    0       RS_Aloha\n",
            "4           2858633976     0      0  ...  1155188179395207168    0       RS_Aloha\n",
            "\n",
            "[5 rows x 40 columns]\n",
            "              Users_ID  cat1  cat10  ...             tweet_id  url      user_name\n",
            "0            205281515     0      0  ...  1155575657402961920    1  THEJEROMEOWEN\n",
            "1            205281515     0      0  ...  1155459426243043328    0  THEJEROMEOWEN\n",
            "2  1126967948504125440     0      0  ...  1126969730307448832    0     rabrakha14\n",
            "3           2858633976     0      0  ...  1155277550794338304    0       RS_Aloha\n",
            "4           2858633976     0      0  ...  1155188179395207168    0       RS_Aloha\n",
            "5            955893199     0      0  ...  1154962871765393408    0  preciselyizzy\n",
            "\n",
            "[6 rows x 40 columns]\n",
            "0       0\n",
            "1       0\n",
            "2       0\n",
            "3       0\n",
            "4       0\n",
            "       ..\n",
            "3268    0\n",
            "3269    0\n",
            "3270    0\n",
            "3271    0\n",
            "3272    0\n",
            "Name: result, Length: 3273, dtype: int64\n",
            "Index(['Users_ID', 'cat1', 'cat10', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6',\n",
            "       'cat7', 'cat8', 'cat9', 'favorite_count', 'hour', 'image', 'level',\n",
            "       'nadj', 'nadv', 'nemoji', 'nlevel', 'nword', 'orginal', 'padj', 'padv',\n",
            "       'pemoji', 'plevel', 'pnoun', 'punc1', 'punc2', 'punc3', 'pword',\n",
            "       'question', 'result', 'retweets_count', 'sarcasm', 'sentiment', 'time',\n",
            "       'tweet_id', 'url'],\n",
            "      dtype='object')\n",
            "38\n",
            "<class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
            "(3273, 34)\n",
            "(3273, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqH7-NMZltTQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "d627fd2a-2004-43b1-9108-fc0457dd7f1e"
      },
      "source": [
        "#computing the labels as per our core model (label of previous tweet = present tweet)\n",
        "\n",
        "##Heuristic 1 -> label of the tweet that is less than 2 minutes from present tweet is same as label of present tweet\n",
        "heuristic_main=[]\n",
        "\n",
        "#our_heuristics\n",
        "temp_result=[]\n",
        "users=[]\n",
        "userIDS=[]\n",
        "twee_ids=[]\n",
        "texts=[]\n",
        "times=[]\n",
        "\n",
        " \n",
        "for i in range(len(df_2['Users_ID'])):\n",
        "  userIDS.append(df_2['Users_ID'][i])\n",
        "  users.append(df_2['user_name'])\n",
        "  temp_result.append(df_2['result'][i])\n",
        "  twee_ids.append(df_2['tweet_id'][i])\n",
        "  texts.append(df_2['text'][i])\n",
        "  times.append(df_2['time'][i])\n",
        "  \n",
        "  \n",
        "  \n",
        "heuristic_res1=[]\n",
        "\n",
        "\n",
        "for i in range(len(df_2['Users_ID'])):\n",
        "  heuristic_main.append(0)\n",
        "\n",
        "for i in range(len(df_main['Users_ID'])):\n",
        "  for j in range(len(df_2['Users_ID'])):\n",
        "    check_count=0\n",
        "    #checking for match\n",
        "    if(check_count==2):\n",
        "      continue\n",
        "    elif (check_count<2 and df_main['Users_ID'][i]==userIDS[j]):\n",
        "      t1=df_main['time'][i]\n",
        "      heuristic_main[j]=df_main['result'][i]\n",
        "      '''\n",
        "      check_count+=1\n",
        "      dt_obj=datetime.strptime(t1, '%d-%b-%Y (%H:%M:%S.%f)')\n",
        "      t2=times[j]#df2 ,previous tweets\n",
        "      dt_obj2=datetime.strptime(t2, '%d-%b-%Y (%H:%M:%S.%f)')\n",
        "      \n",
        "      # heuristic condition and populating labels\n",
        "      if(dt_obj.day==dt_obj2.day and dt_obj.month==dt_obj2.month and dt_obj.year==dt_obj2.year):\n",
        "        if(dt_obj.hour==dt_obj2.hour and dt_obj2.minute-dt_obj.minute<=2):\n",
        "          heuristic_main[j]=df_main['result'][i]\n",
        "        elif(abs(dt_obj.hour-dt_obj2.hour)==1 and (dt_obj2.minute-dt_obj.minute)%60<=2):\n",
        "          heuristic_res1_1[j]=df_main['result'][i]\n",
        "        else:\n",
        "          heuristic_res1_1[j]=1-df_main['result'][i]\n",
        "      else:\n",
        "        heuristic_res1_1[j]=1-df_main['result'][i]\n",
        "   \n",
        "    '''   \n",
        "          \n",
        "    \n",
        "print(len(heuristic_main))\n",
        "\n",
        "\n",
        "#print(df_main['result'][900], \"\\n\", heuristic_res1[900:905])\n",
        "\n",
        "\n",
        "c0=0\n",
        "c1=0\n",
        "for i in heuristic_main:\n",
        "  if(i==0):\n",
        "    c0+=1\n",
        "  else:\n",
        "    c1+=1\n",
        "\n",
        "print(\"\\n c0= \",c0,\"\\n c1= \",c1)\n",
        "    \n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3273\n",
            "\n",
            " c0=  1715 \n",
            " c1=  1558\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eRnxq6-lwle",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "eef573b5-fc59-4c42-addd-ee02e7a85dcc"
      },
      "source": [
        "#tem_core_y=np.concatenate((y, heuristic_main),axis=0)\n",
        "print(type(heuristic_main))\n",
        "\n",
        "heuristic_main=np.array(heuristic_main)\n",
        "\n",
        "print(type(heuristic_main))\n",
        "print(heuristic_main.size)\n",
        "\n",
        "df_core = pd.DataFrame(list(heuristic_main), columns =['heuristic_main'])\n",
        "\n",
        "heu_main=df_core.loc[:, ['heuristic_main']].values\n",
        "print(type(heu_main))\n",
        "print(heu_main.size)\n",
        "\n",
        "tem_main_y=np.concatenate((y_1,heu_main), axis=0)\n",
        "print(\"\\n size of tem_main_y = \", tem_main_y.size)\n",
        "#print(y[1:6])\n",
        "#tem_core_y=np.concatenate((y, heuristic_main),axis=0)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "3273\n",
            "<class 'numpy.ndarray'>\n",
            "3273\n",
            "\n",
            " size of tem_main_y =  5150\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WE1rnWQfmCla",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "##Appending the present and previous tweets into one frame (heuristic_main)\n",
        "\n",
        "#Appending the features and labels\n",
        "##Before applying PCA\n",
        "\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "X=np.concatenate((X_1,X_2), axis=0)\n",
        "#y_prev=tem1y\n",
        "\n",
        "#y=np.concatenate((y_1,y_prev), axis=0)\n",
        "\n",
        "#y=tem1y\n",
        "#y=tem2y\n",
        "#y=tem3y\n",
        "#y=tem_1y\n",
        "y=tem_main_y\n",
        "x_tr,x_valid,y_tr,y_valid=train_test_split(X,y,test_size=0.2)\n",
        "\n",
        "\n",
        "## for multi-class (>=2) classification, the shape of the label should be in the form of (N,m), where N=num_of_data points(rows)\n",
        "## m= number of labels\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "y_labels = to_categorical(y_tr)\n",
        "\n",
        "#y_labels2 = to_categorical(y_tr2)\n",
        "\n",
        "\n",
        "y_valid_labels=to_categorical(y_valid)\n",
        "\n",
        "#y_pred_bool = np.argmax(y_pred, axis=1)\n",
        "\n",
        "\n",
        "#y_valid_labels2=to_categorical(y_valid2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flj2XV83mo1g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "778d3ba6-4634-4fe4-9212-8853a0b4b8c8"
      },
      "source": [
        "\n",
        "\n",
        "###Prediction \n",
        "##The y labels are based on heuristic 1\n",
        "\n",
        "x_tr_pres,x_valid_pres,y_tr_pres,y_valid_pres=train_test_split(X_1,y_1,test_size=0.2)\n",
        "x_tr_prev,x_valid_prev,y_tr_prev,y_valid_prev=train_test_split(X_2,heu_main,test_size=0.2)\n",
        "\n",
        "x_tr=np.concatenate((x_tr_pres,x_tr_prev), axis=0)\n",
        "x_valid=np.concatenate((x_valid_pres, x_valid_prev), axis=0)\n",
        "\n",
        "y_tr=np.concatenate((y_tr_pres,y_tr_prev), axis=0)\n",
        "y_valid=np.concatenate((y_valid_pres, y_valid_prev), axis=0)\n",
        "\n",
        "\n",
        "###Using X_1_new and X_2_new, for generating user IDs based \n",
        "##TO DEMONSTRATE THAT IT \n",
        "X_1_new=df_main[df_main.columns[~df_main.columns.isin(['time', 'tweet_id','result'])]]\n",
        "\n",
        "X_2_new=df_main2[df_main2.columns[~df_main2.columns.isin(['time', 'tweet_id','result'])]]\n",
        "\n",
        "\n",
        "\n",
        "##Appending the present and previous tweets into one frame (prev labels = all are 0s)\n",
        "\n",
        "#Appending the features and labels\n",
        "##Before applying PCA\n",
        "\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "X_new=np.concatenate((X_1_new,X_2_new), axis=0)\n",
        "y_new=np.concatenate((y_1,y_2), axis=0)\n",
        "\n",
        "\n",
        "x_pres_tr_tmp,x_pres_valid_tmp,y_pres_tr_tmp,y_pres_valid_tmp=train_test_split(X_1_new,y_1,test_size=0.2) \n",
        "\n",
        "print(type(x_pres_tr_tmp), type(x_pres_valid_tmp))\n",
        "\n",
        "#To Convert into numpy arrays\n",
        "\n",
        "x_pres_tr_new=x_pres_tr_tmp.values\n",
        "x_pres_valid_new=x_pres_valid_tmp.values\n",
        "\n",
        "x_pres_tmp_lst=list(x_pres_tr_new)\n",
        "x_pres_valid_tmp=list(x_pres_valid_new)\n",
        "\n",
        "print(type(x_pres_tmp_lst), type(x_pres_tmp_lst))\n",
        "\n",
        "\n",
        "X_pres_tr_df=pd.DataFrame(x_pres_tmp_lst, columns=df_main.columns[~df_main.columns.isin(['time', 'tweet_id','result'])])\n",
        "X_pres_valid_df=pd.DataFrame(x_pres_valid_tmp, columns= df_main.columns[~df_main.columns.isin(['time', 'tweet_id','result'])])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#X_prev_df=pd.DataFrame(x_prev_tmp_lst, columns=df_main2.columns[~df_main2.columns.isin(['time', 'tweet_id','result'])])\n",
        "#X_prev_valid_df=pd.DataFrame(x_prev_valid_tmp, columns= df_main2.columns[~df_main2.columns.isin(['time', 'tweet_id','result'])])\n",
        "\n",
        "#For each ID in present training list select a tuple from previous tweets, that has matching Users_ID\n",
        "\n",
        "matching_prev_twts_tr=[]\n",
        "matching_prev_twts_valid=[]\n",
        "\n",
        "matching_y_prev_tr=[]\n",
        "matching_y_prev_valid=[]\n",
        "\n",
        "match_cnt=0\n",
        "index_cnt=0\n",
        "\n",
        "for i in X_pres_tr_df['Users_ID']:\n",
        "  match_cnt=0\n",
        "  index_cnt=0\n",
        "  for j in X_2_new['Users_ID']:\n",
        "    if(match_cnt>0):\n",
        "      break\n",
        "    elif(i==j and match_cnt==0):\n",
        "      matching_prev_twts_tr.append(X_2_new.iloc[index_cnt, :])  #copying row corresponding to current  index (with all columns)\n",
        "\n",
        "      matching_y_prev_tr.append(y_2[index_cnt])\n",
        "\n",
        "      match_cnt+=1\n",
        "      \n",
        "      index_cnt+=1\n",
        "    else:\n",
        "      index_cnt+=1\n",
        "      #print(\"\\n--different path--\\n\")\n",
        "\n",
        "\n",
        "\n",
        "for i in X_pres_valid_df['Users_ID']:\n",
        "  match_cnt=0\n",
        "  index_cnt=0\n",
        "  for j in X_2_new['Users_ID']:\n",
        "    if(match_cnt>0):\n",
        "      break\n",
        "    elif(i==j and match_cnt==0):\n",
        "      matching_prev_twts_valid.append(X_2_new.iloc[index_cnt, :])\n",
        "\n",
        "      matching_y_prev_valid.append(y_2[index_cnt])\n",
        "\n",
        "\n",
        "      match_cnt+=1\n",
        "      index_cnt+=1\n",
        "    else:\n",
        "      index_cnt+=1\n",
        "      #print(\"\\n--different path--\\n\")\n",
        "\n",
        "\n",
        "print(\"\\n type of matching tweets\", type(matching_prev_twts_tr))\n",
        "print(\"\\n the lengths of present tweets training = \", len(X_pres_tr_df), \"\\n length of matching previous tweets for training=\", len(matching_prev_twts_tr))\n",
        "\n",
        "print(\"\\n the lengths of present tweets testing = \", len(X_pres_valid_df), \"\\n length of matching previous tweets for testing=\", len(matching_prev_twts_valid))\n",
        "\n",
        "print(\"\\n lengths of matched previous labels training = \", len(matching_y_prev_tr),\"\\n lengths of matched previous labels validation = \", len(matching_y_prev_valid))\n",
        "\n",
        "\n",
        "#x_tr,x_valid,y_tr,y_valid=train_test_split(X,y,test_size=0.2)\n",
        "\n",
        "#for i in x_pres_tr_new['Users_ID']\n",
        "\n",
        "\n",
        "#X_pres_tmp\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#creating data frames from matching lists.\n",
        "X_prev_tr_df=pd.DataFrame(matching_prev_twts_tr, columns=df_main2.columns[~df_main2.columns.isin(['time', 'tweet_id','result'])])\n",
        "\n",
        "X_prev_valid_df=pd.DataFrame(matching_prev_twts_valid, columns= df_main2.columns[~df_main2.columns.isin(['time', 'tweet_id','result'])])\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'> <class 'pandas.core.frame.DataFrame'>\n",
            "<class 'list'> <class 'list'>\n",
            "\n",
            " type of matching tweets <class 'list'>\n",
            "\n",
            " the lengths of present tweets training =  1501 \n",
            " length of matching previous tweets for training= 1306\n",
            "\n",
            " the lengths of present tweets testing =  376 \n",
            " length of matching previous tweets for testing= 328\n",
            "\n",
            " lengths of matched previous labels training =  1306 \n",
            " lengths of matched previous labels validation =  328\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipYlI8uznWHi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "a151aae1-1dae-4476-99e9-6405eacb2761"
      },
      "source": [
        "## we have less previous tweets compared to present tweets in matched points, \n",
        "## Using the matched previous tweets, Select the present tweets in trianing, which have matching IDs with matched previous tweets (so as to make both present and previous tweets equal in num)\n",
        "\n",
        "\n",
        "##Create temporary dataframes or lists.\n",
        "print(type(X_2_new.iloc[2,:]))\n",
        "\n",
        "min_match_rows_appended=[]\n",
        "min_match_cols_appended=[]\n",
        "\n",
        "min_matched_pres_tr=[]\n",
        "min_matched_pres_valid=[]\n",
        "\n",
        "min_y_prev_tr=[]\n",
        "min_y_prev_valid=[]\n",
        "\n",
        "'''\n",
        "\n",
        "final_min_matched_prev_tr=[]\n",
        "final_min_matched_prev_valid=[]\n",
        "\n",
        "prev_index=0\n",
        "for i in X_prev_tr_df['Users_ID']:\n",
        "  pres_index=0\n",
        "  for j in X_pres_tr_df['Users_ID']:\n",
        "    if(i==j):\n",
        "      final_min_matched_prev.append()\n",
        "      #min_match_rows_appended.append(X_pres_tr_df.iloc[])\n",
        "    else:\n",
        "      prev_index+=1\n",
        "  pres_index+=1\n",
        "'''\n",
        "\n",
        "'''\n",
        "match_flag=0\n",
        "\n",
        "for i in X_pres_tr_df['Users_ID']:\n",
        "  index=0\n",
        "  match_flag=0\n",
        "  for j in X_prev_tr_df['Users_ID']:\n",
        "    if(i==j and match_flag==0):\n",
        "      final_min_matched_prev_tr.append(X_prev_tr_df.iloc[index, :])\n",
        "      match_flag=1\n",
        "      #min_match_rows_appended.append(X_pres_tr_df.iloc[])\n",
        "    else:\n",
        "      index+=1\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "match_flag=0\n",
        "for i in X_pres_valid_df['Users_ID']:\n",
        "  index=0\n",
        "  match_flag=0\n",
        "  for j in X_prev_valid_df['Users_ID']:\n",
        "    if(i==j):\n",
        "      final_min_matched_prev_valid.append(X_prev_valid_df.iloc[index, :])\n",
        "      match_flag=1\n",
        "      #min_match_rows_appended.append(X_pres_valid_df.iloc[])\n",
        "    else:\n",
        "      index+=1\n",
        "\n",
        "\n",
        "This above  logic is not good as it is selects the previous matched tweets again.\n",
        "      \n",
        "           \n",
        "      \n",
        "                      \n",
        "print(\"\\n the min matched lengths are for present training = \\n\",len(X_pres_tr_df) ,\"\\n for previous training =\", len(final_min_matched_prev_tr))\n",
        "\n",
        "'''\n",
        "\n",
        "#Using Break \n",
        "\n",
        "for i in X_prev_tr_df['Users_ID']:\n",
        "  index=0\n",
        "  match_flag=0\n",
        "  for j in X_pres_tr_df['Users_ID']:\n",
        "    if(i==j):\n",
        "      min_matched_pres_tr.append(X_pres_tr_df.iloc[index, :])\n",
        "      break\n",
        "      #print(\"train\")\n",
        "      \n",
        "      #min_match_rows_appended.append(X_pres_tr_df.iloc[])\n",
        "    #else:\n",
        "    #  index+=1\n",
        "    index+=1\n",
        "\n",
        "for i in X_prev_valid_df['Users_ID']:\n",
        "  index=0\n",
        "  match_flag=0\n",
        "  for j in X_pres_valid_df['Users_ID']:\n",
        "    if(i==j):\n",
        "      min_matched_pres_valid.append(X_pres_valid_df.iloc[index, :])\n",
        "      break\n",
        "      \n",
        "      #min_match_rows_appended.append(X_pres_tr_df.iloc[])\n",
        "    #else:\n",
        "    #  index+=1\n",
        "    index+=1\n",
        "\n",
        "\n",
        "print(\"\\n the min matched lengths are for previous training = \\n\",len(X_prev_tr_df) ,\"\\n for new present training =\", len(min_matched_pres_tr))\n",
        "\n",
        "##In other way, using flags instead of break\n",
        "min_matched_pres_tr=[]\n",
        "min_matched_pres_valid=[]\n",
        "\n",
        "min_y_pres_tr=[]\n",
        "min_y_pres_valid=[]\n",
        "\n",
        "prev_index=0\n",
        "for i in X_prev_tr_df['Users_ID']:\n",
        "  index=0\n",
        "  match_flag=0\n",
        "  for j in X_pres_tr_df['Users_ID']:\n",
        "    if(i==j and match_flag==0):\n",
        "      min_matched_pres_tr.append(X_pres_tr_df.iloc[prev_index, :])\n",
        "      #print(\"train\")\n",
        "      match_flag=1\n",
        "      min_y_pres_tr.append(y_pres_tr_tmp[index])\n",
        "      #min_match_rows_appended.append(X_pres_tr_df.iloc[])\n",
        "    else:\n",
        "      index+=1\n",
        "  \n",
        "  prev_index+=1\n",
        "\n",
        "\n",
        "prev_index=0\n",
        "for i in X_prev_valid_df['Users_ID']:\n",
        "  index=0\n",
        "  match_flag=0\n",
        "  for j in X_pres_valid_df['Users_ID']:\n",
        "    if(i==j and match_flag==0 ):\n",
        "      min_matched_pres_valid.append(X_pres_valid_df.iloc[index, :])\n",
        "      \n",
        "      min_y_pres_valid.append(y_pres_valid_tmp[index])\n",
        "      match_flag=1\n",
        "      #min_match_rows_appended.append(X_pres_tr_df.iloc[])\n",
        "    else:\n",
        "      index+=1\n",
        "  prev_index+=1\n",
        "\n",
        "\n",
        "print(\"\\n the min matched lengths are for previous training = \",len(X_prev_tr_df) ,\"\\n for new present training =\", len(min_matched_pres_tr))\n",
        "print(\"\\n the min match lengths for labels of previous training = \", len(matching_y_prev_tr), \"\\n the min match lengths for labels for present training = \", len(min_y_pres_tr))\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n the min matched lengths are for previous validation = \",len(X_prev_valid_df) ,\"\\n for new present validation =\", len(min_matched_pres_valid))\n",
        "print(\"\\n the min match lengths for labels of previous validation = \", len(matching_y_prev_valid), \"\\n the min match lengths for labels for present validation = \", len(min_y_pres_valid))\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.series.Series'>\n",
            "\n",
            " the min matched lengths are for previous training = \n",
            " 1306 \n",
            " for new present training = 1306\n",
            "\n",
            " the min matched lengths are for previous training =  1306 \n",
            " for new present training = 1306\n",
            "\n",
            " the min match lengths for labels of previous training =  1306 \n",
            " the min match lengths for labels for present training =  1306\n",
            "\n",
            " the min matched lengths are for previous validation =  328 \n",
            " for new present validation = 328\n",
            "\n",
            " the min match lengths for labels of previous validation =  328 \n",
            " the min match lengths for labels for present validation =  328\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOyLv6OEnbxQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "9137ac5b-4726-4b1b-a870-0f660961cc97"
      },
      "source": [
        "min_pres_tr_df=pd.DataFrame(min_matched_pres_tr, columns=df_main2.columns[~df_main2.columns.isin(['time', 'tweet_id','result'])])\n",
        "\n",
        "min_pres_valid_df=pd.DataFrame(min_matched_pres_valid, columns=df_main2.columns[~df_main2.columns.isin(['time', 'tweet_id','result'])])\n",
        "\n",
        "#min_pres_tr_df, min_pres_valid_df, X_prev_tr_df, X_prev_valid_df are our train and test splits for presenr and previous  tweets\n",
        "\n",
        "#NOW REMOVE the UserIDs columns and so as to make them usable for training the model\n",
        "\n",
        "new_min_pres_tr_df=pd.DataFrame(min_pres_tr_df, columns=min_pres_tr_df.columns[~min_pres_tr_df.columns.isin(['Users_ID'])])\n",
        "\n",
        "new_min_pres_valid_df=pd.DataFrame(min_pres_valid_df, columns=min_pres_valid_df.columns[~min_pres_valid_df.columns.isin(['Users_ID'])])\n",
        "\n",
        "new_X_prev_tr_df=pd.DataFrame(X_prev_tr_df, columns= X_prev_tr_df.columns[~X_prev_tr_df.columns.isin(['Users_ID'])])\n",
        "\n",
        "new_X_prev_valid_df=pd.DataFrame(X_prev_valid_df, columns= X_prev_valid_df.columns[~X_prev_valid_df.columns.isin(['Users_ID'])])\n",
        "\n",
        "\n",
        "#\n",
        "new_X_pres_tr= new_min_pres_tr_df.iloc[:,:].values\n",
        "new_X_pres_valid= new_min_pres_valid_df.iloc[:,:].values\n",
        "new_X_prev_tr= new_X_prev_tr_df.iloc[:,:].values\n",
        "new_X_prev_valid= new_X_prev_valid_df.iloc[:,:].values\n",
        "\n",
        "\n",
        "\n",
        "new_y_pres_tr=np.array(min_y_pres_tr)\n",
        "new_y_prev_tr=np.array(matching_y_prev_tr)\n",
        "new_y_pres_valid=np.array(min_y_pres_valid)\n",
        "new_y_prev_valid=np.array(matching_y_prev_valid)\n",
        "\n",
        "print(\"\\n shapes of training set of labels for present and previous  \",new_y_pres_tr.shape, new_y_prev_tr.shape)\n",
        "print(\"\\n shapes of validation set of labels for present and previous  \", new_y_pres_valid.shape, new_y_prev_valid.shape)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "new_X_tr=np.concatenate((new_X_pres_tr, new_X_prev_tr), axis=0)\n",
        "new_X_valid=np.concatenate((new_X_pres_valid, new_X_prev_valid), axis=0)\n",
        "\n",
        "new_y_tr=np.concatenate((new_y_pres_tr, new_y_prev_tr), axis=0)\n",
        "new_y_valid=np.concatenate((new_y_pres_valid, new_y_prev_valid), axis=0)\n",
        "\n",
        "#convert into ndarray and build the model\n",
        "from keras.utils import to_categorical\n",
        "y_tr_labels = to_categorical(new_y_tr)\n",
        "\n",
        "#y_labels2 = to_categorical(y_tr2)\n",
        "\n",
        "\n",
        "y_valid_labels=to_categorical(new_y_valid)\n",
        "\n",
        "\n",
        "\n",
        "y_pres_tr_labels = to_categorical(new_y_pres_tr)\n",
        "y_pres_valid_labels=to_categorical(new_y_pres_valid)\n",
        "\n",
        "y_prev_tr_labels = to_categorical(new_y_prev_tr)\n",
        "y_prev_valid_labels=to_categorical(new_y_prev_valid)\n",
        "\n",
        "\n",
        "#y_pred_bool = np.argmax(y_pred, axis=1)\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n shapes of training set of labels for present and previous  \",y_pres_tr_labels.shape, y_prev_tr_labels.shape)\n",
        "print(\"\\n shapes of validation set of labels for present and previous  \", y_pres_valid_labels.shape, y_prev_valid_labels.shape)\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " shapes of training set of labels for present and previous   (1306, 1) (1306, 1)\n",
            "\n",
            " shapes of validation set of labels for present and previous   (328, 1) (328, 1)\n",
            "\n",
            " shapes of training set of labels for present and previous   (1306, 2) (1306, 1)\n",
            "\n",
            " shapes of validation set of labels for present and previous   (328, 2) (328, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgsN3fg4ngkE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "de3859d6-2b9c-4d9a-d8e8-8547c443a8dc"
      },
      "source": [
        "print(len(new_X_tr),  len(y_tr_labels), len(new_X_valid), len(y_valid_labels))\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2612 2612 656 656\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ud6UpJ_6njLt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ced0503e-5ac9-4b07-eaa8-a5d3b9d2afde"
      },
      "source": [
        "#Training the model with this ORCHESTRATED DATASET,\n",
        "##ONLY normal way of prediction\n",
        "##With regularization and with precision and accuracy scores, classification report.and plots\n",
        "import matplotlib\n",
        "from matplotlib import pyplot\n",
        "\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.regularizers import L1L2\n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import classification_report\n",
        "\"\"\"\n",
        "Now, we Set up the logistic regression model with y_labels1 and  y_valid_labels1, the changed labels\n",
        "\"\"\"\n",
        "model = Sequential()\n",
        "feature_vector=new_X_tr[1] #(any row of training data set)\n",
        "model.add(Dense(2,  # output dim is 2, one score per each class\n",
        "                activation='softmax',\n",
        "                kernel_regularizer=L1L2(l1=0.0, l2=0.1),\n",
        "                input_dim=len(feature_vector)))  # input dimension = number of features your data has\n",
        "          \n",
        "\n",
        "model.compile(optimizer='sgd',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "history=model.fit(new_X_tr, y_tr_labels, epochs=100, validation_data=(new_X_valid, y_valid_labels))\n",
        "\n",
        "\n",
        "y_pred = model.predict(new_X_valid, batch_size=64, verbose=1)\n",
        "y_pred_bool = np.argmax(y_pred, axis=1)\n",
        "\n",
        "print(\"\\n Classification Report, \\n\", classification_report(new_y_valid, y_pred_bool), \"\\n\")\n",
        "print(\"\\n Macro scores = \\n\")\n",
        "print(precision_recall_fscore_support(new_y_valid, y_pred_bool, average='macro')) #computes accuracy for each label and then averages them, Doesn't take class imbalance into account\n",
        "\n",
        "print(\"\\n Micro Scores = \\n\")\n",
        "print(precision_recall_fscore_support(new_y_valid, y_pred_bool, average='micro')) #computes accuracy at global level, not for each label.\n",
        "\n",
        "print(\"\\n weighted Scores = \\n\")\n",
        "print(precision_recall_fscore_support(new_y_valid, y_pred_bool, average='weighted'))#computes accuracy for each label and then does weighted average of them, considers support of each label\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "pyplot.plot(history.history['acc'], label='train')\n",
        "pyplot.plot(history.history['val_acc'], label='test')\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 2612 samples, validate on 656 samples\n",
            "Epoch 1/100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "2612/2612 [==============================] - 1s 238us/step - loss: 1.0249 - acc: 0.6784 - val_loss: 0.9030 - val_acc: 0.6814\n",
            "Epoch 2/100\n",
            "2612/2612 [==============================] - 0s 41us/step - loss: 0.7999 - acc: 0.7121 - val_loss: 0.8629 - val_acc: 0.6387\n",
            "Epoch 3/100\n",
            "2612/2612 [==============================] - 0s 42us/step - loss: 0.7137 - acc: 0.7316 - val_loss: 0.6792 - val_acc: 0.7393\n",
            "Epoch 4/100\n",
            "2612/2612 [==============================] - 0s 41us/step - loss: 0.6495 - acc: 0.7423 - val_loss: 0.6339 - val_acc: 0.7424\n",
            "Epoch 5/100\n",
            "2612/2612 [==============================] - 0s 43us/step - loss: 0.6093 - acc: 0.7450 - val_loss: 0.5937 - val_acc: 0.7546\n",
            "Epoch 6/100\n",
            "2612/2612 [==============================] - 0s 41us/step - loss: 0.5860 - acc: 0.7462 - val_loss: 0.6215 - val_acc: 0.7424\n",
            "Epoch 7/100\n",
            "2612/2612 [==============================] - 0s 39us/step - loss: 0.5654 - acc: 0.7454 - val_loss: 0.5649 - val_acc: 0.7439\n",
            "Epoch 8/100\n",
            "2612/2612 [==============================] - 0s 41us/step - loss: 0.5515 - acc: 0.7515 - val_loss: 0.5476 - val_acc: 0.7485\n",
            "Epoch 9/100\n",
            "2612/2612 [==============================] - 0s 41us/step - loss: 0.5353 - acc: 0.7485 - val_loss: 0.6092 - val_acc: 0.7439\n",
            "Epoch 10/100\n",
            "2612/2612 [==============================] - 0s 38us/step - loss: 0.5382 - acc: 0.7534 - val_loss: 0.5755 - val_acc: 0.7454\n",
            "Epoch 11/100\n",
            "2612/2612 [==============================] - 0s 38us/step - loss: 0.5298 - acc: 0.7466 - val_loss: 0.6757 - val_acc: 0.7424\n",
            "Epoch 12/100\n",
            "2612/2612 [==============================] - 0s 37us/step - loss: 0.5550 - acc: 0.7477 - val_loss: 0.5428 - val_acc: 0.7454\n",
            "Epoch 13/100\n",
            "2612/2612 [==============================] - 0s 41us/step - loss: 0.5231 - acc: 0.7523 - val_loss: 0.5794 - val_acc: 0.7409\n",
            "Epoch 14/100\n",
            "2612/2612 [==============================] - 0s 39us/step - loss: 0.5251 - acc: 0.7515 - val_loss: 0.5207 - val_acc: 0.7485\n",
            "Epoch 15/100\n",
            "2612/2612 [==============================] - 0s 39us/step - loss: 0.5337 - acc: 0.7504 - val_loss: 0.6095 - val_acc: 0.7424\n",
            "Epoch 16/100\n",
            "2612/2612 [==============================] - 0s 40us/step - loss: 0.5398 - acc: 0.7416 - val_loss: 0.5471 - val_acc: 0.7454\n",
            "Epoch 17/100\n",
            "2612/2612 [==============================] - 0s 40us/step - loss: 0.5366 - acc: 0.7492 - val_loss: 0.6289 - val_acc: 0.7424\n",
            "Epoch 18/100\n",
            "2612/2612 [==============================] - 0s 40us/step - loss: 0.5318 - acc: 0.7462 - val_loss: 0.5234 - val_acc: 0.7470\n",
            "Epoch 19/100\n",
            "2612/2612 [==============================] - 0s 38us/step - loss: 0.5186 - acc: 0.7473 - val_loss: 0.5540 - val_acc: 0.7470\n",
            "Epoch 20/100\n",
            "2612/2612 [==============================] - 0s 39us/step - loss: 0.5099 - acc: 0.7496 - val_loss: 0.5157 - val_acc: 0.7500\n",
            "Epoch 21/100\n",
            "2612/2612 [==============================] - 0s 39us/step - loss: 0.5325 - acc: 0.7492 - val_loss: 0.6518 - val_acc: 0.7424\n",
            "Epoch 22/100\n",
            "2612/2612 [==============================] - 0s 39us/step - loss: 0.5324 - acc: 0.7443 - val_loss: 0.5137 - val_acc: 0.7470\n",
            "Epoch 23/100\n",
            "2612/2612 [==============================] - 0s 41us/step - loss: 0.5056 - acc: 0.7481 - val_loss: 0.5173 - val_acc: 0.7622\n",
            "Epoch 24/100\n",
            "2612/2612 [==============================] - 0s 38us/step - loss: 0.5073 - acc: 0.7485 - val_loss: 0.5312 - val_acc: 0.7454\n",
            "Epoch 25/100\n",
            "2612/2612 [==============================] - 0s 40us/step - loss: 0.5090 - acc: 0.7508 - val_loss: 0.5142 - val_acc: 0.7500\n",
            "Epoch 26/100\n",
            "2612/2612 [==============================] - 0s 40us/step - loss: 0.5185 - acc: 0.7469 - val_loss: 0.5170 - val_acc: 0.7500\n",
            "Epoch 27/100\n",
            "2612/2612 [==============================] - 0s 40us/step - loss: 0.5134 - acc: 0.7492 - val_loss: 0.5103 - val_acc: 0.7530\n",
            "Epoch 28/100\n",
            "2612/2612 [==============================] - 0s 46us/step - loss: 0.7942 - acc: 0.7152 - val_loss: 0.5467 - val_acc: 0.7607\n",
            "Epoch 29/100\n",
            "2612/2612 [==============================] - 0s 38us/step - loss: 0.5182 - acc: 0.7485 - val_loss: 0.5172 - val_acc: 0.7485\n",
            "Epoch 30/100\n",
            "2612/2612 [==============================] - 0s 38us/step - loss: 0.5409 - acc: 0.7481 - val_loss: 0.5276 - val_acc: 0.7530\n",
            "Epoch 31/100\n",
            "2612/2612 [==============================] - 0s 48us/step - loss: 0.5072 - acc: 0.7469 - val_loss: 0.5395 - val_acc: 0.7515\n",
            "Epoch 32/100\n",
            "2612/2612 [==============================] - 0s 38us/step - loss: 0.5071 - acc: 0.7450 - val_loss: 0.5071 - val_acc: 0.7530\n",
            "Epoch 33/100\n",
            "2612/2612 [==============================] - 0s 42us/step - loss: 0.5129 - acc: 0.7492 - val_loss: 0.5257 - val_acc: 0.7652\n",
            "Epoch 34/100\n",
            "2612/2612 [==============================] - 0s 37us/step - loss: 0.5172 - acc: 0.7458 - val_loss: 0.5289 - val_acc: 0.7454\n",
            "Epoch 35/100\n",
            "2612/2612 [==============================] - 0s 39us/step - loss: 0.5029 - acc: 0.7492 - val_loss: 0.5886 - val_acc: 0.7088\n",
            "Epoch 36/100\n",
            "2612/2612 [==============================] - 0s 40us/step - loss: 0.5148 - acc: 0.7511 - val_loss: 0.5192 - val_acc: 0.7713\n",
            "Epoch 37/100\n",
            "2612/2612 [==============================] - 0s 38us/step - loss: 0.5273 - acc: 0.7446 - val_loss: 0.5615 - val_acc: 0.7454\n",
            "Epoch 38/100\n",
            "2612/2612 [==============================] - 0s 40us/step - loss: 0.5265 - acc: 0.7531 - val_loss: 0.5296 - val_acc: 0.7454\n",
            "Epoch 39/100\n",
            "2612/2612 [==============================] - 0s 41us/step - loss: 0.5378 - acc: 0.7481 - val_loss: 0.5397 - val_acc: 0.7576\n",
            "Epoch 40/100\n",
            "2612/2612 [==============================] - 0s 37us/step - loss: 0.5096 - acc: 0.7519 - val_loss: 0.5192 - val_acc: 0.7576\n",
            "Epoch 41/100\n",
            "2612/2612 [==============================] - 0s 38us/step - loss: 0.5507 - acc: 0.7377 - val_loss: 0.5274 - val_acc: 0.7576\n",
            "Epoch 42/100\n",
            "2612/2612 [==============================] - 0s 39us/step - loss: 0.5105 - acc: 0.7454 - val_loss: 0.5390 - val_acc: 0.7454\n",
            "Epoch 43/100\n",
            "2612/2612 [==============================] - 0s 37us/step - loss: 0.5357 - acc: 0.7431 - val_loss: 0.5449 - val_acc: 0.7485\n",
            "Epoch 44/100\n",
            "2612/2612 [==============================] - 0s 40us/step - loss: 0.5132 - acc: 0.7450 - val_loss: 0.5046 - val_acc: 0.7561\n",
            "Epoch 45/100\n",
            "2612/2612 [==============================] - 0s 39us/step - loss: 0.5328 - acc: 0.7473 - val_loss: 0.5653 - val_acc: 0.7470\n",
            "Epoch 46/100\n",
            "2612/2612 [==============================] - 0s 41us/step - loss: 0.5123 - acc: 0.7458 - val_loss: 0.5078 - val_acc: 0.7607\n",
            "Epoch 47/100\n",
            "2612/2612 [==============================] - 0s 38us/step - loss: 0.5651 - acc: 0.7469 - val_loss: 0.5489 - val_acc: 0.7454\n",
            "Epoch 48/100\n",
            "2612/2612 [==============================] - 0s 38us/step - loss: 0.5141 - acc: 0.7504 - val_loss: 0.5310 - val_acc: 0.7637\n",
            "Epoch 49/100\n",
            "2612/2612 [==============================] - 0s 37us/step - loss: 0.5068 - acc: 0.7466 - val_loss: 0.5181 - val_acc: 0.7500\n",
            "Epoch 50/100\n",
            "2612/2612 [==============================] - 0s 41us/step - loss: 0.5014 - acc: 0.7523 - val_loss: 0.5056 - val_acc: 0.7652\n",
            "Epoch 51/100\n",
            "2612/2612 [==============================] - 0s 40us/step - loss: 0.5155 - acc: 0.7439 - val_loss: 0.5717 - val_acc: 0.7439\n",
            "Epoch 52/100\n",
            "2612/2612 [==============================] - 0s 38us/step - loss: 0.5075 - acc: 0.7511 - val_loss: 0.5782 - val_acc: 0.6921\n",
            "Epoch 53/100\n",
            "2612/2612 [==============================] - 0s 39us/step - loss: 0.5013 - acc: 0.7508 - val_loss: 0.5569 - val_acc: 0.7470\n",
            "Epoch 54/100\n",
            "2612/2612 [==============================] - 0s 43us/step - loss: 0.4963 - acc: 0.7534 - val_loss: 0.5720 - val_acc: 0.7530\n",
            "Epoch 55/100\n",
            "2612/2612 [==============================] - 0s 41us/step - loss: 0.5159 - acc: 0.7500 - val_loss: 0.5077 - val_acc: 0.7530\n",
            "Epoch 56/100\n",
            "2612/2612 [==============================] - 0s 43us/step - loss: 0.6553 - acc: 0.7274 - val_loss: 0.8210 - val_acc: 0.7088\n",
            "Epoch 57/100\n",
            "2612/2612 [==============================] - 0s 38us/step - loss: 0.6272 - acc: 0.7324 - val_loss: 0.5022 - val_acc: 0.7530\n",
            "Epoch 58/100\n",
            "2612/2612 [==============================] - 0s 39us/step - loss: 0.5085 - acc: 0.7504 - val_loss: 0.5090 - val_acc: 0.7485\n",
            "Epoch 59/100\n",
            "2612/2612 [==============================] - 0s 44us/step - loss: 0.5769 - acc: 0.7397 - val_loss: 0.6580 - val_acc: 0.7363\n",
            "Epoch 60/100\n",
            "2612/2612 [==============================] - 0s 38us/step - loss: 0.5209 - acc: 0.7500 - val_loss: 0.5567 - val_acc: 0.7378\n",
            "Epoch 61/100\n",
            "2612/2612 [==============================] - 0s 37us/step - loss: 0.5101 - acc: 0.7554 - val_loss: 0.5611 - val_acc: 0.7607\n",
            "Epoch 62/100\n",
            "2612/2612 [==============================] - 0s 38us/step - loss: 0.7194 - acc: 0.7209 - val_loss: 0.7970 - val_acc: 0.6890\n",
            "Epoch 63/100\n",
            "2612/2612 [==============================] - 0s 38us/step - loss: 0.5860 - acc: 0.7400 - val_loss: 0.5252 - val_acc: 0.7439\n",
            "Epoch 64/100\n",
            "2612/2612 [==============================] - 0s 39us/step - loss: 0.5180 - acc: 0.7439 - val_loss: 0.5940 - val_acc: 0.7470\n",
            "Epoch 65/100\n",
            "2612/2612 [==============================] - 0s 38us/step - loss: 0.5270 - acc: 0.7481 - val_loss: 0.5116 - val_acc: 0.7485\n",
            "Epoch 66/100\n",
            "2612/2612 [==============================] - 0s 38us/step - loss: 0.5009 - acc: 0.7523 - val_loss: 0.5249 - val_acc: 0.7652\n",
            "Epoch 67/100\n",
            "2612/2612 [==============================] - 0s 38us/step - loss: 0.5471 - acc: 0.7489 - val_loss: 0.6468 - val_acc: 0.7073\n",
            "Epoch 68/100\n",
            "2612/2612 [==============================] - 0s 38us/step - loss: 0.5426 - acc: 0.7427 - val_loss: 0.5271 - val_acc: 0.7500\n",
            "Epoch 69/100\n",
            "2612/2612 [==============================] - 0s 37us/step - loss: 0.5133 - acc: 0.7500 - val_loss: 0.5033 - val_acc: 0.7546\n",
            "Epoch 70/100\n",
            "2612/2612 [==============================] - 0s 38us/step - loss: 0.5105 - acc: 0.7466 - val_loss: 0.5119 - val_acc: 0.7500\n",
            "Epoch 71/100\n",
            "2612/2612 [==============================] - 0s 38us/step - loss: 0.5427 - acc: 0.7462 - val_loss: 0.5914 - val_acc: 0.7530\n",
            "Epoch 72/100\n",
            "2612/2612 [==============================] - 0s 37us/step - loss: 0.5860 - acc: 0.7431 - val_loss: 0.5616 - val_acc: 0.7652\n",
            "Epoch 73/100\n",
            "2612/2612 [==============================] - 0s 39us/step - loss: 0.5389 - acc: 0.7423 - val_loss: 0.5470 - val_acc: 0.7683\n",
            "Epoch 74/100\n",
            "2612/2612 [==============================] - 0s 40us/step - loss: 0.5275 - acc: 0.7423 - val_loss: 0.5174 - val_acc: 0.7500\n",
            "Epoch 75/100\n",
            "2612/2612 [==============================] - 0s 35us/step - loss: 0.5166 - acc: 0.7462 - val_loss: 0.5465 - val_acc: 0.7470\n",
            "Epoch 76/100\n",
            "2612/2612 [==============================] - 0s 38us/step - loss: 0.5137 - acc: 0.7454 - val_loss: 0.5509 - val_acc: 0.7241\n",
            "Epoch 77/100\n",
            "2612/2612 [==============================] - 0s 40us/step - loss: 0.5320 - acc: 0.7462 - val_loss: 0.6494 - val_acc: 0.7439\n",
            "Epoch 78/100\n",
            "2612/2612 [==============================] - 0s 40us/step - loss: 0.5326 - acc: 0.7519 - val_loss: 0.5223 - val_acc: 0.7485\n",
            "Epoch 79/100\n",
            "2612/2612 [==============================] - 0s 40us/step - loss: 0.5063 - acc: 0.7527 - val_loss: 0.5050 - val_acc: 0.7546\n",
            "Epoch 80/100\n",
            "2612/2612 [==============================] - 0s 40us/step - loss: 0.5035 - acc: 0.7527 - val_loss: 0.5080 - val_acc: 0.7500\n",
            "Epoch 81/100\n",
            "2612/2612 [==============================] - 0s 37us/step - loss: 0.5056 - acc: 0.7450 - val_loss: 0.5339 - val_acc: 0.7470\n",
            "Epoch 82/100\n",
            "2612/2612 [==============================] - 0s 37us/step - loss: 0.5143 - acc: 0.7485 - val_loss: 0.5055 - val_acc: 0.7622\n",
            "Epoch 83/100\n",
            "2612/2612 [==============================] - 0s 41us/step - loss: 0.5093 - acc: 0.7500 - val_loss: 0.5076 - val_acc: 0.7652\n",
            "Epoch 84/100\n",
            "2612/2612 [==============================] - 0s 37us/step - loss: 0.6864 - acc: 0.7454 - val_loss: 0.8460 - val_acc: 0.7485\n",
            "Epoch 85/100\n",
            "2612/2612 [==============================] - 0s 38us/step - loss: 0.7791 - acc: 0.7404 - val_loss: 0.7382 - val_acc: 0.7515\n",
            "Epoch 86/100\n",
            "2612/2612 [==============================] - 0s 36us/step - loss: 0.6935 - acc: 0.7477 - val_loss: 0.6660 - val_acc: 0.7698\n",
            "Epoch 87/100\n",
            "2612/2612 [==============================] - 0s 42us/step - loss: 0.6575 - acc: 0.7412 - val_loss: 0.7025 - val_acc: 0.7470\n",
            "Epoch 88/100\n",
            "2612/2612 [==============================] - 0s 43us/step - loss: 0.6081 - acc: 0.7435 - val_loss: 0.5777 - val_acc: 0.7546\n",
            "Epoch 89/100\n",
            "2612/2612 [==============================] - 0s 38us/step - loss: 0.7453 - acc: 0.7420 - val_loss: 1.6266 - val_acc: 0.7454\n",
            "Epoch 90/100\n",
            "2612/2612 [==============================] - 0s 39us/step - loss: 1.4437 - acc: 0.7221 - val_loss: 1.3363 - val_acc: 0.7454\n",
            "Epoch 91/100\n",
            "2612/2612 [==============================] - 0s 39us/step - loss: 1.1807 - acc: 0.7266 - val_loss: 1.0825 - val_acc: 0.6997\n",
            "Epoch 92/100\n",
            "2612/2612 [==============================] - 0s 39us/step - loss: 0.9522 - acc: 0.7255 - val_loss: 0.8978 - val_acc: 0.7363\n",
            "Epoch 93/100\n",
            "2612/2612 [==============================] - 0s 39us/step - loss: 0.7582 - acc: 0.7312 - val_loss: 0.7193 - val_acc: 0.7439\n",
            "Epoch 94/100\n",
            "2612/2612 [==============================] - 0s 41us/step - loss: 0.6019 - acc: 0.7332 - val_loss: 0.5895 - val_acc: 0.7454\n",
            "Epoch 95/100\n",
            "2612/2612 [==============================] - 0s 38us/step - loss: 0.5273 - acc: 0.7443 - val_loss: 0.5068 - val_acc: 0.7561\n",
            "Epoch 96/100\n",
            "2612/2612 [==============================] - 0s 38us/step - loss: 0.5030 - acc: 0.7523 - val_loss: 0.5236 - val_acc: 0.7485\n",
            "Epoch 97/100\n",
            "2612/2612 [==============================] - 0s 43us/step - loss: 0.5089 - acc: 0.7504 - val_loss: 0.5592 - val_acc: 0.7515\n",
            "Epoch 98/100\n",
            "2612/2612 [==============================] - 0s 43us/step - loss: 0.5118 - acc: 0.7489 - val_loss: 0.5078 - val_acc: 0.7637\n",
            "Epoch 99/100\n",
            "2612/2612 [==============================] - 0s 40us/step - loss: 0.5116 - acc: 0.7435 - val_loss: 0.5465 - val_acc: 0.7424\n",
            "Epoch 100/100\n",
            "2612/2612 [==============================] - 0s 39us/step - loss: 0.5076 - acc: 0.7504 - val_loss: 0.5557 - val_acc: 0.7424\n",
            "656/656 [==============================] - 0s 31us/step\n",
            "\n",
            " Classification Report, \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.74      1.00      0.85       487\n",
            "         1.0       0.00      0.00      0.00       169\n",
            "\n",
            "    accuracy                           0.74       656\n",
            "   macro avg       0.37      0.50      0.43       656\n",
            "weighted avg       0.55      0.74      0.63       656\n",
            " \n",
            "\n",
            "\n",
            " Macro scores = \n",
            "\n",
            "(0.3711890243902439, 0.5, 0.42607174103237094, None)\n",
            "\n",
            " Micro Scores = \n",
            "\n",
            "(0.7423780487804879, 0.7423780487804879, 0.7423780487804879, None)\n",
            "\n",
            " weighted Scores = \n",
            "\n",
            "(0.5511251673111244, 0.7423780487804879, 0.6326126154962337, None)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdauigYJnjV1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3W1yJGhmJjx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7kCDF2Hl79M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}