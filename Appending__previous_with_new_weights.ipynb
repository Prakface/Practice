{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Appending _previous_with_new_weights.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prakface/Practice/blob/master/Appending__previous_with_new_weights.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeeEVGmX0Yol",
        "colab_type": "code",
        "outputId": "994a84cc-d8ad-41ea-f13a-df76c93c134b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "url='https://raw.githubusercontent.com/Prakface/Practice/master/One_mon_present_full.csv'\n",
        "\n",
        "url2='https://raw.githubusercontent.com/Prakface/Practice/master/Final_one_month_prev_features.csv'\n",
        "\n",
        "data = pd.read_csv(url) \n",
        "\n",
        "data2= pd.read_csv(url2)\n",
        "\n",
        "print(\"Data Shape:\", data.shape) \n",
        "\n",
        "#data=pd.read_csv(url)\n",
        "\n",
        "df=pd.DataFrame(data)\n",
        "print(data.head()) \n",
        "\n",
        "\n",
        "data_modified= data.dropna()\n",
        "\n",
        "data_modified.to_csv(\"modifiedData.csv\", index=False)\n",
        "\n",
        "\n",
        "df2=pd.read_csv(\"modifiedData.csv\")\n",
        "\n",
        "print(df2[0:6])\n",
        "\n",
        "print(df2['result'])\n",
        "\n",
        "df_main=df2[df2.columns[~df2.columns.isin(['text', 'user_name'])]]\n",
        "\n",
        "print(df_main.columns)\n",
        "\n",
        "print(len(df_main.columns))\n",
        "\n",
        "  \n",
        "# X_1, y_1 means rpesent tweets' data\n",
        "X_1=df_main[df_main.columns[~df_main.columns.isin(['time', 'tweet_id','result'])]] #removing result attribute as it is class label, hence we get 34 attributes\n",
        "X_1=X_1.iloc[:,1:len(X_1.columns)].values   #removing the unnamed attribute\n",
        "x_1=df_main[df_main.columns[~df_main.columns.isin(['time', 'tweet_id','result'])]]\n",
        "x_1=x_1.iloc[:,1:len(x_1.columns)].values \n",
        "y_1=df_main.loc[:, ['result']].values\n",
        "\n",
        "\n",
        "print(type(X_1), type(y_1), type(x_1), type(y_1))\n",
        "\n",
        "print(X_1.shape)\n",
        "print(y_1.shape)\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Shape: (1908, 40)\n",
            "  Unnamed: 0 cat1  cat10  ...      tweet_id  url      user_name\n",
            "0          0    0      0  ...  8.323790e+17  0.0  THEJEROMEOWEN\n",
            "1          1    0      0  ...  8.323786e+17  0.0       Acejinjo\n",
            "2          2    0      0  ...  8.323780e+17  0.0     RabRakha21\n",
            "3          3    0      0  ...  8.323777e+17  0.0       RS_Aloha\n",
            "4          4    0      0  ...  8.323767e+17  0.0  preciselyizzy\n",
            "\n",
            "[5 rows x 40 columns]\n",
            "   Unnamed: 0  cat1  cat10  ...      tweet_id  url        user_name\n",
            "0           0     0      0  ...  8.323790e+17  0.0    THEJEROMEOWEN\n",
            "1           1     0      0  ...  8.323786e+17  0.0         Acejinjo\n",
            "2           2     0      0  ...  8.323780e+17  0.0       RabRakha21\n",
            "3           3     0      0  ...  8.323777e+17  0.0         RS_Aloha\n",
            "4           4     0      0  ...  8.323767e+17  0.0    preciselyizzy\n",
            "5           5     0      0  ...  8.323759e+17  0.0  thefireistarted\n",
            "\n",
            "[6 rows x 40 columns]\n",
            "0       1.0\n",
            "1       1.0\n",
            "2       1.0\n",
            "3       1.0\n",
            "4       1.0\n",
            "5       1.0\n",
            "6       1.0\n",
            "7       1.0\n",
            "8       1.0\n",
            "9       1.0\n",
            "10      1.0\n",
            "11      1.0\n",
            "12      1.0\n",
            "13      1.0\n",
            "14      1.0\n",
            "15      1.0\n",
            "16      1.0\n",
            "17      1.0\n",
            "18      1.0\n",
            "19      1.0\n",
            "20      1.0\n",
            "21      1.0\n",
            "22      1.0\n",
            "23      1.0\n",
            "24      1.0\n",
            "25      1.0\n",
            "26      1.0\n",
            "27      1.0\n",
            "28      1.0\n",
            "29      1.0\n",
            "       ... \n",
            "1876    0.0\n",
            "1877    0.0\n",
            "1878    0.0\n",
            "1879    0.0\n",
            "1880    0.0\n",
            "1881    0.0\n",
            "1882    0.0\n",
            "1883    0.0\n",
            "1884    0.0\n",
            "1885    0.0\n",
            "1886    0.0\n",
            "1887    0.0\n",
            "1888    0.0\n",
            "1889    0.0\n",
            "1890    0.0\n",
            "1891    0.0\n",
            "1892    0.0\n",
            "1893    0.0\n",
            "1894    0.0\n",
            "1895    0.0\n",
            "1896    0.0\n",
            "1897    0.0\n",
            "1898    0.0\n",
            "1899    0.0\n",
            "1900    0.0\n",
            "1901    0.0\n",
            "1902    0.0\n",
            "1903    0.0\n",
            "1904    0.0\n",
            "1905    0.0\n",
            "Name: result, Length: 1906, dtype: float64\n",
            "Index(['Unnamed: 0', 'cat1', 'cat10', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6',\n",
            "       'cat7', 'cat8', 'cat9', 'favorite_count', 'hour', 'image', 'level',\n",
            "       'nadj', 'nadv', 'nemoji', 'nlevel', 'nword', 'orginal', 'padj', 'padv',\n",
            "       'pemoji', 'plevel', 'pnoun', 'punc1', 'punc2', 'punc3', 'pword',\n",
            "       'question', 'result', 'retweets_count', 'sarcasm', 'sentiment', 'time',\n",
            "       'tweet_id', 'url'],\n",
            "      dtype='object')\n",
            "38\n",
            "<class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
            "(1906, 34)\n",
            "(1906, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wq0FqtXk0gT9",
        "colab_type": "code",
        "outputId": "22509f4d-e86d-43b3-c3f2-01ef9ecdd7c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "data2= pd.read_csv(url2)\n",
        "\n",
        "print(\"Data Shape:\", data2.shape) \n",
        "\n",
        "#data=pd.read_csv(url)\n",
        "\n",
        "df_prev=pd.DataFrame(data2)\n",
        "print(data2.head()) \n",
        "\n",
        "\n",
        "data2_modified= data2.dropna()\n",
        "\n",
        "data2_modified.to_csv(\"modifiedData2.csv\", index=False)\n",
        "\n",
        "\n",
        "df_2=pd.read_csv(\"modifiedData2.csv\")\n",
        "\n",
        "print(df_2[0:6])\n",
        "\n",
        "print(df_2['result'])\n",
        "\n",
        "df_main2=df_2[df_2.columns[~df_2.columns.isin(['text', 'user_name'])]]\n",
        "\n",
        "print(df_main2.columns)\n",
        "\n",
        "print(len(df_main2.columns))\n",
        "\n",
        "  \n",
        "\n",
        "X_2=df_main2[df_main2.columns[~df_main2.columns.isin(['time', 'tweet_id','result'])]] #removing result attribute as it is class label, hence we get 34 attributes\n",
        "X_2=X_2.iloc[:,1:len(X_2.columns)].values   #removing the unnamed attribute\n",
        "x_2=df_main2[df_main2.columns[~df_main2.columns.isin(['time', 'tweet_id','result'])]]\n",
        "x_2=x_2.iloc[:,1:len(x_2.columns)].values \n",
        "y_2=df_main2.loc[:, ['result']].values\n",
        "\n",
        "\n",
        "print(type(X_2), type(y_2), type(x_2), type(y_2))\n",
        "\n",
        "print(X_2.shape)\n",
        "print(y_2.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Shape: (3004, 38)\n",
            "   Unnamed: 0  cat1  cat10  ...             tweet_id  url      user_name\n",
            "0           0     0      0  ...  1155575657402961920    1  THEJEROMEOWEN\n",
            "1           1     0      0  ...  1155459426243043328    0  THEJEROMEOWEN\n",
            "2           2     0      0  ...  1126969730307448832    0     rabrakha14\n",
            "3           3     0      0  ...  1155277550794338304    0       RS_Aloha\n",
            "4           4     0      0  ...  1155188179395207168    0       RS_Aloha\n",
            "\n",
            "[5 rows x 38 columns]\n",
            "   Unnamed: 0  cat1  cat10  ...             tweet_id  url      user_name\n",
            "0           0     0      0  ...  1155575657402961920    1  THEJEROMEOWEN\n",
            "1           1     0      0  ...  1155459426243043328    0  THEJEROMEOWEN\n",
            "2           2     0      0  ...  1126969730307448832    0     rabrakha14\n",
            "3           3     0      0  ...  1155277550794338304    0       RS_Aloha\n",
            "4           4     0      0  ...  1155188179395207168    0       RS_Aloha\n",
            "5           5     0      0  ...  1154962871765393408    0  preciselyizzy\n",
            "\n",
            "[6 rows x 38 columns]\n",
            "0       0\n",
            "1       0\n",
            "2       0\n",
            "3       0\n",
            "4       0\n",
            "5       0\n",
            "6       0\n",
            "7       0\n",
            "8       0\n",
            "9       0\n",
            "10      0\n",
            "11      0\n",
            "12      0\n",
            "13      0\n",
            "14      0\n",
            "15      0\n",
            "16      0\n",
            "17      0\n",
            "18      0\n",
            "19      0\n",
            "20      0\n",
            "21      0\n",
            "22      0\n",
            "23      0\n",
            "24      0\n",
            "25      0\n",
            "26      0\n",
            "27      0\n",
            "28      0\n",
            "29      0\n",
            "       ..\n",
            "2974    0\n",
            "2975    0\n",
            "2976    0\n",
            "2977    0\n",
            "2978    0\n",
            "2979    0\n",
            "2980    0\n",
            "2981    0\n",
            "2982    0\n",
            "2983    0\n",
            "2984    0\n",
            "2985    0\n",
            "2986    0\n",
            "2987    0\n",
            "2988    0\n",
            "2989    0\n",
            "2990    0\n",
            "2991    0\n",
            "2992    0\n",
            "2993    0\n",
            "2994    0\n",
            "2995    0\n",
            "2996    0\n",
            "2997    0\n",
            "2998    0\n",
            "2999    0\n",
            "3000    0\n",
            "3001    0\n",
            "3002    0\n",
            "3003    0\n",
            "Name: result, Length: 3004, dtype: int64\n",
            "Index(['Unnamed: 0', 'cat1', 'cat10', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6',\n",
            "       'cat7', 'cat8', 'cat9', 'favorite_count', 'hour', 'image', 'level',\n",
            "       'nadj', 'nadv', 'nemoji', 'nlevel', 'nword', 'orginal', 'padj', 'padv',\n",
            "       'pemoji', 'plevel', 'pnoun', 'punc1', 'punc2', 'punc3', 'pword',\n",
            "       'question', 'result', 'retweets_count', 'sarcasm', 'sentiment',\n",
            "       'tweet_id', 'url'],\n",
            "      dtype='object')\n",
            "37\n",
            "<class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
            "(3004, 34)\n",
            "(3004, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IL987O0l0juP",
        "colab_type": "code",
        "outputId": "44184498-cf44-4824-f267-2e6171b85be8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "#Appending present and previosu data\n",
        "\n",
        "tem=np.append(X_1, X_2, axis=0)\n",
        "tem_y=np.append(y_1,y_2,axis=0)\n",
        "print(X_1.shape, X_2.shape, tem.shape)\n",
        "print(y_1.shape, y_2.shape, tem_y.shape)\n",
        "\n",
        "X=tem\n",
        "T=tem_y\n",
        "\n",
        "print(X.shape, T.shape)\n",
        "\n",
        "print(type(X_1))\n",
        "#convert to tensor\n",
        "X = torch.from_numpy(X)\n",
        "T = torch.from_numpy(T)\n",
        "\n",
        "X_1 = torch.from_numpy(X_1)\n",
        "T_1 = torch.from_numpy(y_1)\n",
        "\n",
        "X_2 = torch.from_numpy(X_2)\n",
        "T_2 = torch.from_numpy(y_2)\n",
        "\n",
        "print(type(X), type(T))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1906, 34) (3004, 34) (4910, 34)\n",
            "(1906, 1) (3004, 1) (4910, 1)\n",
            "(4910, 34) (4910, 1)\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'torch.Tensor'> <class 'torch.Tensor'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJyAI-8h0pKH",
        "colab_type": "code",
        "outputId": "4945de7a-428d-4002-ff78-8eed18455cb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        }
      },
      "source": [
        "\n",
        "\n",
        "#X_2 = torch.from_numpy(X_2)\n",
        "#T_2 = torch.from_numpy(y_2)\n",
        "\n",
        "import torch as pytorch\n",
        "\n",
        "pytorch.set_default_tensor_type('torch.DoubleTensor')\n",
        "#random weights\n",
        "X_2=X_2.type(torch.DoubleTensor)\n",
        "T_2=T_2.type(torch.DoubleTensor)\n",
        "\n",
        "print(torch.Tensor(X_2).dtype)\n",
        "print(torch.Tensor(X_1).dtype)\n",
        "print(\"\\n dtype of T_2 \\n\", torch.Tensor(T_2).dtype)\n",
        "\n",
        "W = Variable(torch.randn(1, 34, dtype=torch.double), requires_grad=True)\n",
        "b = Variable(torch.randn(1, 1, dtype=torch.double), requires_grad=True)\n",
        "print(W.size())\n",
        "\n",
        "#Weights for Previoes tweets\n",
        "#W1 = Variable(torch.randn(1, 34, dtype=torch.double), requires_grad=True)\n",
        "W_p = Variable(torch.randn(1, 34, dtype=torch.double), requires_grad=True)\n",
        "#W_p=W_p.type(torch.LongTensor)\n",
        "b_p = Variable(torch.randn(1, 1, dtype=torch.double), requires_grad=True)\n",
        "print(W_p.size(), torch.Tensor(W_p).dtype)\n",
        "print(W.size(), torch.Tensor(W_p).dtype)\n",
        "\n",
        "\n",
        "#sigmoid\n",
        "def sigmoid(x):\n",
        "    return 1/(1+torch.exp(-x))\n",
        "\n",
        "# Loss (cross entropy) error function\n",
        "def error(output, target):\n",
        "    return -target * torch.log(output) - (1-target) * torch.log(1-output)\n",
        "  \n",
        "#out = sigmoid(torch.mm(X, W.view(34,1))+b)\n",
        "\n",
        "out_present= sigmoid(torch.mm(X_1, W.view(34,1))+b)\n",
        "\n",
        "#W1=W1.type(torch.LongTensor)\n",
        "#out_present2= sigmoid(torch.mm(X_2, W1.view(34,1))+b)\n",
        "\n",
        "out_prev= sigmoid(torch.mm(X_2, W_p.view(34,1))+b_p)\n",
        "print(W.size())\n",
        "\n",
        "\n",
        "\n",
        "#we also could use: torch.nn.Sigmoid()\n",
        "#out = torch.nn.Sigmoid()(torch.mm(X, W.view(2,1))+b)\n",
        "\n",
        "#print(out.shape, T.shape)\n",
        "\n",
        "print(out_present.shape, T_1.shape)\n",
        "\n",
        "print(out_prev.shape, T_2.shape)\n",
        "\n",
        "\n",
        "#err = error(out,T)\n",
        "err_1 = error(out_present,T_1)\n",
        "err_2 = error(out_prev,T_2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#loss = torch.mean(err)\n",
        "loss = torch.sum(torch.mean(err_1)+torch.mean(err_2))\n",
        "\n",
        "print(err_1, err_2)\n",
        "print(torch.sum(err_1), torch.sum(err_2))\n",
        "\n",
        "#we need to scale down"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.float64\n",
            "torch.float64\n",
            "\n",
            " dtype of T_2 \n",
            " torch.float64\n",
            "torch.Size([1, 34])\n",
            "torch.Size([1, 34]) torch.float64\n",
            "torch.Size([1, 34]) torch.float64\n",
            "torch.Size([1, 34])\n",
            "torch.Size([1906, 1]) torch.Size([1906, 1])\n",
            "torch.Size([3004, 1]) torch.Size([3004, 1])\n",
            "tensor([[1.3066],\n",
            "        [3.7045],\n",
            "        [0.0309],\n",
            "        ...,\n",
            "        [2.8739],\n",
            "        [0.0708],\n",
            "        [2.5995]], grad_fn=<SubBackward0>) tensor([[1.3550e-05],\n",
            "        [       inf],\n",
            "        [2.7310e-04],\n",
            "        ...,\n",
            "        [2.4086e-02],\n",
            "        [       inf],\n",
            "        [       inf]], grad_fn=<SubBackward0>)\n",
            "tensor(nan, grad_fn=<SumBackward0>) tensor(inf, grad_fn=<SumBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUt6oZX81uQe",
        "colab_type": "text"
      },
      "source": [
        "unexpected type error , while the same type is working for out , it is not for out2 This is because X_2 (the first argument ) is in Long format, so the second argument is also expected to be long.. To solve the issue, we need to convert X_2 to double format.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5PDp5SZ0tbN",
        "colab_type": "code",
        "outputId": "0abc52de-9311-4f68-89be-87a95bbf57be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## Scaling data set and applying the logistic regression\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "'''\n",
        "X=df_main[df_main.columns[~df_main.columns.isin(['time', 'tweet_id','result'])]] #removing result attribute as it is class label, hence we get 34 attributes\n",
        "X=X.iloc[:,1:len(X.columns)].values   #removing the unnamed attribute\n",
        "x=df_main[df_main.columns[~df_main.columns.isin(['time', 'tweet_id','result'])]]\n",
        "x=x.iloc[:,1:len(x.columns)].values \n",
        "y=df_main.loc[:, ['result']].values\n",
        "'''\n",
        "\n",
        "# X_1, y_1 means rpesent tweets' data\n",
        "X_1=df_main[df_main.columns[~df_main.columns.isin(['time', 'tweet_id','result'])]] #removing result attribute as it is class label, hence we get 34 attributes\n",
        "X_1=X_1.iloc[:,1:len(X_1.columns)].values   #removing the unnamed attribute\n",
        "x_1=df_main[df_main.columns[~df_main.columns.isin(['time', 'tweet_id','result'])]]\n",
        "x_1=x_1.iloc[:,1:len(x_1.columns)].values \n",
        "y_1=df_main.loc[:, ['result']].values\n",
        "\n",
        "\n",
        "\n",
        "data1=X_1\n",
        "\n",
        "scaler = StandardScaler()\n",
        "print(scaler.fit(data1))\n",
        "print(scaler.transform(data1))\n",
        "\n",
        "new_data1=scaler.transform(data1)\n",
        "\n",
        "X_1=torch.from_numpy(new_data1)\n",
        "T_1= torch.from_numpy(y_1)\n",
        "\n",
        "#random weights\n",
        "W = Variable(torch.randn(1, 34, dtype=torch.double), requires_grad=True)\n",
        "b = Variable(torch.randn(1, 1, dtype=torch.double), requires_grad=True)\n",
        "\n",
        "\n",
        "import torch as pytorch\n",
        "X_2=df_main2[df_main2.columns[~df_main2.columns.isin(['time', 'tweet_id','result'])]] #removing result attribute as it is class label, hence we get 34 attributes\n",
        "X_2=X_2.iloc[:,1:len(X_2.columns)].values   #removing the unnamed attribute\n",
        "x_2=df_main2[df_main2.columns[~df_main2.columns.isin(['time', 'tweet_id','result'])]]\n",
        "x_2=x_2.iloc[:,1:len(x_2.columns)].values \n",
        "y_2=df_main2.loc[:, ['result']].values\n",
        "\n",
        "\n",
        "data2=X_2\n",
        "\n",
        "scaler = StandardScaler()\n",
        "print(scaler.fit(data2))\n",
        "print(scaler.transform(data2))\n",
        "new_data2=scaler.transform(data2)\n",
        "\n",
        "X_2=torch.from_numpy(new_data2)\n",
        "T_2= torch.from_numpy(y_2)\n",
        "\n",
        "#do rescaling again\n",
        "\n",
        "d3=data2\n",
        "scaler2=StandardScaler()\n",
        "print(scaler.fit(d3))\n",
        "print(scaler.transform(d3))\n",
        "\n",
        "new_data3=scaler.transform(d3)\n",
        "\n",
        "X_2=torch.from_numpy(new_data3)\n",
        "T_2= torch.from_numpy(y_2)\n",
        "\n",
        "\n",
        "#normalizing the data\n",
        "\n",
        "\n",
        "\n",
        "pytorch.set_default_tensor_type('torch.DoubleTensor')\n",
        "#random weights\n",
        "X_2=X_2.type(torch.DoubleTensor)\n",
        "T_2=T_2.type(torch.DoubleTensor)\n",
        "\n",
        "print(torch.Tensor(X_2).dtype)\n",
        "print(torch.Tensor(X_1).dtype)\n",
        "print(\"\\n dtype of T_2 \\n\", torch.Tensor(T_2).dtype)\n",
        "\n",
        "\n",
        "W_p = Variable(torch.randn(1, 34, dtype=torch.double), requires_grad=True)\n",
        "#W_p=W_p.type(torch.LongTensor)\n",
        "b_p = Variable(torch.randn(1, 1, dtype=torch.double), requires_grad=True)\n",
        "print(W_p.size(), torch.Tensor(W_p).dtype)\n",
        "print(W.size(), torch.Tensor(W_p).dtype)\n",
        "\n",
        "\n",
        "#sigmoid\n",
        "def sigmoid(x):\n",
        "    return 1/(1+torch.exp(-x))\n",
        "\n",
        "# Loss (cross entropy) error function\n",
        "def error(output, target):\n",
        "    return -target * torch.log(output) - (1-target) * torch.log(1-output)\n",
        "  \n",
        "out = sigmoid(torch.mm(X_1, W.view(34,1))+b)\n",
        "#we also could use: torch.nn.Sigmoid()\n",
        "#out = torch.nn.Sigmoid()(torch.mm(X, W.view(2,1))+b)\n",
        "#err = error(out,T)\n",
        "#loss = torch.mean(err)  \n",
        "\n",
        "#print(err)\n",
        "#print(torch.mean(err))\n",
        "out_prev= sigmoid(torch.mm(X_2, W_p.view(34,1))+b_p)\n",
        "print(W.size())\n",
        "\n",
        "\n",
        "\n",
        "#we also could use: torch.nn.Sigmoid()\n",
        "#out = torch.nn.Sigmoid()(torch.mm(X, W.view(2,1))+b)\n",
        "\n",
        "#print(out.shape, T.shape)\n",
        "\n",
        "print(out_present.shape, T_1.shape)\n",
        "\n",
        "print(out_prev.shape, T_2.shape)\n",
        "\n",
        "\n",
        "#err = error(out,T)\n",
        "err_1 = error(out,T_1)\n",
        "err_2 = error(out_prev,T_2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#loss = torch.mean(err)\n",
        "loss = torch.sum(torch.mean(err_1)+torch.mean(err_2))\n",
        "\n",
        "print(err_1,\"\\n \\n\" , err_2)\n",
        "\n",
        "print(\"\\n mean of errors \\n\")\n",
        "print(torch.mean(err_1), \"\\n \\n\", torch.mean(err_2))\n",
        "\n",
        "print(\"\\n sum of errors \\n\")\n",
        "print(torch.sum(err_1), \"\\n \\n\", torch.sum(err_2))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "StandardScaler(copy=True, with_mean=True, with_std=True)\n",
            "[[-0.13476792 -0.19493167 -0.19200937 ...  0.         -0.36993901\n",
            "  -0.33372183]\n",
            " [-0.13476792 -0.19493167 -0.19200937 ...  0.         -1.60479674\n",
            "  -0.33372183]\n",
            " [-0.13476792 -0.19493167 -0.19200937 ...  0.          0.86491871\n",
            "  -0.33372183]\n",
            " ...\n",
            " [-0.13476792 -0.19493167 -0.19200937 ...  0.          0.86491871\n",
            "   2.99650757]\n",
            " [-0.13476792 -0.19493167 -0.19200937 ...  0.         -1.60479674\n",
            "  -0.33372183]\n",
            " [-0.13476792 -0.19493167 -0.19200937 ...  0.          0.86491871\n",
            "  -0.33372183]]\n",
            "StandardScaler(copy=True, with_mean=True, with_std=True)\n",
            "[[-0.11763217 -0.21155006 -0.18703747 ...  0.         -0.33180166\n",
            "   1.66123437]\n",
            " [-0.11763217 -0.21155006 -0.18703747 ...  0.         -0.33180166\n",
            "  -0.60196202]\n",
            " [-0.11763217 -0.21155006 -0.18703747 ...  0.          1.10028483\n",
            "  -0.60196202]\n",
            " ...\n",
            " [-0.11763217 -0.21155006 -0.18703747 ...  0.         -1.76388815\n",
            "   1.66123437]\n",
            " [-0.11763217  7.83270806 -0.18703747 ...  0.          1.10028483\n",
            "  -0.60196202]\n",
            " [-0.11763217  3.810579   -0.18703747 ...  0.          1.10028483\n",
            "  -0.60196202]]\n",
            "StandardScaler(copy=True, with_mean=True, with_std=True)\n",
            "[[-0.11763217 -0.21155006 -0.18703747 ...  0.         -0.33180166\n",
            "   1.66123437]\n",
            " [-0.11763217 -0.21155006 -0.18703747 ...  0.         -0.33180166\n",
            "  -0.60196202]\n",
            " [-0.11763217 -0.21155006 -0.18703747 ...  0.          1.10028483\n",
            "  -0.60196202]\n",
            " ...\n",
            " [-0.11763217 -0.21155006 -0.18703747 ...  0.         -1.76388815\n",
            "   1.66123437]\n",
            " [-0.11763217  7.83270806 -0.18703747 ...  0.          1.10028483\n",
            "  -0.60196202]\n",
            " [-0.11763217  3.810579   -0.18703747 ...  0.          1.10028483\n",
            "  -0.60196202]]\n",
            "torch.float64\n",
            "torch.float64\n",
            "\n",
            " dtype of T_2 \n",
            " torch.float64\n",
            "torch.Size([1, 34]) torch.float64\n",
            "torch.Size([1, 34]) torch.float64\n",
            "torch.Size([1, 34])\n",
            "torch.Size([1906, 1]) torch.Size([1906, 1])\n",
            "torch.Size([3004, 1]) torch.Size([3004, 1])\n",
            "tensor([[1.0128],\n",
            "        [0.9911],\n",
            "        [8.7942],\n",
            "        ...,\n",
            "        [0.8421],\n",
            "        [9.0886],\n",
            "        [1.2329]], grad_fn=<SubBackward0>) \n",
            " \n",
            " tensor([[ 1.1444],\n",
            "        [ 8.5467],\n",
            "        [ 1.3210],\n",
            "        ...,\n",
            "        [ 1.0440],\n",
            "        [17.0321],\n",
            "        [ 0.1168]], grad_fn=<SubBackward0>)\n",
            "\n",
            " mean of errors \n",
            "\n",
            "tensor(1.7243, grad_fn=<MeanBackward0>) \n",
            " \n",
            " tensor(2.0100, grad_fn=<MeanBackward0>)\n",
            "\n",
            " sum of errors \n",
            "\n",
            "tensor(3286.5224, grad_fn=<SumBackward0>) \n",
            " \n",
            " tensor(6038.1605, grad_fn=<SumBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xN162bo6PPXO",
        "colab_type": "text"
      },
      "source": [
        "We are getting NaN for previous tweets sum !!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uD2euRoD03Gy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random \n",
        "from random import randint\n",
        "\n",
        "\n",
        "test_X=[]\n",
        "test_y=[]\n",
        "\n",
        "temp=random.sample(range(1,len(X)), 10)\n",
        "print(temp)\n",
        "\n",
        "for i in temp:\n",
        "  test_X.append(X[i])\n",
        "  test_y.append(y[i])\n",
        "  \n",
        "  \n",
        "\n",
        "\n",
        "  \n",
        "  \n",
        "print(type(test_X), type(test_y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eAlkp0UvLHH",
        "colab_type": "code",
        "outputId": "c01b0abc-bae5-473d-b929-65e2cc403ef7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        }
      },
      "source": [
        "x=[1,3,4]\n",
        "y=[1,7,8,5,6,3,4]\n",
        "z=[3,5,6,8,4,9.1]\n",
        "\n",
        "for i, j in zip(y,z):\n",
        "  print(i,j)\n",
        "  \n",
        "\n",
        "print(\"\\n\\nOther-----\\n\")\n",
        "  \n",
        "for i, j, k in zip(x,y,z):\n",
        "  print(i, j,k)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 3\n",
            "7 5\n",
            "8 6\n",
            "5 8\n",
            "6 4\n",
            "3 9.1\n",
            "\n",
            "\n",
            "Other-----\n",
            "\n",
            "1 1 3\n",
            "3 7 5\n",
            "4 8 6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TF14K01Y077q",
        "colab_type": "code",
        "outputId": "bf0a4534-32a6-4491-e8ea-272d466de059",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(X_1.numpy(), T_1.numpy(), test_size=0.33, random_state=42)\n",
        "\n",
        "X_1=torch.from_numpy(X_train1)\n",
        "T_1=torch.from_numpy(y_train1)\n",
        "\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X_2.numpy(), T_2.numpy(), test_size=0.33, random_state=53)\n",
        "\n",
        "X_2=torch.from_numpy(X_train2)\n",
        "T_2=torch.from_numpy(y_train2)\n",
        "#test_X=torch.from_numpy(X_test)\n",
        "#test_y=torch.from_numpy(y_test)\n",
        "test_X1=[]\n",
        "test_y1=[]\n",
        "test_X2=[]\n",
        "test_y2=[]\n",
        "for i in X_test1:\n",
        "  test_X1.append(i)\n",
        "  \n",
        "for i in y_test1:\n",
        "  test_y1.append(i)\n",
        "  \n",
        "  \n",
        "  \n",
        "for i in X_test2:\n",
        "  test_X2.append(i)\n",
        "  \n",
        "for i in y_test2:\n",
        "  test_y2.append(i)\n",
        "  \n",
        "  \n",
        "print(type(test_X1), type(test_y1), type(test_X2), type(test_y2))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'> <class 'list'> <class 'list'> <class 'list'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVU1m3aE0-SD",
        "colab_type": "code",
        "outputId": "c0a6bb6d-9959-4a22-eba3-f91ae6f84edf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        }
      },
      "source": [
        "import math \n",
        "\n",
        "epochs=1500\n",
        "alpha=0.001\n",
        "n_iter=1\n",
        "\n",
        "torch.set_default_tensor_type('torch.DoubleTensor')\n",
        "for i in range(epochs):\n",
        "    #alternative0 (explicit definition)\n",
        "    out = sigmoid(torch.mm(X_1, W.view(34,1))+b)\n",
        "    out_prev=sigmoid(torch.mm(X_2,W_p.view(34,1))+b_p)\n",
        "    #we also could use: torch.nn.Sigmoid()\n",
        "    #out = torch.nn.Sigmoid()(torch.mm(X, W.view(2,1))+b)\n",
        "    err_1 = error(out,T_1)\n",
        "    err_2= error(out_prev, T_2)\n",
        "    loss = torch.sum(torch.mean(err_1)+toch.mean(err_2)\n",
        "\n",
        "    #alternative1 (pytorch defined loss function)\n",
        "    #out = torch.mm(X, W.view(2,1))+b\n",
        "    #loss = criterion(out, T.double())     \n",
        "\n",
        "    #alternative2 (custom error function)\n",
        "    #cross_entropy = CE.apply\n",
        "    #out = sigmoid(torch.mm(X, W.view(2,1))+b)\n",
        "    #err = cross_entropy(out,T)\n",
        "    #loss = torch.mean(err)    \n",
        "    \n",
        "    #compute gradients\n",
        "    loss.backward()\n",
        "\n",
        "    last_w = W.data.numpy()[0].copy()\n",
        "    last_b = b.data.numpy()[0].copy()\n",
        "    last_w2 = W_p.data.numpy()[0].copy()\n",
        "    last_b2 = b_p.data.numpy()[0].copy()                 \n",
        "       \n",
        "    with torch.no_grad():\n",
        "        W -= alpha * W.grad\n",
        "        b -= alpha * b.grad\n",
        "        W_p -= alpha * W_p.grad\n",
        "        b_p -= alpha * b_p.grad             \n",
        "        \n",
        "\n",
        "        # Manually zero the gradients after updating weights\n",
        "        W.grad.zero_()\n",
        "        b.grad.zero_()\n",
        "                     \n",
        "        W_p.grad.zero_()\n",
        "        b_p.grad.zero_()             \n",
        "        #print(loss)\n",
        "    \n",
        "    n_iter+=1\n",
        "    if n_iter%100==0:\n",
        "      correct = 0\n",
        "      total = 0\n",
        "            \n",
        "      # Iterate through train dataset\n",
        "      for X_v1, y_v1, X_v2,y_v2 in zip(X_train1,y_train1,X_train2,y_train2):\n",
        "        #print(\"Hi\")\n",
        "        # Load images to a Torch Variable\n",
        "        #images = Variable(images.view(-1, 28*28))\n",
        "        # Forward pass only to get logits/output\n",
        "        #outputs = sigmoid(X_val)\n",
        "        X_v1=torch.Tensor(X_v1)\n",
        "        X_temp1=(X_v1.view(1,34))\n",
        "        W_temp1=(W.view(34,1))\n",
        "                     \n",
        "        X_v2=torch.Tensor(X_v2)\n",
        "        X_temp2=(X_v2.view(1,34))\n",
        "        W_temp2=(W_p.view(34,1))\n",
        "        #print(X_val)\n",
        "        outputs=sigmoid(torch.mm(X_temp,W_temp)+b)\n",
        "        \n",
        "        outputs2=sigmoid(torch.mm(X_temp2,W_temp2)+b_p)\n",
        "        #print(outputs)\n",
        "        # Get predictions from the maximum value\n",
        "        #_, predicted = torch.max(outputs.data, 1)\n",
        "        # prediction part              \n",
        "        pred_val1=(outputs.detach().numpy())\n",
        "                     \n",
        "        pred_val2=(outputs2.detach().numpy())\n",
        "                     \n",
        "                     \n",
        "        pred_val= (2*pred_val1*pred_val2)/ (pred_val1 + pred_val2)   #using harmonic mean\n",
        "                     \n",
        "        #pred_val= (pred_val1 + pred_val2 ) /2      #using Arithmetic mean\n",
        "                     \n",
        "                     \n",
        "                     \n",
        "                     \n",
        "        #print(type(pred_val), pred_val) \n",
        "        # for accessing scalar value from tensor, by converting it to numpy array\n",
        "        for i in pred_val:\n",
        "          for j in i:\n",
        "            #print(type(j))\n",
        "            temp=j\n",
        "            \n",
        "        predicted=round(temp)\n",
        "        # Total number of labels\n",
        "        #total += y_val.size(0)\n",
        "        total+=1\n",
        "        # Total correct predictions\n",
        "        correct += (predicted == y_v).sum()\n",
        "        \n",
        "      accuracy = 100 * correct / total\n",
        "      print(\"\\n In epoch \", n_iter, \" Training accuracy= \", accuracy)\n",
        "      \n",
        "      \n",
        "      # Iterate through test dataset\n",
        "      for X_v1, y_v1, X_v2,y_v2 in zip(X_test1,y_test1,X_test2,y_test2):\n",
        "        #print(\"Hi\")\n",
        "        # Load images to a Torch Variable\n",
        "        #images = Variable(images.view(-1, 28*28))\n",
        "        # Forward pass only to get logits/output\n",
        "        #outputs = sigmoid(X_val)\n",
        "        X_v1=torch.Tensor(X_v1)\n",
        "        X_temp1=(X_v1.view(1,34))\n",
        "        W_temp1=(W.view(34,1))\n",
        "                     \n",
        "        X_v2=torch.Tensor(X_v2)\n",
        "        X_temp2=(X_v2.view(1,34))\n",
        "        W_temp2=(W_p.view(34,1))\n",
        "        #print(X_val)\n",
        "        outputs=sigmoid(torch.mm(X_temp,W_temp)+b)\n",
        "        \n",
        "        outputs2=sigmoid(torch.mm(X_temp2,W_temp2)+b_p)\n",
        "        #print(outputs)\n",
        "        # Get predictions from the maximum value\n",
        "        #_, predicted = torch.max(outputs.data, 1)\n",
        "        # prediction part              \n",
        "        pred_val1=(outputs.detach().numpy())\n",
        "                     \n",
        "        pred_val2=(outputs2.detach().numpy())\n",
        "                     \n",
        "                     \n",
        "        pred_val= (2*pred_val1*pred_val2)/ (pred_val1 + pred_val2)   #using harmonic mean\n",
        "                     \n",
        "        #pred_val= (pred_val1 + pred_val2 ) /2      #using Arithmetic mean\n",
        "                     \n",
        "                     \n",
        "                     \n",
        "                     \n",
        "        #print(type(pred_val), pred_val) \n",
        "        # for accessing scalar value from tensor, by converting it to numpy array\n",
        "        for i in pred_val:\n",
        "          for j in i:\n",
        "            #print(type(j))\n",
        "            temp=j\n",
        "            \n",
        "        predicted=round(temp)\n",
        "        # Total number of labels\n",
        "        #total += y_val.size(0)\n",
        "        total+=1\n",
        "        # Total correct predictions\n",
        "        correct += (predicted == y_v).sum()\n",
        "                     \n",
        "      accuracy = 100 * correct / total\n",
        "      print(\"\\n In epoch \", n_iter, \" Testing accuracy= \", accuracy)\n",
        "      "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-37ed390755e9>\"\u001b[0;36m, line \u001b[0;32m29\u001b[0m\n\u001b[0;31m    loss.backward()\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9fBFG5WtZQv",
        "colab_type": "code",
        "outputId": "b8a1b404-c95f-4d0d-c6ad-e50aa5289dcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "########With different loss functions, independent training####\n",
        "\n",
        "\n",
        "## Scaling data set and applying the logistic regression\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "'''\n",
        "X=df_main[df_main.columns[~df_main.columns.isin(['time', 'tweet_id','result'])]] #removing result attribute as it is class label, hence we get 34 attributes\n",
        "X=X.iloc[:,1:len(X.columns)].values   #removing the unnamed attribute\n",
        "x=df_main[df_main.columns[~df_main.columns.isin(['time', 'tweet_id','result'])]]\n",
        "x=x.iloc[:,1:len(x.columns)].values \n",
        "y=df_main.loc[:, ['result']].values\n",
        "'''\n",
        "\n",
        "# X_1, y_1 means rpesent tweets' data\n",
        "X_1=df_main[df_main.columns[~df_main.columns.isin(['time', 'tweet_id','result'])]] #removing result attribute as it is class label, hence we get 34 attributes\n",
        "X_1=X_1.iloc[:,1:len(X_1.columns)].values   #removing the unnamed attribute\n",
        "x_1=df_main[df_main.columns[~df_main.columns.isin(['time', 'tweet_id','result'])]]\n",
        "x_1=x_1.iloc[:,1:len(x_1.columns)].values \n",
        "y_1=df_main.loc[:, ['result']].values\n",
        "\n",
        "\n",
        "\n",
        "data1=X_1\n",
        "\n",
        "scaler = StandardScaler()\n",
        "print(scaler.fit(data1))\n",
        "print(scaler.transform(data1))\n",
        "\n",
        "new_data1=scaler.transform(data1)\n",
        "\n",
        "X_1=torch.from_numpy(new_data1)\n",
        "T_1= torch.from_numpy(y_1)\n",
        "\n",
        "#random weights\n",
        "W = Variable(torch.randn(1, 34, dtype=torch.double), requires_grad=True)\n",
        "b = Variable(torch.randn(1, 1, dtype=torch.double), requires_grad=True)\n",
        "\n",
        "\n",
        "import torch as pytorch\n",
        "X_2=df_main2[df_main2.columns[~df_main2.columns.isin(['time', 'tweet_id','result'])]] #removing result attribute as it is class label, hence we get 34 attributes\n",
        "X_2=X_2.iloc[:,1:len(X_2.columns)].values   #removing the unnamed attribute\n",
        "x_2=df_main2[df_main2.columns[~df_main2.columns.isin(['time', 'tweet_id','result'])]]\n",
        "x_2=x_2.iloc[:,1:len(x_2.columns)].values \n",
        "y_2=df_main2.loc[:, ['result']].values\n",
        "\n",
        "\n",
        "data2=X_2\n",
        "\n",
        "scaler = StandardScaler()\n",
        "print(scaler.fit(data2))\n",
        "print(scaler.transform(data2))\n",
        "new_data2=scaler.transform(data2)\n",
        "\n",
        "X_2=torch.from_numpy(new_data2)\n",
        "T_2= torch.from_numpy(y_2)\n",
        "\n",
        "#do rescaling again\n",
        "\n",
        "d3=data2\n",
        "scaler2=StandardScaler()\n",
        "print(scaler.fit(d3))\n",
        "print(scaler.transform(d3))\n",
        "\n",
        "new_data3=scaler.transform(d3)\n",
        "\n",
        "X_2=torch.from_numpy(new_data3)\n",
        "T_2= torch.from_numpy(y_2)\n",
        "\n",
        "\n",
        "#normalizing the data\n",
        "\n",
        "\n",
        "\n",
        "pytorch.set_default_tensor_type('torch.DoubleTensor')\n",
        "#random weights\n",
        "X_2=X_2.type(torch.DoubleTensor)\n",
        "T_2=T_2.type(torch.DoubleTensor)\n",
        "\n",
        "print(torch.Tensor(X_2).dtype)\n",
        "print(torch.Tensor(X_1).dtype)\n",
        "print(\"\\n dtype of T_2 \\n\", torch.Tensor(T_2).dtype)\n",
        "\n",
        "\n",
        "W_p = Variable(torch.randn(1, 34, dtype=torch.double), requires_grad=True)\n",
        "#W_p=W_p.type(torch.LongTensor)\n",
        "b_p = Variable(torch.randn(1, 1, dtype=torch.double), requires_grad=True)\n",
        "print(W_p.size(), torch.Tensor(W_p).dtype)\n",
        "print(W.size(), torch.Tensor(W_p).dtype)\n",
        "\n",
        "\n",
        "#sigmoid\n",
        "def sigmoid(x):\n",
        "    return 1/(1+torch.exp(-x))\n",
        "\n",
        "# Loss (cross entropy) error function\n",
        "def error(output, target):\n",
        "    return -target * torch.log(output) - (1-target) * torch.log(1-output)\n",
        "  \n",
        "out = sigmoid(torch.mm(X_1, W.view(34,1))+b)\n",
        "#we also could use: torch.nn.Sigmoid()\n",
        "#out = torch.nn.Sigmoid()(torch.mm(X, W.view(2,1))+b)\n",
        "#err = error(out,T)\n",
        "#loss = torch.mean(err)  \n",
        "\n",
        "#print(err)\n",
        "#print(torch.mean(err))\n",
        "out_prev= sigmoid(torch.mm(X_2, W_p.view(34,1))+b_p)\n",
        "print(W.size())\n",
        "\n",
        "\n",
        "\n",
        "#we also could use: torch.nn.Sigmoid()\n",
        "#out = torch.nn.Sigmoid()(torch.mm(X, W.view(2,1))+b)\n",
        "\n",
        "#print(out.shape, T.shape)\n",
        "\n",
        "print(out_present.shape, T_1.shape)\n",
        "\n",
        "print(out_prev.shape, T_2.shape)\n",
        "\n",
        "\n",
        "#err = error(out,T)\n",
        "err_1 = error(out,T_1)\n",
        "err_2 = error(out_prev,T_2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#loss = torch.mean(err)\n",
        "loss1 = torch.mean(err_1)\n",
        "loss2 = torch.mean(err_2)\n",
        "\n",
        "print(err_1,\"\\n \\n\" , err_2)\n",
        "\n",
        "print(\"\\n mean of errors \\n\")\n",
        "print(torch.mean(err_1), \"\\n \\n\", torch.mean(err_2))\n",
        "\n",
        "print(\"\\n sum of errors \\n\")\n",
        "print(torch.sum(err_1), \"\\n \\n\", torch.sum(err_2))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "StandardScaler(copy=True, with_mean=True, with_std=True)\n",
            "[[-0.13476792 -0.19493167 -0.19200937 ...  0.         -0.36993901\n",
            "  -0.33372183]\n",
            " [-0.13476792 -0.19493167 -0.19200937 ...  0.         -1.60479674\n",
            "  -0.33372183]\n",
            " [-0.13476792 -0.19493167 -0.19200937 ...  0.          0.86491871\n",
            "  -0.33372183]\n",
            " ...\n",
            " [-0.13476792 -0.19493167 -0.19200937 ...  0.          0.86491871\n",
            "   2.99650757]\n",
            " [-0.13476792 -0.19493167 -0.19200937 ...  0.         -1.60479674\n",
            "  -0.33372183]\n",
            " [-0.13476792 -0.19493167 -0.19200937 ...  0.          0.86491871\n",
            "  -0.33372183]]\n",
            "StandardScaler(copy=True, with_mean=True, with_std=True)\n",
            "[[-0.11763217 -0.21155006 -0.18703747 ...  0.         -0.33180166\n",
            "   1.66123437]\n",
            " [-0.11763217 -0.21155006 -0.18703747 ...  0.         -0.33180166\n",
            "  -0.60196202]\n",
            " [-0.11763217 -0.21155006 -0.18703747 ...  0.          1.10028483\n",
            "  -0.60196202]\n",
            " ...\n",
            " [-0.11763217 -0.21155006 -0.18703747 ...  0.         -1.76388815\n",
            "   1.66123437]\n",
            " [-0.11763217  7.83270806 -0.18703747 ...  0.          1.10028483\n",
            "  -0.60196202]\n",
            " [-0.11763217  3.810579   -0.18703747 ...  0.          1.10028483\n",
            "  -0.60196202]]\n",
            "StandardScaler(copy=True, with_mean=True, with_std=True)\n",
            "[[-0.11763217 -0.21155006 -0.18703747 ...  0.         -0.33180166\n",
            "   1.66123437]\n",
            " [-0.11763217 -0.21155006 -0.18703747 ...  0.         -0.33180166\n",
            "  -0.60196202]\n",
            " [-0.11763217 -0.21155006 -0.18703747 ...  0.          1.10028483\n",
            "  -0.60196202]\n",
            " ...\n",
            " [-0.11763217 -0.21155006 -0.18703747 ...  0.         -1.76388815\n",
            "   1.66123437]\n",
            " [-0.11763217  7.83270806 -0.18703747 ...  0.          1.10028483\n",
            "  -0.60196202]\n",
            " [-0.11763217  3.810579   -0.18703747 ...  0.          1.10028483\n",
            "  -0.60196202]]\n",
            "torch.float64\n",
            "torch.float64\n",
            "\n",
            " dtype of T_2 \n",
            " torch.float64\n",
            "torch.Size([1, 34]) torch.float64\n",
            "torch.Size([1, 34]) torch.float64\n",
            "torch.Size([1, 34])\n",
            "torch.Size([1906, 1]) torch.Size([1906, 1])\n",
            "torch.Size([3004, 1]) torch.Size([3004, 1])\n",
            "tensor([[9.8023e-03],\n",
            "        [8.7459e+00],\n",
            "        [2.7872e-01],\n",
            "        ...,\n",
            "        [3.4870e-04],\n",
            "        [2.1524e-02],\n",
            "        [8.8377e-01]], grad_fn=<SubBackward0>) \n",
            " \n",
            " tensor([[2.9260e-03],\n",
            "        [1.0237e+00],\n",
            "        [3.0704e+00],\n",
            "        ...,\n",
            "        [3.6017e-06],\n",
            "        [1.9809e-08],\n",
            "        [4.0610e+00]], grad_fn=<SubBackward0>)\n",
            "\n",
            " mean of errors \n",
            "\n",
            "tensor(2.4100, grad_fn=<MeanBackward0>) \n",
            " \n",
            " tensor(1.4930, grad_fn=<MeanBackward0>)\n",
            "\n",
            " sum of errors \n",
            "\n",
            "tensor(4593.3869, grad_fn=<SumBackward0>) \n",
            " \n",
            " tensor(4484.9151, grad_fn=<SumBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzDK4Gt8ttqo",
        "colab_type": "code",
        "outputId": "303def2b-88be-4089-c080-dff18dcabd80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(X_1.numpy(), T_1.numpy(), test_size=0.33, random_state=42)\n",
        "\n",
        "X_1=torch.from_numpy(X_train1)\n",
        "T_1=torch.from_numpy(y_train1)\n",
        "\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X_2.numpy(), T_2.numpy(), test_size=0.33, random_state=53)\n",
        "\n",
        "X_2=torch.from_numpy(X_train2)\n",
        "T_2=torch.from_numpy(y_train2)\n",
        "#test_X=torch.from_numpy(X_test)\n",
        "#test_y=torch.from_numpy(y_test)\n",
        "test_X1=[]\n",
        "test_y1=[]\n",
        "test_X2=[]\n",
        "test_y2=[]\n",
        "for i in X_test1:\n",
        "  test_X1.append(i)\n",
        "  \n",
        "for i in y_test1:\n",
        "  test_y1.append(i)\n",
        "  \n",
        "  \n",
        "  \n",
        "for i in X_test2:\n",
        "  test_X2.append(i)\n",
        "  \n",
        "for i in y_test2:\n",
        "  test_y2.append(i)\n",
        "  \n",
        "  \n",
        "print(type(test_X1), type(test_y1), type(test_X2), type(test_y2))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'> <class 'list'> <class 'list'> <class 'list'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7F2CQ0at70E",
        "colab_type": "code",
        "outputId": "3ced30c4-1628-4f28-ea0f-2c62087abf39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import math \n",
        "\n",
        "epochs=1500\n",
        "alpha=0.001\n",
        "n_iter=1\n",
        "\n",
        "loss1_values=[]\n",
        "loss2_values=[]\n",
        "\n",
        "torch.set_default_tensor_type('torch.DoubleTensor')\n",
        "for i in range(epochs):\n",
        "    #alternative0 (explicit definition)\n",
        "    out = sigmoid(torch.mm(X_1, W.view(34,1))+b)\n",
        "    out_prev=sigmoid(torch.mm(X_2,W_p.view(34,1))+b_p)\n",
        "    #we also could use: torch.nn.Sigmoid()\n",
        "    #out = torch.nn.Sigmoid()(torch.mm(X, W.view(2,1))+b)\n",
        "    err_1 = error(out,T_1)\n",
        "    err_2= error(out_prev, T_2)\n",
        "    loss1 = torch.mean(err_1) \n",
        "    loss2 = torch.mean(err_2)\n",
        "    loss1_values.append(loss1)\n",
        "    loss2_values.append(loss2)\n",
        "\n",
        "    #alternative1 (pytorch defined loss function)\n",
        "    #out = torch.mm(X, W.view(2,1))+b\n",
        "    #loss = criterion(out, T.double())     \n",
        "\n",
        "    #alternative2 (custom error function)\n",
        "    #cross_entropy = CE.apply\n",
        "    #out = sigmoid(torch.mm(X, W.view(2,1))+b)\n",
        "    #err = cross_entropy(out,T)\n",
        "    #loss = torch.mean(err)    \n",
        "    \n",
        "    #compute gradients\n",
        "    loss1.backward()\n",
        "    loss2.backward()\n",
        "\n",
        "    last_w = W.data.numpy()[0].copy()\n",
        "    last_b = b.data.numpy()[0].copy()\n",
        "    last_w2 = W_p.data.numpy()[0].copy()\n",
        "    last_b2 = b_p.data.numpy()[0].copy()     \n",
        "    \n",
        "    \n",
        "    \n",
        "       \n",
        "    with torch.no_grad():\n",
        "        W -= alpha * W.grad\n",
        "        b -= alpha * b.grad\n",
        "        W_p -= alpha * W_p.grad\n",
        "        b_p -= alpha * b_p.grad             \n",
        "        \n",
        "\n",
        "        # Manually zero the gradients after updating weights\n",
        "        W.grad.zero_()\n",
        "        b.grad.zero_()\n",
        "                     \n",
        "        W_p.grad.zero_()\n",
        "        b_p.grad.zero_()             \n",
        "        #print(loss)\n",
        "        \n",
        "    #loss1_values.append(loss1)\n",
        "    #loss2_values.append(loss2)\n",
        "    \n",
        "    n_iter+=1\n",
        "    if n_iter%100==0:\n",
        "      correct = 0\n",
        "      total = 0\n",
        "            \n",
        "      # Iterate through train dataset\n",
        "      for X_v1, y_v1, X_v2,y_v2 in zip(X_train1,y_train1,X_train2,y_train2):\n",
        "        #print(\"Hi\")\n",
        "        # Load images to a Torch Variable\n",
        "        #images = Variable(images.view(-1, 28*28))\n",
        "        # Forward pass only to get logits/output\n",
        "        #outputs = sigmoid(X_val)\n",
        "        X_v1=torch.Tensor(X_v1)\n",
        "        X_temp1=(X_v1.view(1,34))\n",
        "        W_temp1=(W.view(34,1))\n",
        "                     \n",
        "        X_v2=torch.Tensor(X_v2)\n",
        "        X_temp2=(X_v2.view(1,34))\n",
        "        W_temp2=(W_p.view(34,1))\n",
        "        #print(X_val)\n",
        "        outputs=sigmoid(torch.mm(X_temp1,W_temp1)+b)\n",
        "        \n",
        "        outputs2=sigmoid(torch.mm(X_temp2,W_temp2)+b_p)\n",
        "        #print(outputs)\n",
        "        # Get predictions from the maximum value\n",
        "        #_, predicted = torch.max(outputs.data, 1)\n",
        "        # prediction part              \n",
        "        pred_val1=(outputs.detach().numpy())\n",
        "                     \n",
        "        pred_val2=(outputs2.detach().numpy())\n",
        "                     \n",
        "                     \n",
        "        pred_val= (2*pred_val1*pred_val2)/ (pred_val1 + pred_val2)   #using harmonic mean\n",
        "                     \n",
        "        #pred_val= (pred_val1 + pred_val2 ) /2      #using Arithmetic mean\n",
        "                     \n",
        "                     \n",
        "                     \n",
        "                     \n",
        "        #print(type(pred_val), pred_val) \n",
        "        # for accessing scalar value from tensor, by converting it to numpy array\n",
        "        for i in pred_val:\n",
        "          for j in i:\n",
        "            #print(type(j))\n",
        "            temp=j\n",
        "            \n",
        "        predicted=round(temp)\n",
        "        # Total number of labels\n",
        "        #total += y_val.size(0)\n",
        "        total+=1\n",
        "        # Total correct predictions\n",
        "        correct += (predicted == y_v1).sum()\n",
        "        \n",
        "      accuracy = 100 * correct / total\n",
        "      print(\"\\n In epoch \", n_iter, \" Training accuracy= \", accuracy)\n",
        "      \n",
        "      \n",
        "      # Iterate through test dataset\n",
        "      for X_v1, y_v1, X_v2,y_v2 in zip(X_test1,y_test1,X_test2,y_test2):\n",
        "        #print(\"Hi\")\n",
        "        # Load images to a Torch Variable\n",
        "        #images = Variable(images.view(-1, 28*28))\n",
        "        # Forward pass only to get logits/output\n",
        "        #outputs = sigmoid(X_val)\n",
        "        X_v1=torch.Tensor(X_v1)\n",
        "        X_temp1=(X_v1.view(1,34))\n",
        "        W_temp1=(W.view(34,1))\n",
        "                     \n",
        "        X_v2=torch.Tensor(X_v2)\n",
        "        X_temp2=(X_v2.view(1,34))\n",
        "        W_temp2=(W_p.view(34,1))\n",
        "        #print(X_val)\n",
        "        outputs=sigmoid(torch.mm(X_temp1,W_temp1)+b)\n",
        "        \n",
        "        outputs2=sigmoid(torch.mm(X_temp2,W_temp2)+b_p)\n",
        "        #print(outputs)\n",
        "        # Get predictions from the maximum value\n",
        "        #_, predicted = torch.max(outputs.data, 1)\n",
        "        # prediction part              \n",
        "        pred_val1=(outputs.detach().numpy())\n",
        "                     \n",
        "        pred_val2=(outputs2.detach().numpy())\n",
        "                     \n",
        "                     \n",
        "        pred_val= (2*pred_val1*pred_val2)/ (pred_val1 + pred_val2)   #using harmonic mean\n",
        "                     \n",
        "        #pred_val= (pred_val1 + pred_val2 ) /2      #using Arithmetic mean\n",
        "                     \n",
        "                     \n",
        "                     \n",
        "                     \n",
        "        #print(type(pred_val), pred_val) \n",
        "        # for accessing scalar value from tensor, by converting it to numpy array\n",
        "        for i in pred_val:\n",
        "          for j in i:\n",
        "            #print(type(j))\n",
        "            temp=j\n",
        "            \n",
        "        predicted=round(temp)\n",
        "        # Total number of labels\n",
        "        #total += y_val.size(0)\n",
        "        total+=1\n",
        "        # Total correct predictions\n",
        "        correct += (predicted == y_v1).sum()\n",
        "                     \n",
        "      accuracy = 100 * correct / total\n",
        "      print(\"\\n In epoch \", n_iter, \" Testing accuracy= \", accuracy)\n",
        "      "
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " In epoch  100  Training accuracy=  47.06342991386061\n",
            "\n",
            " In epoch  100  Testing accuracy=  47.061909758656874\n",
            "\n",
            " In epoch  200  Training accuracy=  47.846515270164446\n",
            "\n",
            " In epoch  200  Testing accuracy=  47.79643231899266\n",
            "\n",
            " In epoch  300  Training accuracy=  48.15974941268598\n",
            "\n",
            " In epoch  300  Testing accuracy=  48.32109129066107\n",
            "\n",
            " In epoch  400  Training accuracy=  48.003132341425214\n",
            "\n",
            " In epoch  400  Testing accuracy=  48.32109129066107\n",
            "\n",
            " In epoch  500  Training accuracy=  48.31636648394675\n",
            "\n",
            " In epoch  500  Testing accuracy=  48.68835257082896\n",
            "\n",
            " In epoch  600  Training accuracy=  48.707909162098666\n",
            "\n",
            " In epoch  600  Testing accuracy=  49.00314795383001\n",
            "\n",
            " In epoch  700  Training accuracy=  49.0211433046202\n",
            "\n",
            " In epoch  700  Testing accuracy=  49.21301154249738\n",
            "\n",
            " In epoch  800  Training accuracy=  49.64761158966327\n",
            "\n",
            " In epoch  800  Testing accuracy=  49.73767051416579\n",
            "\n",
            " In epoch  900  Training accuracy=  49.569303054032886\n",
            "\n",
            " In epoch  900  Testing accuracy=  49.73767051416579\n",
            "\n",
            " In epoch  1000  Training accuracy=  49.64761158966327\n",
            "\n",
            " In epoch  1000  Testing accuracy=  49.68520461699895\n",
            "\n",
            " In epoch  1100  Training accuracy=  50.19577133907596\n",
            "\n",
            " In epoch  1100  Testing accuracy=  50.10493179433368\n",
            "\n",
            " In epoch  1200  Training accuracy=  50.430696945967114\n",
            "\n",
            " In epoch  1200  Testing accuracy=  50.524658971668416\n",
            "\n",
            " In epoch  1300  Training accuracy=  50.587314017227875\n",
            "\n",
            " In epoch  1300  Testing accuracy=  50.73452256033578\n",
            "\n",
            " In epoch  1400  Training accuracy=  50.743931088488644\n",
            "\n",
            " In epoch  1400  Testing accuracy=  50.94438614900315\n",
            "\n",
            " In epoch  1500  Training accuracy=  50.66562255285826\n",
            "\n",
            " In epoch  1500  Testing accuracy=  50.94438614900315\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBRZVZJ1KZu1",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m24KPsorKa2D"
      },
      "source": [
        "But here zip function takes only first 1906 tweets out of total 3004 previous tweets !!\n",
        "so we have to train both present and revious separately"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkSDlLdmJZ1B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "498cb12b-0b42-453a-fd05-572944dd2aec"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#a=[2,4,6,3]\n",
        "a=[1,4,9,16, 25,36,49,64,81,100,121,144]\n",
        "\n",
        "#plt.plot([1,4,9,16, 25,36,49,64,81,100,121,144])\n",
        "plt.plot(a)\n",
        "plt.ylabel('some numbers')\n",
        "plt.xlabel('epoch')\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XecVOXZ//HPRYelswvIwsKCCAKC\nwFKMWGKJGAuJLVYERdTHlsSoYIr5PUmeaKJGU1WkCnYxomIsaCwxlKV36WVpS1sQWLZdvz9mMCtZ\nYVhm9kz5vl8vXjvnnjN7rgF2vnvuc5/7NndHRETkcNWCLkBEROKTAkJERCqkgBARkQopIEREpEIK\nCBERqZACQkREKqSAEBGRCikgRESkQgoIERGpUI2gCzge6enp3q5du6DLEBFJKLNnz97u7hlH2y+h\nA6Jdu3bk5uYGXYaISEIxs3WR7KcuJhERqZACQkREKqSAEBGRCikgRESkQgoIERGpkAJCREQqpIAQ\nEZEKKSBERBKIu/PEB1+wZNOemB8roW+UExFJNaM/W8MTH6zgYEkZXVo1jOmxdAYhIpIg3lm4md9M\nXcrAri257zudYn68mAWEmY0xs21mtqiC5+41Mzez9PC2mdkfzWylmS0ws16xqktEJBHNXreLH740\nj1PbNOaJq0+lWjWL+TFjeQYxDhh4eKOZtQG+A6wv13wh0DH8ZzjwtxjWJSKSUNbt2MctE3Jp0bAO\nzw7OoU7N6lVy3JgFhLt/Auys4Kk/APcDXq5tEDDBQ6YDjc3shFjVJiKSKHbtK2LI2FmUuTNuaB+a\n1a9dZceu0msQZjYIyHP3+Yc9lQlsKLe9MdwmIpKyCotLuWVCLnm7DzBqcA7tM+pX6fGrbBSTmdUD\nHiTUvXQ832c4oW4osrKyolCZiEj8KStz7n1lPrnrdvGna3rSp13TKq+hKs8gOgDZwHwzWwu0BuaY\nWUsgD2hTbt/W4bb/4u7PuHuOu+dkZBx1vQsRkYT0yLvLeHvBZkZc2JlLerQKpIYqCwh3X+juzd29\nnbu3I9SN1MvdtwBTgMHh0Uz9gQJ331xVtYmIxJOJ09fx9Merua5fFree2T6wOmI5zPUF4N9AJzPb\naGY3H2H3qcBqYCUwCvifWNUlIhLPPlq2jV+8sYhvd8rg/13aFbPYD2f9JjG7BuHu1xzl+XblHjtw\nR6xqERFJBIvyCrjj+TmcfEJD/nxtL2pUD/ZeZt1JLSISB/J2H2DouFk0rluTMUP6kFY7+JmQFBAi\nIgHbU1jM0LEzKSwqZezQvrRoWCfokgBN1iciEqiikjJunzib1fn7GH9TXzq1bBB0SV9RQIiIBMTd\nGTl5If9auYNHr+zB6SemB13S16iLSUQkIE9OW8FrczZyz7kduaJ366DL+S8KCBGRALw6eyNPfLCC\ny3u15ofndQy6nAopIEREqtjnK7cz4rUFfKtDM3572SmB3utwJAoIEZEq9MXWvdw6cTbtM9L42/W9\nqVUjfj+G47cyEZEks21PIUPHzqJOzeqMGdKHRnVrBl3SESkgRESqwL6DJdw0fha79hcxdkgfWjep\nF3RJR6WAEBGJsZLSMu56YS5LNu3hL9f2oltmo6BLiojugxARiSF355dvLubDZdv49fe68e3OzYMu\nKWI6gxARiaFnPlnNxOnrufXM9lzfv23Q5RwTBYSISIy8tWATv31nGRd1P4EHBnYOupxjpoAQEYmB\n3LU7+fHL88lp24THruxBtWrxea/DkSggRESibM32fdwyIZfMxnUZNTiHOjWrB11SpSggRESiaMeX\nBxkydiZmxtghfWiSVivokipNASEiEiWFxaUMm5DLloJCRg3OoV16WtAlHRcNcxURiYKyMudHL81j\n3obd/PXaXvRu2yToko5bzM4gzGyMmW0zs0Xl2n5vZsvMbIGZvW5mjcs9N9LMVprZcjO7IFZ1iYjE\nwm/fWco7i7bw0++ezIWnnBB0OVERyy6mccDAw9reB7q5e3fgC2AkgJl1Aa4GuoZf81czS8yrOiKS\ncsb9aw2jPl3Djae15eYB2UGXEzUxCwh3/wTYeVjbe+5eEt6cDhxaIWMQ8KK7H3T3NcBKoG+sahMR\niZZXcjfwyzeXcH6XFvzikq5xO3V3ZQR5kfom4J3w40xgQ7nnNobb/ouZDTezXDPLzc/Pj3GJIiLf\n7K0Fm3jgtQWc0TGdP13Tk+oJeK/DkQQSEGb2U6AEmHSsr3X3Z9w9x91zMjIyol+ciEgEpi3dyg9f\nnEfvtk14+obeCXuvw5FU+SgmMxsCXAyc6+4ebs4D2pTbrXW4TUQk7vxr5XZunzSHLq0aMnpIH+rV\nSs4BoVV6BmFmA4H7gUvdfX+5p6YAV5tZbTPLBjoCM6uyNhGRSOSu3cmw8blkN0tj/NC+NKwT34v+\nHI+YxZ6ZvQCcDaSb2UbgIUKjlmoD74cv5Ex399vcfbGZvQwsIdT1dIe7l8aqNhGRyliUV8DQsbNo\n2agOzw3rm9B3SUfC/tPLk3hycnI8Nzc36DJEJAV8sXUvP3j639SrVYNXbjuNVo3rBl1SpZnZbHfP\nOdp+mmpDROQo1m7fx3XPzqBm9WpMGtYvocPhWCggRESOIG/3Aa57dgYlpWVMGtYv4edXOhbJeeld\nRCQKtu0p5LpR09lTWMwLt/SnY4sGQZdUpXQGISJSgV37irh+9Ay27T3IuKF96JbZKOiSqpzOIERE\nDrOnsJjBY2aydsd+xg3pQ++2TYMuKRA6gxARKWd/UQk3jZ3F0s17eOr6XnzrxPSgSwqMAkJEJKyw\nuJThE2YzZ/0unry6J+d0bhF0SYFSF5OICFBcWsadz8/hs5XbefTKHlzUPTnWdDgeOoMQkZRXGl4N\n7oOl2/jVoK5c0bv10V+UAhQQIpLSysqcEa8t4K0Fmxl5YWduOK1d0CXFDQWEiKQsd+d/31rCK7M3\ncve5Hbn1rA5BlxRXFBAikrJ+/+5yxn2+lmEDsvnReR2DLifuKCBEJCX95aOV/PWfq7i2XxY/vejk\npFoqNFoUECKScsZ8tobfv7uc7/fM5NeDuikcvoECQkRSyosz1/O/by1hYNeW/P6K7lRLsnWko0kB\nISIp4415eYx8fSFnd8rgj9f0pEZ1fQQeif52RCQlvLd4Cz9+eT79spvy1PW9qVVDH39Ho78hEUl6\nn3yRz53Pz+WUzEY8e2Mf6tSsHnRJCSFmAWFmY8xsm5ktKtfW1MzeN7MV4a9Nwu1mZn80s5VmtsDM\nesWqLhFJLTPX7GT4c7l0aF6f8UP7Ur+2ZhiKVCzPIMYBAw9rGwFMc/eOwLTwNsCFQMfwn+HA32JY\nl4ikiPkbdnPTuFlkNq7Lczf3pVG9mkGXlFBiFhDu/gmw87DmQcD48OPxwPfKtU/wkOlAYzPTTFki\nUmlLN+9h8JiZNEmryaRh/UmvXzvokhJOVV+DaOHum8OPtwCH5tLNBDaU229juE1E5Jit3LaXG0bP\noG7N6jw/rD8tG9UJuqSEFNhFand3wI/1dWY23MxyzSw3Pz8/BpWJSCJblFfAVU9PB4yJw/rRpmm9\noEtKWFUdEFsPdR2Fv24Lt+cBbcrt1zrc9l/c/Rl3z3H3nIyMjJgWKyKJZfa6nVwzajp1a1bnldtO\n48Tm9YMuKaFVdUBMAW4MP74ReKNc++DwaKb+QEG5rigRkaP618rt3DB6Jun1a/PybaeRnZ4WdEkJ\n76gBYWZXmlmD8OOfmdnkSIahmtkLwL+BTma20cxuBh4GzjezFcB54W2AqcBqYCUwCvifSr0bEUlJ\nHyzZytBxs2jTpB4v3dqfzMZ1gy4pKUQyIPjn7v6KmQ0g9KH+e0LDUPsd6UXufs03PHVuBfs6cEcE\ntYiIfM2b8zfxo5fm0bVVQ8bf1JfG9WoFXVLSiKSLqTT89SLgGXd/G9C/gIgE7qVZ67n7xbn0atuE\nicP6KRyiLJKAyDOzp4EfAFPNrHaErxMRiZkxn63hgdcWcmbHDMYP7UuDOroJLtoi+aC/CngXuMDd\ndwNNgftiWpWIyDdwd/784Yqvpux+ZnBv6tbS3EqxcMRrEGZWHZjj7p0PtYVHF2mEkYhUOXfnkX8s\n56mPV3FZz0x+d0V3TdkdQ0f8m3X3UmC5mWVVUT0iIhUqK3N+8cZinvp4Fdf3z+LRK3soHGIsklFM\nTYDFZjYT2Heo0d0vjVlVIiLllJSWcf9rC5g8J49bz2rPiIGdtUxoFYhomGvMqxAR+QZFJWXc8+Jc\n3lm0hXvPP4k7zzlR4VBFjhoQ7v6xmbUFOrr7B2ZWD9AVIRGJuQNFpdw2cTYff5HPzy/uws0DsoMu\nKaVEcif1LcCrwNPhpkzg77EsSkTky4MlDBk7k09W5PPI5acoHAIQyRWeO4DTgT0A7r4CaB7LokQk\nte3eX8R1z85g9rpdPHl1T37QR+NkghDJNYiD7l50qM/PzGpQiWm6RUQikb/3IDeMnsHq7ft46vre\nnNelxdFfJDERSUB8bGYPAnXN7HxCE+m9GduyRCQVbdp9gOuencGWgkLGDunD6SemB11SSouki2kE\nkA8sBG4lNPPqz2JZlIiknrXb93HlU/9m+5cHmTisr8IhDkQyiqnMzMYDMwh1LS0Pz74qIhIVy7fs\n5frRMygtc164pT/dMhsFXZIQQUCY2UXAU8AqwIBsM7vV3d+JdXEikvwWbNzN4DEzqV2jGs8P70/H\nFg2CLknCIrkG8RjwbXdfCWBmHYC3AQWEiByXWWt3MnTsLBrXq8nzw/qT1UzrR8eTSAJi76FwCFsN\n7I1RPSKSIj5dkc8tE3Jp1bguk4b144RGWgUu3nxjQJjZZeGHuWY2FXiZ0DWIK4FZVVCbiCSpdxdv\n4a7n59KheX2eu7kv6fVrB12SVOBIZxCXlHu8FTgr/DgfUNSLSKX8fW4e974yn+6tGzFuSF8a1dNC\nP/HqGwPC3YfG6qBm9iNgGKEzkoXAUOAE4EWgGTAbuMHdi2JVg4hUvednrOenf1/Iae2bMWpwDmm1\nI+nllqBEMhdTtpk9bmaTzWzKoT+VPaCZZQJ3Aznu3o3QxH9XA48Af3D3E4FdwM2VPYaIxJ9Rn6zm\nwdcX8u1OzRkzpI/CIQFE8i/0d2A0obuny6J43LpmVgzUI7RC3TnAteHnxwO/BP4WpeOJSEBKSsv4\n1VtLGP/vdVzU/QT+cNWp1KqhhX4SQSQBUejuf4zWAd09z8weBdYDB4D3CHUp7Xb3kvBuGwnNGisi\nCWxvYTF3vTCXfy7P59Yz2/PAwM5Uq6a1HBJFJAHxpJk9ROiD/OChRnefU5kDmlkTYBCQDewGXgEG\nHsPrhwPDAbKyNMOjSLzauGs/N4/LZVX+l/z2slO4pq9+XhNNJAFxCnADoS6gQ11MHt6ujPOANe6e\nD2BmkwlNJ97YzGqEzyJaA3kVvdjdnwGeAcjJydGUHyJxaO76XdwyYTYHS0oZf5PmVUpUkQTElUD7\nKI4oWg/0D69MdwA4F8gFPgKuIDSS6UbgjSgdT0Sq0FsLNnHvy/Np0bAOLw7vz4nN6wddklRSJFeK\nFgGNo3VAd59BaIW6OYSGuFYjdEbwAPBjM1tJaKjr6GgdU0Riz93584cruPP5uXRv3Yi/33G6wiHB\nRXIG0RhYZmaz+Po1iEsre1B3fwh46LDm1UDfyn5PEQnOwZJSRk5eyOQ5eXy/ZyYPX34KtWto6fpE\nF0lAHP5BLiLylZ37irjtudnMXLuTH59/EnedcyKHVqCUxBbJehAfV0UhIpJ4VuV/yU3jZrG5oJA/\nXtOTS3u0CrokiaJI1oPYy3/WoK4F1AT2uXvDWBYmIvHt81Xbue252dSsXo0XbulP77ZNgi5JoiyS\nM4ivVu+w0HnjIKB/LIsSkfj28qwNPPj6QtpnpDH6xj60aap1HJLRMd3v7iF/By6IUT0iEsfKypzf\nvrOU+19bwGkdmvHq7d9SOCSxSLqYLiu3WQ3IAQpjVpGIxKX9RSX86KV5vLt4K9f3z+KXl3SlRnXN\nqZTMIhnFVH5diBJgLaFuJhFJEVv3FDJsfC6LNxXwi4u7MPT0dhqplAIiuQYRs3UhRCT+Ld5UwLDx\nuRQcKGbU4BzOPblF0CVJFYmkiykDuAVoV35/d78pdmWJSDz4YMlW7n5xLo3q1uTV275Fl1YavJhK\nIuliegP4FPgAKI1tOSISD9yd0Z+t4TdTl3JKZiOeHZxD84Z1gi5LqlgkAVHP3R+IeSUiEheKS8v4\n5ZTFTJqxnoFdW/KHH5xK3VqaNiMVRRIQb5nZd919asyrEZFAFRwo5s7n5/Dpiu3cdlYH7r+gkxb4\nSWGRBMQ9wINmdhAoBozQLRHqjBRJIht27mfouFms3b6P313Rnaty2gRdkgTsmO6kFpHkNHvdToZP\nmE1JmTPh5r58q4MW+JHIziBEJIm9MS+P+15dQKtGdRgzpA/tM7SGg4QoIERSlLvz5LQVPPHBCvpm\nN+Xp63vTJK1W0GVJHFFAiKSgvYXFjJi8kLcXbObyXq35v8u6aYEf+S8RBYSZDQA6uvvY8I1z9d19\nTWxLE5FYWJRXwJ3Pz2HDrgOMuLAzt57ZXtNmSIUiuZP6IUIT9HUCxhJaD2IicHpsSxORaHJ3Js5Y\nz6/eXELTtFq8OLw/fdo1DbosiWORnEF8H+gJzAFw901mdlwjm8ysMfAs0I3QYkQ3AcuBlwhN6bEW\nuMrddx3PcUQkpHyX0tmdMnj8qlNpqusNchSRzNVb5O5OeFU5M0uLwnGfBP7h7p2BHsBSYAQwzd07\nAtPC2yJynBblFXDxnz7jH4u28MDAzoy5sY/CQSISyRnEy2b2NNDYzG4h9Nv+qMoe0MwaAWcCQwDc\nvQgoMrNBwNnh3cYD/wQ0xYdIJbk7E6ev41dvLVWXklRKJDfKPWpm5wN7CF2H+IW7v38cx8wG8oGx\nZtYDmE3obu0W7r45vM8WoMI5hc1sODAcICsr6zjKEEleewqLGfnaQt5euJlvd8rgMXUpSSVYqPco\ngh3NGvL16b53VuqAZjnAdOB0d59hZk8SCp+73L1xuf12ufsRV0HPycnx3NzcypQhkrQW5RVwx/Nz\n2LjrAPdd0InhZ7TXfEryNWY2291zjrZfJKOYbgX+H6FlRssIz8UEtK9kbRuBje4+I7z9KqHrDVvN\n7AR332xmJwDbKvn9RVJS+S6lZvVr8dLw/uSoS0mOQyTXIH4CdHP37dE4oLtvMbMNZtbJ3ZcD5wJL\nwn9uBB4Of30jGscTSQV7CosZ8doCpi7coi4liZpIAmIVsD/Kx70LmGRmtYDVwFBCI6peNrObgXXA\nVVE+pkhSWrgx1KWUtzt045u6lCRaIgmIkcDnZjYDOHio0d3vruxB3X0eoZvvDnduZb+nSKpxd56b\nvo5fq0tJYiSSgHga+BBYSOgahIgE7PAupcevOlUT7UnURRIQNd39xzGvREQiUr5LaeSFnblFXUoS\nI5EExDvhew/e5OtdTJUa5ioilePuTPj3On7z9lLS69fi5Vv707utupQkdiIJiGvCX0eWazueYa4i\ncoz2FBbzwKsLeGfRFs7p3JzHruyhLiWJuUjupM6uikJEpGLlu5Qe/G5nhg1Ql5JUjUhulKsJ3E5o\n/iQIzZH0tLsXx7AukZSnLiUJWiRdTH8jtAbEX8PbN4TbhsWqKJFUV3Ag1KX0j8XqUpLgRBIQfdy9\nR7ntD81sfqwKEkl1Czbu5o7n57B5d6G6lCRQkQREqZl1cPdVAGbWHiiNbVkiqcfdGf/5Wn4zdSkZ\n9Wvz0q2n0bvtEeerFImpSALiPuAjM1tNaKK+toSmxhCRKNmwcz8Pvr6QT1ds59zOzXlUXUoSByIZ\nxTTNzDoSWgsCYLm7HzzSa0QkMqVlzoR/r+X37y7HgF8N6sr1/dtipi4lCV4ko5iuJLQ86AIz+xnQ\ny8x+7e5zYl+eSPJauW0vD7y2kNnrdnHWSRn832WnkNm4btBliXwlki6mn7v7K2Y2gNBkeo8SGsXU\nL6aViSSp4tIynv54FX+ctpJ6tavz+FU9+H7PTJ01SNyJ6CJ1+OtFwCh3f9vMfh3DmkSS1qK8Au57\ndQFLN+/hou4n8MtLupLRoHbQZYlUKJKAyDOzp4HzgUfMrDahtRtEJEKFxaU88cEKRn26mqZptXj6\nht5c0LVl0GWJHFEkAXEVMBB41N13h5cDvS+2ZYkkjxmrdzBi8kLWbN/HD3La8OB3T6ZRvZpBlyVy\nVJGMYtoPTC63vRnYHMuiRJLB3sJifveP5Tw3fR2tm9Rl4s39GNAxPeiyRCIWyRmEiByjj5Zv46eT\nF7J5TyE3nZ7NTy44iXq19OMmiSWw/7FmVh3IBfLc/WIzywZeBJoBs4Eb3L0oqPpEKmPXviJ+9dYS\nJs/No2Pz+rx2+7folaW7oSUxBXmx+R5gabntR4A/uPuJwC7g5kCqEqkEd+etBZs47/GPmTJ/E3ef\ncyJv3T1A4SAJLZCAMLPWhIbNPhveNuAc4NXwLuOB7wVRm8ix2rqnkOHPzebO5+fSqnFd3rxrAD/+\nTidq16gedGkixyWoLqYngPuBBuHtZsBudy8Jb28EMoMoTCRS7s7LuRv49dtLKSopY+SFnbl5QDY1\nqmsUuCSHKg8IM7sY2Obus83s7Eq8fjgwHCArKyvK1YlEZv2O/YyYvIDPV+2gb3ZTHrm8O9npaUGX\nJRJVQZxBnA5cambfBeoADYEngcZmViN8FtEayKvoxe7+DPAMQE5OjldNySIhpWXOuM/X8ui7y6le\nzfj197pxbd8srdcgSanKA8LdRwIjAcJnED9x9+vM7BXgCkIjmW4E3qjq2kSO5Iute7n/1QXM27Cb\nb3fK4DffP4VWmlxPklg8Dcx+AHgxPM/TXGB0wPWIAFBUUsZTH6/iTx+uoH7tGjzxg1MZdGorTa4n\nSS/QgHD3fwL/DD9eDfQNsh6Rw83fsJsHXlvAsi17uaRHKx66pAvp9TW5nqSGeDqDEIkbebsP8Nh7\ny3l9bh7NG9Rm1OAczu/SIuiyRKqUAkKknD2Fxfz1o1WM+dcaAIaf2Z47vn0iDetocj1JPQoIEULX\nGSZOX8efPlzBrv3FXNYzkx9/5yRaN6kXdGkigVFASEpzd6Yu3MLv3l3Guh37Of3EZoy88GS6ZTYK\nujSRwCkgJGXNWruT37y9lHkbdtOpRQPGDu3D2SdlaHSSSJgCQlLOqvwveeSdZby3ZCstGtbmd5d3\n5/Leramum91EvkYBISkjf+9Bnpz2BS/M3ECdGtX4yXdO4qYB2VqnQeQb6CdDkt6BolKe/XQ1T328\nisKSMq7tm8U953XU/QwiR6GAkKRVWua8OnsDj7//BVv3HOSCri24f2BnOmTUD7o0kYSggJCk4+78\n84t8Hp66jOVb99IzqzF/vrYXfdo1Dbo0kYSigJCksiivgP+bupTPV+2gbbN6/PW6XlzYraVGJolU\nggJCksLGXft57L0veH1uHk3q1eShS7pwXb+21KqhxXtEKksBIQmt4EAxf/1oJWM/X4sBt5/dgdvP\n7qCpMUSiQAEhCelgSSkTp6/nTx+uoOBAMZf1bM293zlJ6zOIRJECQhKKu/PWgs387t1lbNh5gDM6\npjPiws50baWpMUSiTQEhCaGopIw3529i1KerWbZlL51bNmDCTX0586SMoEsTSVoKCIlrBfuLmTRz\nHeM/X8vWPQc5qUV9HruyB9/rmampMURiTAEhcWn9jv2M+dcaXs7dwP6iUgacmM4jl3fnLE2mJ1Jl\nFBASV+as38Wzn67mH4u2UL2acUmPVgwb0J4urRoGXZpIyqnygDCzNsAEoAXgwDPu/qSZNQVeAtoB\na4Gr3H1XVdcnVa+0zHl/yRZGfbqG2et20bBODW49qwM3ntaOlo3qBF2eSMoK4gyiBLjX3eeYWQNg\ntpm9DwwBprn7w2Y2AhgBPBBAfVJF9heV8EruRsb8aw3rduynTdO6PHRJF67KaUNabZ3cigStyn8K\n3X0zsDn8eK+ZLQUygUHA2eHdxgP/RAGRlLbtKWTc52uZNGM9BQeK6ZnVmAcGduaCri114VkkjgT6\na5qZtQN6AjOAFuHwANhCqAtKksiyLXsY9ckapszPo6TMuaBLS245M5vebTWJnkg8CiwgzKw+8Brw\nQ3ffU35kiru7mfk3vG44MBwgKyurKkqV4+DufLpiO6M+Xc2nK7ZTt2Z1ru2bxU0DsmnbLC3o8kTk\nCAIJCDOrSSgcJrn75HDzVjM7wd03m9kJwLaKXuvuzwDPAOTk5FQYIhK8gyWlTJm3idGfrWHZlr00\nb1Cb+y7oxHX9smhcr1bQ5YlIBIIYxWTAaGCpuz9e7qkpwI3Aw+Gvb1R1bXL8du8vYtKM9Yz7fC35\new/SuWUDHr2yB5f0OIHaNaoHXZ6IHIMgziBOB24AFprZvHDbg4SC4WUzuxlYB1wVQG1SSet27GP0\nZ2t4JXcjB4pLOaNjOo9d2YMzOqbrxjaRBBXEKKbPgG/6xDi3KmuR41NcWsanK/J5adYG3luylRrV\njEGnZjLsjGw6t9SNbSKJToPN5ZiUlTmz1u7kjfmbeGfhZnbtL6ZxvZr8z9mhG9uaN9SNbSLJQgEh\nR+XuLN60hynzN/Hm/E1sLiikbs3qnN+lBYNObcUZHTO0cptIElJAyDdanf8lU+ZvYsr8TazO30eN\nasbZnTIYcWFnzu/Sgnq19N9HJJnpJ1y+ZktBIW8t2MQb8zaxMK8AM+iX3ZRhA9pzYbeWNEnTEFWR\nVKGAEHbvL2Lqwi1MmZ/HjDU7cYdTMhvxs4tO5uLurTRhnkiKUkCkqP1FJby/ZCtT5m3ikxX5FJc6\n7TPSuOfcjlzaoxXtM+oHXaKIBEwBkUKKSsr45It8pszfxPtLtnKguJSWDesw9PRsLu3Riq6tGuqe\nBRH5igIiyZWVOTPW7GTK/DymLtxCwYHQsNTv98pkUI9W9GnXlGqaQVVEKqCASELuzqK8PbwxL483\nF2xi656D1KtVne90acGlp7ZiwIkalioiR6eASBJbCgqZvnoH01fv4PNVO1i/cz81qxtnndScn17U\nivNObq5hqSJyTPSJkaA2FxxgxuqdX4XC2h37AWhQpwb9spty+9kduLBbS82cKiKVpoBIEN8UCA3r\n1KBvdjOu79+W/u2bcfIJDbWWNgAPAAAG/UlEQVQqm4hEhQIiTm0uOBAKg1U7mb5mB+sUCCJSxRQQ\nceJogXCDAkFEqpgCIiCbdh9gxpqKA6Ff+2YMPq0d/ds3pXNLBYKIBEMBUUUUCCKSaBQQUVRUUsb6\nnftZs30fq/O/DH3dvo/V+fvY/uVBQIEgIolDAXGM3J2tew6yOv9LVm/f97Uw2LDrAKVl/tW+zdJq\nkZ2exjmdM+jUsqECQUQSigLiG+wpLGZN/n8C4FAYrNm+j/1FpV/tV6dmNbLT69O1VSMu7t6K9hlp\nZKen0T69Po3q1QzwHYiIHJ+4CwgzGwg8CVQHnnX3h2N1rENdQl91Bx0KhO3/6RICqGbQukk9stPT\n6JvdlPbpabTPqE92ehotG9bRXEYikpTiKiDMrDrwF+B8YCMwy8ymuPuSaB7no2Xb+OWbi9mwcz/l\neoRollaL9hmhLqFDAdA+PY2sZvWoXaN6NEsQEYl7cRUQQF9gpbuvBjCzF4FBQFQDomlaLbplNuLS\nHq1CIZBRn+xmaeoSEhEpJ94CIhPYUG57I9Cv/A5mNhwYDpCVlVWpg/Ro05i/XNurkiWKiKSGhJvz\n2d2fcfccd8/JyMgIuhwRkaQVbwGRB7Qpt9063CYiIlUs3gJiFtDRzLLNrBZwNTAl4JpERFJSXF2D\ncPcSM7sTeJfQMNcx7r444LJERFJSXAUEgLtPBaYGXYeISKqLty4mERGJEwoIERGpkAJCREQqZO5+\n9L3ilJnlA+sq+fJ0YHsUy4k3yfz+9N4SVzK/v0R6b23d/ag3kiV0QBwPM8t195yg64iVZH5/em+J\nK5nfXzK+N3UxiYhIhRQQIiJSoVQOiGeCLiDGkvn96b0lrmR+f0n33lL2GoSIiBxZKp9BiIjIEaRk\nQJjZQDNbbmYrzWxE0PVEi5m1MbOPzGyJmS02s3uCrinazKy6mc01s7eCriXazKyxmb1qZsvMbKmZ\nnRZ0TdFiZj8K/59cZGYvmFmdoGs6HmY2xsy2mdmicm1Nzex9M1sR/tokyBqjIeUCotyyphcCXYBr\nzKxLsFVFTQlwr7t3AfoDdyTRezvkHmBp0EXEyJPAP9y9M9CDJHmfZpYJ3A3kuHs3QhNxXh1sVcdt\nHDDwsLYRwDR37whMC28ntJQLCMota+ruRcChZU0Tnrtvdvc54cd7CX3AZAZbVfSYWWvgIuDZoGuJ\nNjNrBJwJjAZw9yJ33x1sVVFVA6hrZjWAesCmgOs5Lu7+CbDzsOZBwPjw4/HA96q0qBhIxYCoaFnT\npPkQPcTM2gE9gRnBVhJVTwD3A2VBFxID2UA+MDbchfasmaUFXVQ0uHse8CiwHtgMFLj7e8FWFRMt\n3H1z+PEWoEWQxURDKgZE0jOz+sBrwA/dfU/Q9USDmV0MbHP32UHXEiM1gF7A39y9J7CPJOiiAAj3\nxQ8iFIKtgDQzuz7YqmLLQ8NDE36IaCoGRFIva2pmNQmFwyR3nxx0PVF0OnCpma0l1C14jplNDLak\nqNoIbHT3Q2d8rxIKjGRwHrDG3fPdvRiYDHwr4JpiYauZnQAQ/rot4HqOWyoGRNIua2pmRqgPe6m7\nPx50PdHk7iPdvbW7tyP0b/ahuyfNb6HuvgXYYGadwk3nAksCLCma1gP9zaxe+P/ouSTJBfjDTAFu\nDD++EXgjwFqiIu5WlIu1JF/W9HTgBmChmc0Ltz0YXqVP4t9dwKTwLy6rgaEB1xMV7j7DzF4F5hAa\naTeXBL/r2MxeAM4G0s1sI/AQ8DDwspndTGiW6auCqzA6dCe1iIhUKBW7mEREJAIKCBERqZACQkRE\nKqSAEBGRCikgRESkQgoIkYCY2dnJOCutJA8FhIiIVEgBIXIUZna9mc00s3lm9nR4TYovzewP4TUO\npplZRnjfU81supktMLPXD60JYGYnmtkHZjbfzOaYWYfwt69fbg2ISeE7jUXiggJC5AjM7GTgB8Dp\n7n4qUApcB6QBue7eFfiY0J20ABOAB9y9O7CwXPsk4C/u3oPQPESHZv3sCfyQ0Nok7QndDS8SF1Ju\nqg2RY3Qu0BuYFf7lvi6hSdjKgJfC+0wEJofXdGjs7h+H28cDr5hZAyDT3V8HcPdCgPD3m+nuG8Pb\n84B2wGexf1siR6eAEDkyA8a7+8ivNZr9/LD9KjtnzcFyj0vRz6TEEXUxiRzZNOAKM2sOX6073JbQ\nz84V4X2uBT5z9wJgl5mdEW6/Afg4vLrfRjP7Xvh71DazelX6LkQqQb+tiByBuy8xs58B75lZNaAY\nuIPQgj59w89tI3SdAkLTPD8VDoDyM7LeADxtZv8b/h5XVuHbEKkUzeYqUglm9qW71w+6DpFYUheT\niIhUSGcQIiJSIZ1BiIhIhRQQIiJSIQWEiIhUSAEhIiIVUkCIiEiFFBAiIlKh/w9thayuRTS3dgAA\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zj48yJ_nKM3k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "60542c42-ae6a-4e68-e801-62e274fc0656"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "l1=np.array(loss1_values)\n",
        "l2=np.array(loss2_values)\n",
        "l3=l1+l2\n",
        "\n",
        "print(l1[1],l2[1],l3[1])\n",
        "\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.plot(l1)\n",
        "plt.plot(l2)\n",
        "plt.plot(l3)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1.5776, grad_fn=<MeanBackward0>) tensor(0.9444, grad_fn=<MeanBackward0>) tensor(2.5221, grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f17157db048>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoFQpPdIwded",
        "colab_type": "text"
      },
      "source": [
        "Here we plot the mean of two loss components separately along with their sum !!"
      ]
    }
  ]
}