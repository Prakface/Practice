{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Appending_independent_training_weights_data_loader.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prakface/Practice/blob/master/Appending_independent_training_weights_data_loader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dc2TTUcJWg_p",
        "colab_type": "code",
        "outputId": "94ba8c30-0c42-4cad-a293-16ce4fcc30da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "url='https://raw.githubusercontent.com/Prakface/Practice/master/One_mon_present_full.csv'\n",
        "\n",
        "url2='https://raw.githubusercontent.com/Prakface/Practice/master/Final_one_month_prev_features.csv'\n",
        "\n",
        "data = pd.read_csv(url) \n",
        "\n",
        "data2= pd.read_csv(url2)\n",
        "\n",
        "print(\"Data Shape:\", data.shape) \n",
        "\n",
        "#data=pd.read_csv(url)\n",
        "\n",
        "df=pd.DataFrame(data)\n",
        "print(data.head()) \n",
        "\n",
        "\n",
        "data_modified= data.dropna()\n",
        "\n",
        "data_modified.to_csv(\"modifiedData.csv\", index=False)\n",
        "\n",
        "\n",
        "df2=pd.read_csv(\"modifiedData.csv\")\n",
        "\n",
        "print(df2[0:6])\n",
        "\n",
        "print(df2['result'])\n",
        "\n",
        "df_main=df2[df2.columns[~df2.columns.isin(['text', 'user_name'])]]\n",
        "\n",
        "print(df_main.columns)\n",
        "\n",
        "print(len(df_main.columns))\n",
        "\n",
        "  \n",
        "# X_1, y_1 means rpesent tweets' data\n",
        "X_1=df_main[df_main.columns[~df_main.columns.isin(['time', 'tweet_id','result'])]] #removing result attribute as it is class label, hence we get 34 attributes\n",
        "X_1=X_1.iloc[:,1:len(X_1.columns)].values   #removing the unnamed attribute\n",
        "x_1=df_main[df_main.columns[~df_main.columns.isin(['time', 'tweet_id','result'])]]\n",
        "x_1=x_1.iloc[:,1:len(x_1.columns)].values \n",
        "y_1=df_main.loc[:, ['result']].values\n",
        "\n",
        "\n",
        "print(type(X_1), type(y_1), type(x_1), type(y_1))\n",
        "\n",
        "print(X_1.shape)\n",
        "print(y_1.shape)\n",
        "\n",
        "\n"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Shape: (1908, 40)\n",
            "  Unnamed: 0 cat1  cat10  ...      tweet_id  url      user_name\n",
            "0          0    0      0  ...  8.323790e+17  0.0  THEJEROMEOWEN\n",
            "1          1    0      0  ...  8.323786e+17  0.0       Acejinjo\n",
            "2          2    0      0  ...  8.323780e+17  0.0     RabRakha21\n",
            "3          3    0      0  ...  8.323777e+17  0.0       RS_Aloha\n",
            "4          4    0      0  ...  8.323767e+17  0.0  preciselyizzy\n",
            "\n",
            "[5 rows x 40 columns]\n",
            "   Unnamed: 0  cat1  cat10  ...      tweet_id  url        user_name\n",
            "0           0     0      0  ...  8.323790e+17  0.0    THEJEROMEOWEN\n",
            "1           1     0      0  ...  8.323786e+17  0.0         Acejinjo\n",
            "2           2     0      0  ...  8.323780e+17  0.0       RabRakha21\n",
            "3           3     0      0  ...  8.323777e+17  0.0         RS_Aloha\n",
            "4           4     0      0  ...  8.323767e+17  0.0    preciselyizzy\n",
            "5           5     0      0  ...  8.323759e+17  0.0  thefireistarted\n",
            "\n",
            "[6 rows x 40 columns]\n",
            "0       1.0\n",
            "1       1.0\n",
            "2       1.0\n",
            "3       1.0\n",
            "4       1.0\n",
            "5       1.0\n",
            "6       1.0\n",
            "7       1.0\n",
            "8       1.0\n",
            "9       1.0\n",
            "10      1.0\n",
            "11      1.0\n",
            "12      1.0\n",
            "13      1.0\n",
            "14      1.0\n",
            "15      1.0\n",
            "16      1.0\n",
            "17      1.0\n",
            "18      1.0\n",
            "19      1.0\n",
            "20      1.0\n",
            "21      1.0\n",
            "22      1.0\n",
            "23      1.0\n",
            "24      1.0\n",
            "25      1.0\n",
            "26      1.0\n",
            "27      1.0\n",
            "28      1.0\n",
            "29      1.0\n",
            "       ... \n",
            "1876    0.0\n",
            "1877    0.0\n",
            "1878    0.0\n",
            "1879    0.0\n",
            "1880    0.0\n",
            "1881    0.0\n",
            "1882    0.0\n",
            "1883    0.0\n",
            "1884    0.0\n",
            "1885    0.0\n",
            "1886    0.0\n",
            "1887    0.0\n",
            "1888    0.0\n",
            "1889    0.0\n",
            "1890    0.0\n",
            "1891    0.0\n",
            "1892    0.0\n",
            "1893    0.0\n",
            "1894    0.0\n",
            "1895    0.0\n",
            "1896    0.0\n",
            "1897    0.0\n",
            "1898    0.0\n",
            "1899    0.0\n",
            "1900    0.0\n",
            "1901    0.0\n",
            "1902    0.0\n",
            "1903    0.0\n",
            "1904    0.0\n",
            "1905    0.0\n",
            "Name: result, Length: 1906, dtype: float64\n",
            "Index(['Unnamed: 0', 'cat1', 'cat10', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6',\n",
            "       'cat7', 'cat8', 'cat9', 'favorite_count', 'hour', 'image', 'level',\n",
            "       'nadj', 'nadv', 'nemoji', 'nlevel', 'nword', 'orginal', 'padj', 'padv',\n",
            "       'pemoji', 'plevel', 'pnoun', 'punc1', 'punc2', 'punc3', 'pword',\n",
            "       'question', 'result', 'retweets_count', 'sarcasm', 'sentiment', 'time',\n",
            "       'tweet_id', 'url'],\n",
            "      dtype='object')\n",
            "38\n",
            "<class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
            "(1906, 34)\n",
            "(1906, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "on-MaTcOW2aY",
        "colab_type": "code",
        "outputId": "1483d095-1440-4eeb-efd9-326d1c49a010",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "data2= pd.read_csv(url2)\n",
        "\n",
        "print(\"Data Shape:\", data2.shape) \n",
        "\n",
        "#data=pd.read_csv(url)\n",
        "\n",
        "df_prev=pd.DataFrame(data2)\n",
        "print(data2.head()) \n",
        "\n",
        "\n",
        "data2_modified= data2.dropna()\n",
        "\n",
        "data2_modified.to_csv(\"modifiedData2.csv\", index=False)\n",
        "\n",
        "\n",
        "df_2=pd.read_csv(\"modifiedData2.csv\")\n",
        "\n",
        "print(df_2[0:6])\n",
        "\n",
        "print(df_2['result'])\n",
        "\n",
        "df_main2=df_2[df_2.columns[~df_2.columns.isin(['text', 'user_name'])]]\n",
        "\n",
        "print(df_main2.columns)\n",
        "\n",
        "print(len(df_main2.columns))\n",
        "\n",
        "  \n",
        "\n",
        "X_2=df_main2[df_main2.columns[~df_main2.columns.isin(['time', 'tweet_id','result'])]] #removing result attribute as it is class label, hence we get 34 attributes\n",
        "X_2=X_2.iloc[:,1:len(X_2.columns)].values   #removing the unnamed attribute\n",
        "x_2=df_main2[df_main2.columns[~df_main2.columns.isin(['time', 'tweet_id','result'])]]\n",
        "x_2=x_2.iloc[:,1:len(x_2.columns)].values \n",
        "y_2=df_main2.loc[:, ['result']].values\n",
        "\n",
        "\n",
        "print(type(X_2), type(y_2), type(x_2), type(y_2))\n",
        "\n",
        "print(X_2.shape)\n",
        "print(y_2.shape)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Shape: (3004, 38)\n",
            "   Unnamed: 0  cat1  cat10  ...             tweet_id  url      user_name\n",
            "0           0     0      0  ...  1155575657402961920    1  THEJEROMEOWEN\n",
            "1           1     0      0  ...  1155459426243043328    0  THEJEROMEOWEN\n",
            "2           2     0      0  ...  1126969730307448832    0     rabrakha14\n",
            "3           3     0      0  ...  1155277550794338304    0       RS_Aloha\n",
            "4           4     0      0  ...  1155188179395207168    0       RS_Aloha\n",
            "\n",
            "[5 rows x 38 columns]\n",
            "   Unnamed: 0  cat1  cat10  ...             tweet_id  url      user_name\n",
            "0           0     0      0  ...  1155575657402961920    1  THEJEROMEOWEN\n",
            "1           1     0      0  ...  1155459426243043328    0  THEJEROMEOWEN\n",
            "2           2     0      0  ...  1126969730307448832    0     rabrakha14\n",
            "3           3     0      0  ...  1155277550794338304    0       RS_Aloha\n",
            "4           4     0      0  ...  1155188179395207168    0       RS_Aloha\n",
            "5           5     0      0  ...  1154962871765393408    0  preciselyizzy\n",
            "\n",
            "[6 rows x 38 columns]\n",
            "0       0\n",
            "1       0\n",
            "2       0\n",
            "3       0\n",
            "4       0\n",
            "5       0\n",
            "6       0\n",
            "7       0\n",
            "8       0\n",
            "9       0\n",
            "10      0\n",
            "11      0\n",
            "12      0\n",
            "13      0\n",
            "14      0\n",
            "15      0\n",
            "16      0\n",
            "17      0\n",
            "18      0\n",
            "19      0\n",
            "20      0\n",
            "21      0\n",
            "22      0\n",
            "23      0\n",
            "24      0\n",
            "25      0\n",
            "26      0\n",
            "27      0\n",
            "28      0\n",
            "29      0\n",
            "       ..\n",
            "2974    0\n",
            "2975    0\n",
            "2976    0\n",
            "2977    0\n",
            "2978    0\n",
            "2979    0\n",
            "2980    0\n",
            "2981    0\n",
            "2982    0\n",
            "2983    0\n",
            "2984    0\n",
            "2985    0\n",
            "2986    0\n",
            "2987    0\n",
            "2988    0\n",
            "2989    0\n",
            "2990    0\n",
            "2991    0\n",
            "2992    0\n",
            "2993    0\n",
            "2994    0\n",
            "2995    0\n",
            "2996    0\n",
            "2997    0\n",
            "2998    0\n",
            "2999    0\n",
            "3000    0\n",
            "3001    0\n",
            "3002    0\n",
            "3003    0\n",
            "Name: result, Length: 3004, dtype: int64\n",
            "Index(['Unnamed: 0', 'cat1', 'cat10', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6',\n",
            "       'cat7', 'cat8', 'cat9', 'favorite_count', 'hour', 'image', 'level',\n",
            "       'nadj', 'nadv', 'nemoji', 'nlevel', 'nword', 'orginal', 'padj', 'padv',\n",
            "       'pemoji', 'plevel', 'pnoun', 'punc1', 'punc2', 'punc3', 'pword',\n",
            "       'question', 'result', 'retweets_count', 'sarcasm', 'sentiment',\n",
            "       'tweet_id', 'url'],\n",
            "      dtype='object')\n",
            "37\n",
            "<class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
            "(3004, 34)\n",
            "(3004, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipbi41taW-UC",
        "colab_type": "code",
        "outputId": "a446f1a8-98f8-434a-b87d-4d8f0c51de0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "#Appending present and previosu data\n",
        "\n",
        "tem=np.append(X_1, X_2, axis=0)\n",
        "tem_y=np.append(y_1,y_2,axis=0)\n",
        "print(X_1.shape, X_2.shape, tem.shape)\n",
        "print(y_1.shape, y_2.shape, tem_y.shape)\n",
        "\n",
        "X=tem\n",
        "T=tem_y\n",
        "\n",
        "print(X.shape, T.shape)\n",
        "\n",
        "print(type(X_1))\n",
        "#convert to tensor\n",
        "X = torch.from_numpy(X)\n",
        "T = torch.from_numpy(T)\n",
        "\n",
        "X_1 = torch.from_numpy(X_1)\n",
        "T_1 = torch.from_numpy(y_1)\n",
        "\n",
        "X_2 = torch.from_numpy(X_2)\n",
        "T_2 = torch.from_numpy(y_2)\n",
        "\n",
        "print(type(X), type(T))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1906, 34) (3004, 34) (4910, 34)\n",
            "(1906, 1) (3004, 1) (4910, 1)\n",
            "(4910, 34) (4910, 1)\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'torch.Tensor'> <class 'torch.Tensor'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h765PsJvW_8Q",
        "colab_type": "code",
        "outputId": "e62df55f-8330-4013-99eb-e4cc7c79c81b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torch.nn.functional as F\n",
        "\n",
        "## create training and validation split \n",
        "split_size = int(0.8 * len(X_1))\n",
        "index_list = list(range(len(X_1)))\n",
        "train_idx, valid_idx = index_list[:split_size], index_list[split_size:]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "x_tr = torch.tensor(X[train_idx], dtype=torch.long)\n",
        "y_tr = torch.tensor(T[train_idx], dtype=torch.float32)\n",
        "train = TensorDataset(x_tr, y_tr)\n",
        "trainloader = DataLoader(train, batch_size=128)\n",
        "'''\n",
        "\n",
        "x_tr = torch.tensor(X_1[train_idx], dtype=torch.long)\n",
        "y_tr = torch.tensor(T_1[train_idx], dtype=torch.float32)\n",
        "train = TensorDataset(x_tr, y_tr)\n",
        "trainloader = DataLoader(train, batch_size=128)\n",
        "\n",
        "x_valid = torch.tensor(X_1[valid_idx], dtype=torch.long)\n",
        "y_valid = torch.tensor(T_1[valid_idx], dtype=torch.float32)\n",
        "valid = TensorDataset(x_valid, y_valid)\n",
        "validloader = DataLoader(valid, batch_size=128)\n",
        "\n",
        "x_tr2 = torch.tensor(X_2[train_idx], dtype=torch.long)\n",
        "y_tr2 = torch.tensor(T_2[train_idx], dtype=torch.float32)\n",
        "train2 = TensorDataset(x_tr2, y_tr2)\n",
        "trainloader2 = DataLoader(train2, batch_size=128)\n",
        "\n",
        "\n",
        "\n",
        "##Following is Auxiliary data set\n",
        "\n",
        "\n",
        "split_size = int(0.8 * len(X_2))\n",
        "index_list = list(range(len(X_2)))\n",
        "train_idx, valid_idx = index_list[:split_size], index_list[split_size:]\n",
        "\n",
        "\n",
        "x_tr2 = torch.tensor(X_2[train_idx], dtype=torch.long)\n",
        "y_tr2 = torch.tensor(T_2[train_idx], dtype=torch.float32)\n",
        "train2 = TensorDataset(x_tr2, y_tr2)\n",
        "trainloader2 = DataLoader(train2, batch_size=128)\n",
        "\n",
        "x_valid2 = torch.tensor(X_2[valid_idx], dtype=torch.long)\n",
        "y_valid2 = torch.tensor(T_2[valid_idx], dtype=torch.float32)\n",
        "valid2 = TensorDataset(x_valid2, y_valid2)\n",
        "validloader = DataLoader(valid2, batch_size=128)\n",
        "\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_Ew4z3bkfi1",
        "colab_type": "code",
        "outputId": "7bb4f8e9-9b7c-4821-d569-cf886540e8d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "##Concatenating the tensors\n",
        "\n",
        "import torch as pytorch\n",
        "\n",
        "pytorch.set_default_tensor_type('torch.DoubleTensor')\n",
        "\n",
        "X_1=X_1.type(torch.DoubleTensor)\n",
        "T_1=T_1.type(torch.DoubleTensor)\n",
        "\n",
        "X_2=X_2.type(torch.DoubleTensor)\n",
        "T_2=T_2.type(torch.DoubleTensor)\n",
        "\n",
        "\n",
        "X=torch.cat((X_1,X_2), 0)\n",
        "\n",
        "print(X.shape)\n",
        "\n",
        "X_tr=torch.cat((x_tr,x_tr2),0)\n",
        "\n",
        "print(X_tr.shape)\n",
        "\n",
        "Y_tr=torch.cat((y_tr,y_tr2),0)  ##y_tr2 doesn't have values, we should calculate using heuristics\n",
        "\n",
        "X_valid=torch.cat((x_valid,x_valid2),0)\n",
        "\n",
        "Y_valid=torch.cat((y_valid,y_valid2),0)\n",
        "\n",
        "Train=TensorDataset(X_tr, Y_tr)\n",
        "TrainLoader=DataLoader(Train, batch_size=128)\n",
        "\n",
        "\n",
        "Valid=TensorDataset(X_valid, Y_valid)\n",
        "ValidLoader=DataLoader(Valid, batch_size=128)\n",
        "\n",
        "\n",
        "print(X_tr.type())\n",
        "\n",
        "X_tr=torch.tensor(X_tr, dtype=torch.double)\n",
        "\n",
        "X_valid=torch.tensor(X_valid, dtype=torch.double)\n",
        "\n",
        "print(X_tr.type(), X_valid.type())"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4910, 34])\n",
            "torch.Size([3927, 34])\n",
            "torch.LongTensor\n",
            "torch.DoubleTensor torch.DoubleTensor\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjGSNl9QWvEH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 50\n",
        "n_iters = 3000\n",
        "epochs = n_iters / (len(X_1) / batch_size)\n",
        "epochs2= n_iters / (len(X_2) / batch_size)\n",
        "#epochs= n_iters/((len(X_1)+len(X_2))/batch_size)\n",
        "input_dim = 34\n",
        "output_dim = 2\n",
        "lr_rate = 0.001\n",
        "\n",
        "class LogisticRegression(torch.nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
        "        #self.linear = torch.nn.Linear(2*input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = self.linear(x)\n",
        "        return outputs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLNU1iJhaNHM",
        "colab_type": "code",
        "outputId": "25ef30b7-eb3f-45ab-e536-f56a991998a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "model=LogisticRegression(input_dim, output_dim)\n",
        "print(type(model.parameters()))\n",
        "print(model.parameters())\n",
        "#a=model.parameters()\n",
        "#len(a)\n",
        "\n",
        "model.parameters"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'generator'>\n",
            "<generator object Module.parameters at 0x7f13bdeb4c50>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.parameters of LogisticRegression(\n",
              "  (linear): Linear(in_features=34, out_features=2, bias=True)\n",
              ")>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGXgLCU9X4_b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "5f65944d-2828-4d49-a8c3-5a28416c5a18"
      },
      "source": [
        "model = LogisticRegression(input_dim, output_dim)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss() # computes softmax and then the cross entropy\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr_rate)\n",
        "\n",
        "##Training the data\n",
        "iterations = 0\n",
        "for epoch in range(int(epochs)):\n",
        "    for i, (data_X, labels) in enumerate(trainloader):\n",
        "        data_X = Variable(data_X.view(-1,34))\n",
        "        labels = Variable(labels)\n",
        "        labels=torch.tensor(labels, dtype=torch.long)\n",
        "        #print(labels.__class__)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        data_X=torch.tensor(data_X, dtype=torch.double)\n",
        "        #print(data_X.shape, data_X.type())\n",
        "        \n",
        "        outputs = model(data_X)\n",
        "        labels = labels.squeeze_()\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        iterations+=1\n",
        "        if iterations%100==0:\n",
        "            # calculate Accuracy\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            for data_X, labels in validloader:\n",
        "                data_X = Variable(data_X.view(-1, 34))\n",
        "                \n",
        "                data_X=torch.tensor(data_X, dtype=torch.double)\n",
        "                labels=torch.tensor(labels, dtype=torch.long)\n",
        "                \n",
        "                outputs = model(data_X)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total+= labels.size(0)\n",
        "                # for gpu, bring the predicted and labels back to cpu fro python operations to work\n",
        "                pr=predicted.numpy()\n",
        "                lb=labels.numpy()\n",
        "                for i in range(len(pr)):\n",
        "                  if(pr[i]==lb[i]):\n",
        "                    correct = correct+ 1\n",
        "                    \n",
        "                #correct+= (predicted == labels).sum()\n",
        "            accuracy = 100 * correct/total\n",
        "            print(\"Iteration: {}. Loss: {}.Correct:{}. total:{}. Accuracy: {}.\".format(iterations, loss.item(), correct, total,  accuracy))"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: 100. Loss: 0.8344388483572802.Correct:94. total:601. Accuracy: 15.640599001663894.\n",
            "Iteration: 200. Loss: 1.1599956079003768.Correct:79. total:601. Accuracy: 13.144758735440933.\n",
            "Iteration: 300. Loss: 0.7168058831779667.Correct:233. total:601. Accuracy: 38.76871880199667.\n",
            "Iteration: 400. Loss: 0.7135340330267647.Correct:76. total:601. Accuracy: 12.645590682196339.\n",
            "Iteration: 500. Loss: 1.0840634899678798.Correct:53. total:601. Accuracy: 8.81863560732113.\n",
            "Iteration: 600. Loss: 0.7197903935956562.Correct:293. total:601. Accuracy: 48.75207986688852.\n",
            "Iteration: 700. Loss: 0.6306900215249203.Correct:60. total:601. Accuracy: 9.983361064891847.\n",
            "Iteration: 800. Loss: 1.041728365646043.Correct:39. total:601. Accuracy: 6.4891846921797.\n",
            "Iteration: 900. Loss: 0.7219927378307744.Correct:313. total:601. Accuracy: 52.079866888519135.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtVsgXN6hj75",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4PYWKKWdxXa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6L8iJFTdxuB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#####Using different weights for previous tweets and defining loss via adding different forward function\n",
        "\n",
        "batch_size = 50\n",
        "n_iters = 3000\n",
        "epochs = n_iters / (len(X_1) / batch_size)\n",
        "epochs2= n_iters / (len(X_2) / batch_size)\n",
        "#epochs= n_iters/((len(X_1)+len(X_2))/batch_size)\n",
        "input_dim = 34\n",
        "output_dim = 2\n",
        "lr_rate = 0.001\n",
        "\n",
        "class LogisticRegression(torch.nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
        "        #self.linear = torch.nn.Linear(2*input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        out1 = self.linear(x1)\n",
        "        out2 = self.linear(x2)\n",
        "        outputs=(out1+out2)/2\n",
        "        return outputs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vkm2DexPefWY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AQOKSehhj_c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e8682cd0-94aa-43d5-ed7e-ab326a845edd"
      },
      "source": [
        "import torch as pytorch\n",
        "import numpy as np\n",
        "\n",
        "pytorch.set_default_tensor_type('torch.DoubleTensor')\n",
        "\n",
        "X_1=X_1.type(torch.DoubleTensor)\n",
        "T_1=T_1.type(torch.DoubleTensor)\n",
        "\n",
        "\n",
        "X_2=X_2.type(torch.DoubleTensor)\n",
        "T_2=T_2.type(torch.DoubleTensor)\n",
        "\n",
        "W = Variable(torch.randn(1, 34, dtype=torch.double), requires_grad=True)\n",
        "b = Variable(torch.randn(1, 1, dtype=torch.double), requires_grad=True)\n",
        "print(W.size())\n",
        "\n",
        "#Weights for Previoes tweets\n",
        "#W1 = Variable(torch.randn(1, 34, dtype=torch.double), requires_grad=True)\n",
        "#W_p = Variable(torch.randn(1, 34, dtype=torch.double), requires_grad=True)\n",
        "#W_p=W_p.type(torch.LongTensor)\n",
        "#b_p = Variable(torch.randn(1, 1, dtype=torch.double), requires_grad=True)\n",
        "#print(W_p.size(), torch.Tensor(W_p).dtype)\n",
        "#print(W.size(), torch.Tensor(W_p).dtype)\n",
        "\n",
        "\n",
        "#sigmoid\n",
        "def sigmoid(x):\n",
        "    return 1/(1+torch.exp(-x))\n",
        "\n",
        "#out = sigmoid(torch.mm(X, W.view(34,1))+b)\n",
        "\n",
        "out_present= sigmoid(torch.mm(X_1, W.view(34,1))+b)\n",
        "\n",
        "#out_prev= sigmoid(torch.mm(X_2, W_p.view(34,1))+b_p)\n",
        "out_prev_same_weight=sigmoid(torch.mm(X_2,W.view(34,1))+b)\n",
        "\n",
        "'''\n",
        "\n",
        "def my_loss(out1, target1, out2, target2,cnt):\n",
        "  print(out1.type(),len(out1), target1.type() ,out2.type(),len(out2), target2.type())\n",
        "  target1=target1.numpy()\n",
        "  target2=target2.numpy()\n",
        "  #out1=out1.numpy(), we can't call numpy on out1 and out2 because we can't call numpy on var that require grad\n",
        "  #out2=out2.numpy()\n",
        "  print(\"\\n After numpy conversion \\n\", len(target1), len(target2))\n",
        "  \n",
        "  l1=-target1 * torch.log(out1) - (1-target1) * torch.log(1-out1)\n",
        "  \n",
        "  #print(target1.type(), out1.type())\n",
        "  l2=-target2 * torch.log(out2) - (1-target2) * torch.log(1-out2)\n",
        "  \n",
        "  return torch.tensor(np.mean(l1+l2))\n",
        "  \n",
        "'''\n",
        "  \n",
        "  \n",
        "def my_loss(out1, target1, out2, target2):\n",
        "  target1=torch.tensor(target1, dtype=torch.double)\n",
        "  target2=torch.tensor(target2, dtype=torch.double)\n",
        "\n",
        "  \n",
        "  #print(out1.type(),len(out1), target1.type(),len(target1) ,out2.type(),len(out2), target2.type(),len(target2))\n",
        "  l1=-target1 * torch.log(out1) - (1-target1) * torch.log(1-out1)\n",
        "  \n",
        "  #print(target1.type(), out1.type())\n",
        "  l2=-target2 * torch.log(out2) - (1-target2) * torch.log(1-out2)\n",
        "  return torch.mean(l1+l2)\n",
        "\n",
        "  "
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 34])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsPtk7WFfE0g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "559ac66a-24b9-483f-91af-79d50f31b37b"
      },
      "source": [
        "model = LogisticRegression(input_dim, output_dim)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss() # computes softmax and then the cross entropy\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr_rate)\n",
        "\n",
        "##Training the data\n",
        "iterations = 0\n",
        "for epoch in range(int(epochs)):\n",
        "    cnt=0\n",
        "    for  (data_X, labels,x2,y2) in zip(x_tr,y_tr,x_tr2,y_tr2):\n",
        "        data_X = Variable(data_X.view(-1,34))\n",
        "        labels = Variable(labels)\n",
        "        \n",
        "        x2 = Variable(data_X.view(-1,34))\n",
        "        y2 = Variable(y2)\n",
        "        \n",
        "        data_X=torch.tensor(data_X, dtype=torch.double)\n",
        "        labels=torch.tensor(labels, dtype=torch.long)\n",
        "        \n",
        "        x2=torch.tensor(x2,dtype=torch.double)\n",
        "        y2=torch.tensor(y2,dtype=torch.long)\n",
        "        \n",
        "       \n",
        "        #print(labels.__class__)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        #print(data_X.shape, data_X.type())\n",
        "        \n",
        "        outputs = model(data_X)\n",
        "        #labels = labels.squeeze_()\n",
        "        \n",
        "        out1=model(data_X)\n",
        "        out2=model(x2)\n",
        "        \n",
        "        #label_val=torch.tensor(labels[cnt])\n",
        "        #y2_val=torch.tensor(y2[cnt])\n",
        "        \n",
        "        #y2=y2.squeeze_()\n",
        "        #loss = criterion(outputs, labels)\n",
        "        #loss=my_loss(out1,label_val,out2,y2_val)\n",
        "        #loss=my_loss(out1,labels,out2,y2,cnt)\n",
        "        loss=my_loss(out1,labels,out2,y2)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        cnt+=1\n",
        "\n",
        "        iterations+=1\n",
        "        \n",
        "        if iterations%100==0:\n",
        "            # calculate Accuracy\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            for data_X, labels,x2,y2 in zip(x_valid,y_valid, x_valid2,y_valid2):\n",
        "                data_X = Variable(data_X.view(-1, 34))\n",
        "                \n",
        "                data_X=torch.tensor(data_X, dtype=torch.double)\n",
        "                labels=torch.tensor(labels, dtype=torch.long)\n",
        "                \n",
        "                x2 = Variable(data_X.view(-1,34))\n",
        "                y2 = Variable(y2)\n",
        "                \n",
        "                x2=torch.tensor(x2,dtype=torch.double)\n",
        "                y2=torch.tensor(y2, dtype=torch.long)\n",
        "                \n",
        "                outputs = model(data_X)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total+= labels.size(0)\n",
        "                # for gpu, bring the predicted and labels back to cpu fro python operations to work\n",
        "                pr=predicted.numpy()\n",
        "                lb=labels.numpy()\n",
        "                for i in range(len(pr)):\n",
        "                  if(pr[i]==lb[i]):\n",
        "                    correct = correct+ 1\n",
        "                    \n",
        "                #correct+= (predicted == labels).sum()\n",
        "            accuracy = 100 * correct/total\n",
        "            print(\"Iteration: {}. Loss: {}.Correct:{}. total:{}. Accuracy: {}.\".format(iterations, loss.item(), correct, total,  accuracy))"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: 100. Loss: nan.Correct:3. total:382. Accuracy: 0.7853403141361257.\n",
            "Iteration: 200. Loss: nan.Correct:0. total:382. Accuracy: 0.0.\n",
            "Iteration: 300. Loss: nan.Correct:0. total:382. Accuracy: 0.0.\n",
            "Iteration: 400. Loss: nan.Correct:0. total:382. Accuracy: 0.0.\n",
            "Iteration: 500. Loss: nan.Correct:0. total:382. Accuracy: 0.0.\n",
            "Iteration: 600. Loss: nan.Correct:5. total:382. Accuracy: 1.3089005235602094.\n",
            "Iteration: 700. Loss: nan.Correct:7. total:382. Accuracy: 1.8324607329842932.\n",
            "Iteration: 800. Loss: nan.Correct:7. total:382. Accuracy: 1.8324607329842932.\n",
            "Iteration: 900. Loss: nan.Correct:7. total:382. Accuracy: 1.8324607329842932.\n",
            "Iteration: 1000. Loss: nan.Correct:7. total:382. Accuracy: 1.8324607329842932.\n",
            "Iteration: 1100. Loss: nan.Correct:7. total:382. Accuracy: 1.8324607329842932.\n",
            "Iteration: 1200. Loss: nan.Correct:0. total:382. Accuracy: 0.0.\n",
            "Iteration: 1300. Loss: nan.Correct:0. total:382. Accuracy: 0.0.\n",
            "Iteration: 1400. Loss: nan.Correct:0. total:382. Accuracy: 0.0.\n",
            "Iteration: 1500. Loss: nan.Correct:0. total:382. Accuracy: 0.0.\n",
            "Iteration: 1600. Loss: nan.Correct:0. total:382. Accuracy: 0.0.\n",
            "Iteration: 1700. Loss: nan.Correct:0. total:382. Accuracy: 0.0.\n",
            "Iteration: 1800. Loss: nan.Correct:0. total:382. Accuracy: 0.0.\n",
            "Iteration: 1900. Loss: nan.Correct:0. total:382. Accuracy: 0.0.\n",
            "Iteration: 2000. Loss: nan.Correct:0. total:382. Accuracy: 0.0.\n",
            "Iteration: 2100. Loss: nan.Correct:0. total:382. Accuracy: 0.0.\n",
            "Iteration: 2200. Loss: nan.Correct:0. total:382. Accuracy: 0.0.\n",
            "Iteration: 2300. Loss: nan.Correct:0. total:382. Accuracy: 0.0.\n",
            "Iteration: 2400. Loss: nan.Correct:0. total:382. Accuracy: 0.0.\n",
            "Iteration: 2500. Loss: nan.Correct:0. total:382. Accuracy: 0.0.\n",
            "Iteration: 2600. Loss: nan.Correct:0. total:382. Accuracy: 0.0.\n",
            "Iteration: 2700. Loss: nan.Correct:0. total:382. Accuracy: 0.0.\n",
            "Iteration: 2800. Loss: nan.Correct:0. total:382. Accuracy: 0.0.\n",
            "Iteration: 2900. Loss: nan.Correct:0. total:382. Accuracy: 0.0.\n",
            "Iteration: 3000. Loss: nan.Correct:0. total:382. Accuracy: 0.0.\n",
            "Iteration: 3100. Loss: nan.Correct:0. total:382. Accuracy: 0.0.\n",
            "Iteration: 3200. Loss: nan.Correct:0. total:382. Accuracy: 0.0.\n",
            "Iteration: 3300. Loss: nan.Correct:0. total:382. Accuracy: 0.0.\n",
            "Iteration: 3400. Loss: nan.Correct:0. total:382. Accuracy: 0.0.\n",
            "Iteration: 3500. Loss: nan.Correct:0. total:382. Accuracy: 0.0.\n",
            "Iteration: 3600. Loss: nan.Correct:0. total:382. Accuracy: 0.0.\n",
            "Iteration: 3700. Loss: nan.Correct:0. total:382. Accuracy: 0.0.\n",
            "Iteration: 3800. Loss: nan.Correct:1. total:382. Accuracy: 0.2617801047120419.\n",
            "Iteration: 3900. Loss: nan.Correct:1. total:382. Accuracy: 0.2617801047120419.\n",
            "Iteration: 4000. Loss: nan.Correct:2. total:382. Accuracy: 0.5235602094240838.\n",
            "Iteration: 4100. Loss: nan.Correct:4. total:382. Accuracy: 1.0471204188481675.\n",
            "Iteration: 4200. Loss: nan.Correct:9. total:382. Accuracy: 2.356020942408377.\n",
            "Iteration: 4300. Loss: nan.Correct:10. total:382. Accuracy: 2.6178010471204187.\n",
            "Iteration: 4400. Loss: nan.Correct:10. total:382. Accuracy: 2.6178010471204187.\n",
            "Iteration: 4500. Loss: nan.Correct:10. total:382. Accuracy: 2.6178010471204187.\n",
            "Iteration: 4600. Loss: nan.Correct:11. total:382. Accuracy: 2.8795811518324608.\n",
            "Iteration: 4700. Loss: nan.Correct:11. total:382. Accuracy: 2.8795811518324608.\n",
            "Iteration: 4800. Loss: nan.Correct:11. total:382. Accuracy: 2.8795811518324608.\n",
            "Iteration: 4900. Loss: nan.Correct:13. total:382. Accuracy: 3.4031413612565444.\n",
            "Iteration: 5000. Loss: nan.Correct:14. total:382. Accuracy: 3.6649214659685865.\n",
            "Iteration: 5100. Loss: nan.Correct:16. total:382. Accuracy: 4.18848167539267.\n",
            "Iteration: 5200. Loss: nan.Correct:16. total:382. Accuracy: 4.18848167539267.\n",
            "Iteration: 5300. Loss: nan.Correct:16. total:382. Accuracy: 4.18848167539267.\n",
            "Iteration: 5400. Loss: nan.Correct:16. total:382. Accuracy: 4.18848167539267.\n",
            "Iteration: 5500. Loss: nan.Correct:16. total:382. Accuracy: 4.18848167539267.\n",
            "Iteration: 5600. Loss: nan.Correct:16. total:382. Accuracy: 4.18848167539267.\n",
            "Iteration: 5700. Loss: nan.Correct:17. total:382. Accuracy: 4.450261780104712.\n",
            "Iteration: 5800. Loss: nan.Correct:18. total:382. Accuracy: 4.712041884816754.\n",
            "Iteration: 5900. Loss: nan.Correct:23. total:382. Accuracy: 6.020942408376963.\n",
            "Iteration: 6000. Loss: nan.Correct:23. total:382. Accuracy: 6.020942408376963.\n",
            "Iteration: 6100. Loss: nan.Correct:25. total:382. Accuracy: 6.544502617801047.\n",
            "Iteration: 6200. Loss: nan.Correct:25. total:382. Accuracy: 6.544502617801047.\n",
            "Iteration: 6300. Loss: nan.Correct:25. total:382. Accuracy: 6.544502617801047.\n",
            "Iteration: 6400. Loss: nan.Correct:30. total:382. Accuracy: 7.853403141361256.\n",
            "Iteration: 6500. Loss: nan.Correct:32. total:382. Accuracy: 8.37696335078534.\n",
            "Iteration: 6600. Loss: nan.Correct:37. total:382. Accuracy: 9.68586387434555.\n",
            "Iteration: 6700. Loss: nan.Correct:40. total:382. Accuracy: 10.471204188481675.\n",
            "Iteration: 6800. Loss: nan.Correct:40. total:382. Accuracy: 10.471204188481675.\n",
            "Iteration: 6900. Loss: nan.Correct:41. total:382. Accuracy: 10.732984293193716.\n",
            "Iteration: 7000. Loss: nan.Correct:43. total:382. Accuracy: 11.256544502617801.\n",
            "Iteration: 7100. Loss: nan.Correct:44. total:382. Accuracy: 11.518324607329843.\n",
            "Iteration: 7200. Loss: nan.Correct:45. total:382. Accuracy: 11.780104712041885.\n",
            "Iteration: 7300. Loss: nan.Correct:47. total:382. Accuracy: 12.303664921465968.\n",
            "Iteration: 7400. Loss: nan.Correct:48. total:382. Accuracy: 12.565445026178011.\n",
            "Iteration: 7500. Loss: nan.Correct:48. total:382. Accuracy: 12.565445026178011.\n",
            "Iteration: 7600. Loss: nan.Correct:50. total:382. Accuracy: 13.089005235602095.\n",
            "Iteration: 7700. Loss: nan.Correct:51. total:382. Accuracy: 13.350785340314136.\n",
            "Iteration: 7800. Loss: nan.Correct:52. total:382. Accuracy: 13.612565445026178.\n",
            "Iteration: 7900. Loss: nan.Correct:52. total:382. Accuracy: 13.612565445026178.\n",
            "Iteration: 8000. Loss: nan.Correct:53. total:382. Accuracy: 13.87434554973822.\n",
            "Iteration: 8100. Loss: nan.Correct:58. total:382. Accuracy: 15.18324607329843.\n",
            "Iteration: 8200. Loss: nan.Correct:61. total:382. Accuracy: 15.968586387434556.\n",
            "Iteration: 8300. Loss: nan.Correct:61. total:382. Accuracy: 15.968586387434556.\n",
            "Iteration: 8400. Loss: nan.Correct:62. total:382. Accuracy: 16.230366492146597.\n",
            "Iteration: 8500. Loss: nan.Correct:62. total:382. Accuracy: 16.230366492146597.\n",
            "Iteration: 8600. Loss: nan.Correct:64. total:382. Accuracy: 16.75392670157068.\n",
            "Iteration: 8700. Loss: nan.Correct:68. total:382. Accuracy: 17.801047120418847.\n",
            "Iteration: 8800. Loss: nan.Correct:68. total:382. Accuracy: 17.801047120418847.\n",
            "Iteration: 8900. Loss: nan.Correct:68. total:382. Accuracy: 17.801047120418847.\n",
            "Iteration: 9000. Loss: nan.Correct:69. total:382. Accuracy: 18.06282722513089.\n",
            "Iteration: 9100. Loss: nan.Correct:70. total:382. Accuracy: 18.32460732984293.\n",
            "Iteration: 9200. Loss: nan.Correct:70. total:382. Accuracy: 18.32460732984293.\n",
            "Iteration: 9300. Loss: nan.Correct:71. total:382. Accuracy: 18.586387434554975.\n",
            "Iteration: 9400. Loss: nan.Correct:72. total:382. Accuracy: 18.848167539267017.\n",
            "Iteration: 9500. Loss: nan.Correct:72. total:382. Accuracy: 18.848167539267017.\n",
            "Iteration: 9600. Loss: nan.Correct:76. total:382. Accuracy: 19.895287958115183.\n",
            "Iteration: 9700. Loss: nan.Correct:81. total:382. Accuracy: 21.20418848167539.\n",
            "Iteration: 9800. Loss: nan.Correct:81. total:382. Accuracy: 21.20418848167539.\n",
            "Iteration: 9900. Loss: nan.Correct:82. total:382. Accuracy: 21.465968586387433.\n",
            "Iteration: 10000. Loss: nan.Correct:84. total:382. Accuracy: 21.98952879581152.\n",
            "Iteration: 10100. Loss: nan.Correct:84. total:382. Accuracy: 21.98952879581152.\n",
            "Iteration: 10200. Loss: nan.Correct:84. total:382. Accuracy: 21.98952879581152.\n",
            "Iteration: 10300. Loss: nan.Correct:84. total:382. Accuracy: 21.98952879581152.\n",
            "Iteration: 10400. Loss: nan.Correct:84. total:382. Accuracy: 21.98952879581152.\n",
            "Iteration: 10500. Loss: nan.Correct:84. total:382. Accuracy: 21.98952879581152.\n",
            "Iteration: 10600. Loss: nan.Correct:85. total:382. Accuracy: 22.25130890052356.\n",
            "Iteration: 10700. Loss: nan.Correct:85. total:382. Accuracy: 22.25130890052356.\n",
            "Iteration: 10800. Loss: nan.Correct:85. total:382. Accuracy: 22.25130890052356.\n",
            "Iteration: 10900. Loss: nan.Correct:85. total:382. Accuracy: 22.25130890052356.\n",
            "Iteration: 11000. Loss: nan.Correct:85. total:382. Accuracy: 22.25130890052356.\n",
            "Iteration: 11100. Loss: nan.Correct:88. total:382. Accuracy: 23.036649214659686.\n",
            "Iteration: 11200. Loss: nan.Correct:89. total:382. Accuracy: 23.298429319371728.\n",
            "Iteration: 11300. Loss: nan.Correct:92. total:382. Accuracy: 24.083769633507853.\n",
            "Iteration: 11400. Loss: nan.Correct:93. total:382. Accuracy: 24.345549738219894.\n",
            "Iteration: 11500. Loss: nan.Correct:94. total:382. Accuracy: 24.607329842931936.\n",
            "Iteration: 11600. Loss: nan.Correct:94. total:382. Accuracy: 24.607329842931936.\n",
            "Iteration: 11700. Loss: nan.Correct:94. total:382. Accuracy: 24.607329842931936.\n",
            "Iteration: 11800. Loss: nan.Correct:95. total:382. Accuracy: 24.869109947643977.\n",
            "Iteration: 11900. Loss: nan.Correct:97. total:382. Accuracy: 25.392670157068064.\n",
            "Iteration: 12000. Loss: nan.Correct:97. total:382. Accuracy: 25.392670157068064.\n",
            "Iteration: 12100. Loss: nan.Correct:101. total:382. Accuracy: 26.43979057591623.\n",
            "Iteration: 12200. Loss: nan.Correct:102. total:382. Accuracy: 26.701570680628272.\n",
            "Iteration: 12300. Loss: nan.Correct:102. total:382. Accuracy: 26.701570680628272.\n",
            "Iteration: 12400. Loss: nan.Correct:102. total:382. Accuracy: 26.701570680628272.\n",
            "Iteration: 12500. Loss: nan.Correct:104. total:382. Accuracy: 27.225130890052355.\n",
            "Iteration: 12600. Loss: nan.Correct:105. total:382. Accuracy: 27.486910994764397.\n",
            "Iteration: 12700. Loss: nan.Correct:108. total:382. Accuracy: 28.272251308900522.\n",
            "Iteration: 12800. Loss: nan.Correct:109. total:382. Accuracy: 28.534031413612567.\n",
            "Iteration: 12900. Loss: nan.Correct:109. total:382. Accuracy: 28.534031413612567.\n",
            "Iteration: 13000. Loss: nan.Correct:109. total:382. Accuracy: 28.534031413612567.\n",
            "Iteration: 13100. Loss: nan.Correct:110. total:382. Accuracy: 28.79581151832461.\n",
            "Iteration: 13200. Loss: nan.Correct:113. total:382. Accuracy: 29.581151832460733.\n",
            "Iteration: 13300. Loss: nan.Correct:113. total:382. Accuracy: 29.581151832460733.\n",
            "Iteration: 13400. Loss: nan.Correct:116. total:382. Accuracy: 30.36649214659686.\n",
            "Iteration: 13500. Loss: nan.Correct:116. total:382. Accuracy: 30.36649214659686.\n",
            "Iteration: 13600. Loss: nan.Correct:116. total:382. Accuracy: 30.36649214659686.\n",
            "Iteration: 13700. Loss: nan.Correct:116. total:382. Accuracy: 30.36649214659686.\n",
            "Iteration: 13800. Loss: nan.Correct:116. total:382. Accuracy: 30.36649214659686.\n",
            "Iteration: 13900. Loss: nan.Correct:116. total:382. Accuracy: 30.36649214659686.\n",
            "Iteration: 14000. Loss: nan.Correct:116. total:382. Accuracy: 30.36649214659686.\n",
            "Iteration: 14100. Loss: nan.Correct:116. total:382. Accuracy: 30.36649214659686.\n",
            "Iteration: 14200. Loss: nan.Correct:116. total:382. Accuracy: 30.36649214659686.\n",
            "Iteration: 14300. Loss: nan.Correct:119. total:382. Accuracy: 31.151832460732983.\n",
            "Iteration: 14400. Loss: nan.Correct:119. total:382. Accuracy: 31.151832460732983.\n",
            "Iteration: 14500. Loss: nan.Correct:119. total:382. Accuracy: 31.151832460732983.\n",
            "Iteration: 14600. Loss: nan.Correct:119. total:382. Accuracy: 31.151832460732983.\n",
            "Iteration: 14700. Loss: nan.Correct:120. total:382. Accuracy: 31.413612565445025.\n",
            "Iteration: 14800. Loss: nan.Correct:120. total:382. Accuracy: 31.413612565445025.\n",
            "Iteration: 14900. Loss: nan.Correct:120. total:382. Accuracy: 31.413612565445025.\n",
            "Iteration: 15000. Loss: nan.Correct:120. total:382. Accuracy: 31.413612565445025.\n",
            "Iteration: 15100. Loss: nan.Correct:121. total:382. Accuracy: 31.67539267015707.\n",
            "Iteration: 15200. Loss: nan.Correct:122. total:382. Accuracy: 31.93717277486911.\n",
            "Iteration: 15300. Loss: nan.Correct:122. total:382. Accuracy: 31.93717277486911.\n",
            "Iteration: 15400. Loss: nan.Correct:122. total:382. Accuracy: 31.93717277486911.\n",
            "Iteration: 15500. Loss: nan.Correct:122. total:382. Accuracy: 31.93717277486911.\n",
            "Iteration: 15600. Loss: nan.Correct:122. total:382. Accuracy: 31.93717277486911.\n",
            "Iteration: 15700. Loss: nan.Correct:124. total:382. Accuracy: 32.460732984293195.\n",
            "Iteration: 15800. Loss: nan.Correct:126. total:382. Accuracy: 32.98429319371728.\n",
            "Iteration: 15900. Loss: nan.Correct:126. total:382. Accuracy: 32.98429319371728.\n",
            "Iteration: 16000. Loss: nan.Correct:126. total:382. Accuracy: 32.98429319371728.\n",
            "Iteration: 16100. Loss: nan.Correct:126. total:382. Accuracy: 32.98429319371728.\n",
            "Iteration: 16200. Loss: nan.Correct:126. total:382. Accuracy: 32.98429319371728.\n",
            "Iteration: 16300. Loss: nan.Correct:126. total:382. Accuracy: 32.98429319371728.\n",
            "Iteration: 16400. Loss: nan.Correct:126. total:382. Accuracy: 32.98429319371728.\n",
            "Iteration: 16500. Loss: nan.Correct:126. total:382. Accuracy: 32.98429319371728.\n",
            "Iteration: 16600. Loss: nan.Correct:127. total:382. Accuracy: 33.246073298429316.\n",
            "Iteration: 16700. Loss: nan.Correct:127. total:382. Accuracy: 33.246073298429316.\n",
            "Iteration: 16800. Loss: nan.Correct:130. total:382. Accuracy: 34.031413612565444.\n",
            "Iteration: 16900. Loss: nan.Correct:128. total:382. Accuracy: 33.50785340314136.\n",
            "Iteration: 17000. Loss: nan.Correct:130. total:382. Accuracy: 34.031413612565444.\n",
            "Iteration: 17100. Loss: nan.Correct:130. total:382. Accuracy: 34.031413612565444.\n",
            "Iteration: 17200. Loss: nan.Correct:132. total:382. Accuracy: 34.55497382198953.\n",
            "Iteration: 17300. Loss: nan.Correct:135. total:382. Accuracy: 35.340314136125656.\n",
            "Iteration: 17400. Loss: nan.Correct:135. total:382. Accuracy: 35.340314136125656.\n",
            "Iteration: 17500. Loss: nan.Correct:135. total:382. Accuracy: 35.340314136125656.\n",
            "Iteration: 17600. Loss: nan.Correct:135. total:382. Accuracy: 35.340314136125656.\n",
            "Iteration: 17700. Loss: nan.Correct:135. total:382. Accuracy: 35.340314136125656.\n",
            "Iteration: 17800. Loss: nan.Correct:136. total:382. Accuracy: 35.602094240837694.\n",
            "Iteration: 17900. Loss: nan.Correct:139. total:382. Accuracy: 36.38743455497382.\n",
            "Iteration: 18000. Loss: nan.Correct:141. total:382. Accuracy: 36.910994764397905.\n",
            "Iteration: 18100. Loss: nan.Correct:142. total:382. Accuracy: 37.17277486910995.\n",
            "Iteration: 18200. Loss: nan.Correct:142. total:382. Accuracy: 37.17277486910995.\n",
            "Iteration: 18300. Loss: nan.Correct:142. total:382. Accuracy: 37.17277486910995.\n",
            "Iteration: 18400. Loss: nan.Correct:142. total:382. Accuracy: 37.17277486910995.\n",
            "Iteration: 18500. Loss: nan.Correct:142. total:382. Accuracy: 37.17277486910995.\n",
            "Iteration: 18600. Loss: nan.Correct:142. total:382. Accuracy: 37.17277486910995.\n",
            "Iteration: 18700. Loss: nan.Correct:143. total:382. Accuracy: 37.43455497382199.\n",
            "Iteration: 18800. Loss: nan.Correct:145. total:382. Accuracy: 37.95811518324607.\n",
            "Iteration: 18900. Loss: nan.Correct:147. total:382. Accuracy: 38.481675392670155.\n",
            "Iteration: 19000. Loss: nan.Correct:147. total:382. Accuracy: 38.481675392670155.\n",
            "Iteration: 19100. Loss: nan.Correct:147. total:382. Accuracy: 38.481675392670155.\n",
            "Iteration: 19200. Loss: nan.Correct:148. total:382. Accuracy: 38.7434554973822.\n",
            "Iteration: 19300. Loss: nan.Correct:148. total:382. Accuracy: 38.7434554973822.\n",
            "Iteration: 19400. Loss: nan.Correct:150. total:382. Accuracy: 39.26701570680628.\n",
            "Iteration: 19500. Loss: nan.Correct:150. total:382. Accuracy: 39.26701570680628.\n",
            "Iteration: 19600. Loss: nan.Correct:150. total:382. Accuracy: 39.26701570680628.\n",
            "Iteration: 19700. Loss: nan.Correct:151. total:382. Accuracy: 39.52879581151832.\n",
            "Iteration: 19800. Loss: nan.Correct:151. total:382. Accuracy: 39.52879581151832.\n",
            "Iteration: 19900. Loss: nan.Correct:151. total:382. Accuracy: 39.52879581151832.\n",
            "Iteration: 20000. Loss: nan.Correct:151. total:382. Accuracy: 39.52879581151832.\n",
            "Iteration: 20100. Loss: nan.Correct:151. total:382. Accuracy: 39.52879581151832.\n",
            "Iteration: 20200. Loss: nan.Correct:151. total:382. Accuracy: 39.52879581151832.\n",
            "Iteration: 20300. Loss: nan.Correct:151. total:382. Accuracy: 39.52879581151832.\n",
            "Iteration: 20400. Loss: nan.Correct:151. total:382. Accuracy: 39.52879581151832.\n",
            "Iteration: 20500. Loss: nan.Correct:151. total:382. Accuracy: 39.52879581151832.\n",
            "Iteration: 20600. Loss: nan.Correct:151. total:382. Accuracy: 39.52879581151832.\n",
            "Iteration: 20700. Loss: nan.Correct:151. total:382. Accuracy: 39.52879581151832.\n",
            "Iteration: 20800. Loss: nan.Correct:151. total:382. Accuracy: 39.52879581151832.\n",
            "Iteration: 20900. Loss: nan.Correct:152. total:382. Accuracy: 39.79057591623037.\n",
            "Iteration: 21000. Loss: nan.Correct:152. total:382. Accuracy: 39.79057591623037.\n",
            "Iteration: 21100. Loss: nan.Correct:152. total:382. Accuracy: 39.79057591623037.\n",
            "Iteration: 21200. Loss: nan.Correct:152. total:382. Accuracy: 39.79057591623037.\n",
            "Iteration: 21300. Loss: nan.Correct:152. total:382. Accuracy: 39.79057591623037.\n",
            "Iteration: 21400. Loss: nan.Correct:152. total:382. Accuracy: 39.79057591623037.\n",
            "Iteration: 21500. Loss: nan.Correct:152. total:382. Accuracy: 39.79057591623037.\n",
            "Iteration: 21600. Loss: nan.Correct:152. total:382. Accuracy: 39.79057591623037.\n",
            "Iteration: 21700. Loss: nan.Correct:152. total:382. Accuracy: 39.79057591623037.\n",
            "Iteration: 21800. Loss: nan.Correct:153. total:382. Accuracy: 40.05235602094241.\n",
            "Iteration: 21900. Loss: nan.Correct:158. total:382. Accuracy: 41.361256544502616.\n",
            "Iteration: 22000. Loss: nan.Correct:158. total:382. Accuracy: 41.361256544502616.\n",
            "Iteration: 22100. Loss: nan.Correct:158. total:382. Accuracy: 41.361256544502616.\n",
            "Iteration: 22200. Loss: nan.Correct:158. total:382. Accuracy: 41.361256544502616.\n",
            "Iteration: 22300. Loss: nan.Correct:158. total:382. Accuracy: 41.361256544502616.\n",
            "Iteration: 22400. Loss: nan.Correct:161. total:382. Accuracy: 42.146596858638745.\n",
            "Iteration: 22500. Loss: nan.Correct:161. total:382. Accuracy: 42.146596858638745.\n",
            "Iteration: 22600. Loss: nan.Correct:161. total:382. Accuracy: 42.146596858638745.\n",
            "Iteration: 22700. Loss: nan.Correct:162. total:382. Accuracy: 42.40837696335078.\n",
            "Iteration: 22800. Loss: nan.Correct:162. total:382. Accuracy: 42.40837696335078.\n",
            "Iteration: 22900. Loss: nan.Correct:162. total:382. Accuracy: 42.40837696335078.\n",
            "Iteration: 23000. Loss: nan.Correct:162. total:382. Accuracy: 42.40837696335078.\n",
            "Iteration: 23100. Loss: nan.Correct:162. total:382. Accuracy: 42.40837696335078.\n",
            "Iteration: 23200. Loss: nan.Correct:162. total:382. Accuracy: 42.40837696335078.\n",
            "Iteration: 23300. Loss: nan.Correct:163. total:382. Accuracy: 42.67015706806283.\n",
            "Iteration: 23400. Loss: nan.Correct:163. total:382. Accuracy: 42.67015706806283.\n",
            "Iteration: 23500. Loss: nan.Correct:164. total:382. Accuracy: 42.931937172774866.\n",
            "Iteration: 23600. Loss: nan.Correct:164. total:382. Accuracy: 42.931937172774866.\n",
            "Iteration: 23700. Loss: nan.Correct:164. total:382. Accuracy: 42.931937172774866.\n",
            "Iteration: 23800. Loss: nan.Correct:165. total:382. Accuracy: 43.19371727748691.\n",
            "Iteration: 23900. Loss: nan.Correct:165. total:382. Accuracy: 43.19371727748691.\n",
            "Iteration: 24000. Loss: nan.Correct:166. total:382. Accuracy: 43.455497382198956.\n",
            "Iteration: 24100. Loss: nan.Correct:166. total:382. Accuracy: 43.455497382198956.\n",
            "Iteration: 24200. Loss: nan.Correct:168. total:382. Accuracy: 43.97905759162304.\n",
            "Iteration: 24300. Loss: nan.Correct:168. total:382. Accuracy: 43.97905759162304.\n",
            "Iteration: 24400. Loss: nan.Correct:168. total:382. Accuracy: 43.97905759162304.\n",
            "Iteration: 24500. Loss: nan.Correct:168. total:382. Accuracy: 43.97905759162304.\n",
            "Iteration: 24600. Loss: nan.Correct:168. total:382. Accuracy: 43.97905759162304.\n",
            "Iteration: 24700. Loss: nan.Correct:168. total:382. Accuracy: 43.97905759162304.\n",
            "Iteration: 24800. Loss: nan.Correct:168. total:382. Accuracy: 43.97905759162304.\n",
            "Iteration: 24900. Loss: nan.Correct:169. total:382. Accuracy: 44.24083769633508.\n",
            "Iteration: 25000. Loss: nan.Correct:170. total:382. Accuracy: 44.50261780104712.\n",
            "Iteration: 25100. Loss: nan.Correct:170. total:382. Accuracy: 44.50261780104712.\n",
            "Iteration: 25200. Loss: nan.Correct:170. total:382. Accuracy: 44.50261780104712.\n",
            "Iteration: 25300. Loss: nan.Correct:170. total:382. Accuracy: 44.50261780104712.\n",
            "Iteration: 25400. Loss: nan.Correct:170. total:382. Accuracy: 44.50261780104712.\n",
            "Iteration: 25500. Loss: nan.Correct:170. total:382. Accuracy: 44.50261780104712.\n",
            "Iteration: 25600. Loss: nan.Correct:171. total:382. Accuracy: 44.76439790575916.\n",
            "Iteration: 25700. Loss: nan.Correct:171. total:382. Accuracy: 44.76439790575916.\n",
            "Iteration: 25800. Loss: nan.Correct:171. total:382. Accuracy: 44.76439790575916.\n",
            "Iteration: 25900. Loss: nan.Correct:172. total:382. Accuracy: 45.026178010471206.\n",
            "Iteration: 26000. Loss: nan.Correct:172. total:382. Accuracy: 45.026178010471206.\n",
            "Iteration: 26100. Loss: nan.Correct:172. total:382. Accuracy: 45.026178010471206.\n",
            "Iteration: 26200. Loss: nan.Correct:172. total:382. Accuracy: 45.026178010471206.\n",
            "Iteration: 26300. Loss: nan.Correct:172. total:382. Accuracy: 45.026178010471206.\n",
            "Iteration: 26400. Loss: nan.Correct:172. total:382. Accuracy: 45.026178010471206.\n",
            "Iteration: 26500. Loss: nan.Correct:174. total:382. Accuracy: 45.54973821989529.\n",
            "Iteration: 26600. Loss: nan.Correct:173. total:382. Accuracy: 45.287958115183244.\n",
            "Iteration: 26700. Loss: nan.Correct:173. total:382. Accuracy: 45.287958115183244.\n",
            "Iteration: 26800. Loss: nan.Correct:174. total:382. Accuracy: 45.54973821989529.\n",
            "Iteration: 26900. Loss: nan.Correct:174. total:382. Accuracy: 45.54973821989529.\n",
            "Iteration: 27000. Loss: nan.Correct:174. total:382. Accuracy: 45.54973821989529.\n",
            "Iteration: 27100. Loss: nan.Correct:175. total:382. Accuracy: 45.81151832460733.\n",
            "Iteration: 27200. Loss: nan.Correct:175. total:382. Accuracy: 45.81151832460733.\n",
            "Iteration: 27300. Loss: nan.Correct:175. total:382. Accuracy: 45.81151832460733.\n",
            "Iteration: 27400. Loss: nan.Correct:175. total:382. Accuracy: 45.81151832460733.\n",
            "Iteration: 27500. Loss: nan.Correct:175. total:382. Accuracy: 45.81151832460733.\n",
            "Iteration: 27600. Loss: nan.Correct:175. total:382. Accuracy: 45.81151832460733.\n",
            "Iteration: 27700. Loss: nan.Correct:175. total:382. Accuracy: 45.81151832460733.\n",
            "Iteration: 27800. Loss: nan.Correct:175. total:382. Accuracy: 45.81151832460733.\n",
            "Iteration: 27900. Loss: nan.Correct:175. total:382. Accuracy: 45.81151832460733.\n",
            "Iteration: 28000. Loss: nan.Correct:175. total:382. Accuracy: 45.81151832460733.\n",
            "Iteration: 28100. Loss: nan.Correct:175. total:382. Accuracy: 45.81151832460733.\n",
            "Iteration: 28200. Loss: nan.Correct:175. total:382. Accuracy: 45.81151832460733.\n",
            "Iteration: 28300. Loss: nan.Correct:175. total:382. Accuracy: 45.81151832460733.\n",
            "Iteration: 28400. Loss: nan.Correct:175. total:382. Accuracy: 45.81151832460733.\n",
            "Iteration: 28500. Loss: nan.Correct:175. total:382. Accuracy: 45.81151832460733.\n",
            "Iteration: 28600. Loss: nan.Correct:176. total:382. Accuracy: 46.07329842931937.\n",
            "Iteration: 28700. Loss: nan.Correct:176. total:382. Accuracy: 46.07329842931937.\n",
            "Iteration: 28800. Loss: nan.Correct:176. total:382. Accuracy: 46.07329842931937.\n",
            "Iteration: 28900. Loss: nan.Correct:176. total:382. Accuracy: 46.07329842931937.\n",
            "Iteration: 29000. Loss: nan.Correct:176. total:382. Accuracy: 46.07329842931937.\n",
            "Iteration: 29100. Loss: nan.Correct:176. total:382. Accuracy: 46.07329842931937.\n",
            "Iteration: 29200. Loss: nan.Correct:176. total:382. Accuracy: 46.07329842931937.\n",
            "Iteration: 29300. Loss: nan.Correct:176. total:382. Accuracy: 46.07329842931937.\n",
            "Iteration: 29400. Loss: nan.Correct:176. total:382. Accuracy: 46.07329842931937.\n",
            "Iteration: 29500. Loss: nan.Correct:177. total:382. Accuracy: 46.33507853403141.\n",
            "Iteration: 29600. Loss: nan.Correct:177. total:382. Accuracy: 46.33507853403141.\n",
            "Iteration: 29700. Loss: nan.Correct:177. total:382. Accuracy: 46.33507853403141.\n",
            "Iteration: 29800. Loss: nan.Correct:177. total:382. Accuracy: 46.33507853403141.\n",
            "Iteration: 29900. Loss: nan.Correct:177. total:382. Accuracy: 46.33507853403141.\n",
            "Iteration: 30000. Loss: nan.Correct:177. total:382. Accuracy: 46.33507853403141.\n",
            "Iteration: 30100. Loss: nan.Correct:177. total:382. Accuracy: 46.33507853403141.\n",
            "Iteration: 30200. Loss: nan.Correct:177. total:382. Accuracy: 46.33507853403141.\n",
            "Iteration: 30300. Loss: nan.Correct:177. total:382. Accuracy: 46.33507853403141.\n",
            "Iteration: 30400. Loss: nan.Correct:177. total:382. Accuracy: 46.33507853403141.\n",
            "Iteration: 30500. Loss: nan.Correct:177. total:382. Accuracy: 46.33507853403141.\n",
            "Iteration: 30600. Loss: nan.Correct:177. total:382. Accuracy: 46.33507853403141.\n",
            "Iteration: 30700. Loss: nan.Correct:177. total:382. Accuracy: 46.33507853403141.\n",
            "Iteration: 30800. Loss: nan.Correct:177. total:382. Accuracy: 46.33507853403141.\n",
            "Iteration: 30900. Loss: nan.Correct:177. total:382. Accuracy: 46.33507853403141.\n",
            "Iteration: 31000. Loss: nan.Correct:177. total:382. Accuracy: 46.33507853403141.\n",
            "Iteration: 31100. Loss: nan.Correct:177. total:382. Accuracy: 46.33507853403141.\n",
            "Iteration: 31200. Loss: nan.Correct:177. total:382. Accuracy: 46.33507853403141.\n",
            "Iteration: 31300. Loss: nan.Correct:177. total:382. Accuracy: 46.33507853403141.\n",
            "Iteration: 31400. Loss: nan.Correct:177. total:382. Accuracy: 46.33507853403141.\n",
            "Iteration: 31500. Loss: nan.Correct:177. total:382. Accuracy: 46.33507853403141.\n",
            "Iteration: 31600. Loss: nan.Correct:177. total:382. Accuracy: 46.33507853403141.\n",
            "Iteration: 31700. Loss: nan.Correct:177. total:382. Accuracy: 46.33507853403141.\n",
            "Iteration: 31800. Loss: nan.Correct:177. total:382. Accuracy: 46.33507853403141.\n",
            "Iteration: 31900. Loss: nan.Correct:177. total:382. Accuracy: 46.33507853403141.\n",
            "Iteration: 32000. Loss: nan.Correct:177. total:382. Accuracy: 46.33507853403141.\n",
            "Iteration: 32100. Loss: nan.Correct:177. total:382. Accuracy: 46.33507853403141.\n",
            "Iteration: 32200. Loss: nan.Correct:177. total:382. Accuracy: 46.33507853403141.\n",
            "Iteration: 32300. Loss: nan.Correct:177. total:382. Accuracy: 46.33507853403141.\n",
            "Iteration: 32400. Loss: nan.Correct:177. total:382. Accuracy: 46.33507853403141.\n",
            "Iteration: 32500. Loss: nan.Correct:178. total:382. Accuracy: 46.596858638743456.\n",
            "Iteration: 32600. Loss: nan.Correct:178. total:382. Accuracy: 46.596858638743456.\n",
            "Iteration: 32700. Loss: nan.Correct:178. total:382. Accuracy: 46.596858638743456.\n",
            "Iteration: 32800. Loss: nan.Correct:178. total:382. Accuracy: 46.596858638743456.\n",
            "Iteration: 32900. Loss: nan.Correct:178. total:382. Accuracy: 46.596858638743456.\n",
            "Iteration: 33000. Loss: nan.Correct:178. total:382. Accuracy: 46.596858638743456.\n",
            "Iteration: 33100. Loss: nan.Correct:178. total:382. Accuracy: 46.596858638743456.\n",
            "Iteration: 33200. Loss: nan.Correct:178. total:382. Accuracy: 46.596858638743456.\n",
            "Iteration: 33300. Loss: nan.Correct:178. total:382. Accuracy: 46.596858638743456.\n",
            "Iteration: 33400. Loss: nan.Correct:178. total:382. Accuracy: 46.596858638743456.\n",
            "Iteration: 33500. Loss: nan.Correct:178. total:382. Accuracy: 46.596858638743456.\n",
            "Iteration: 33600. Loss: nan.Correct:178. total:382. Accuracy: 46.596858638743456.\n",
            "Iteration: 33700. Loss: nan.Correct:178. total:382. Accuracy: 46.596858638743456.\n",
            "Iteration: 33800. Loss: nan.Correct:178. total:382. Accuracy: 46.596858638743456.\n",
            "Iteration: 33900. Loss: nan.Correct:178. total:382. Accuracy: 46.596858638743456.\n",
            "Iteration: 34000. Loss: nan.Correct:178. total:382. Accuracy: 46.596858638743456.\n",
            "Iteration: 34100. Loss: nan.Correct:178. total:382. Accuracy: 46.596858638743456.\n",
            "Iteration: 34200. Loss: nan.Correct:178. total:382. Accuracy: 46.596858638743456.\n",
            "Iteration: 34300. Loss: nan.Correct:178. total:382. Accuracy: 46.596858638743456.\n",
            "Iteration: 34400. Loss: nan.Correct:178. total:382. Accuracy: 46.596858638743456.\n",
            "Iteration: 34500. Loss: nan.Correct:178. total:382. Accuracy: 46.596858638743456.\n",
            "Iteration: 34600. Loss: nan.Correct:179. total:382. Accuracy: 46.8586387434555.\n",
            "Iteration: 34700. Loss: nan.Correct:179. total:382. Accuracy: 46.8586387434555.\n",
            "Iteration: 34800. Loss: nan.Correct:179. total:382. Accuracy: 46.8586387434555.\n",
            "Iteration: 34900. Loss: nan.Correct:179. total:382. Accuracy: 46.8586387434555.\n",
            "Iteration: 35000. Loss: nan.Correct:179. total:382. Accuracy: 46.8586387434555.\n",
            "Iteration: 35100. Loss: nan.Correct:179. total:382. Accuracy: 46.8586387434555.\n",
            "Iteration: 35200. Loss: nan.Correct:179. total:382. Accuracy: 46.8586387434555.\n",
            "Iteration: 35300. Loss: nan.Correct:179. total:382. Accuracy: 46.8586387434555.\n",
            "Iteration: 35400. Loss: nan.Correct:179. total:382. Accuracy: 46.8586387434555.\n",
            "Iteration: 35500. Loss: nan.Correct:179. total:382. Accuracy: 46.8586387434555.\n",
            "Iteration: 35600. Loss: nan.Correct:180. total:382. Accuracy: 47.12041884816754.\n",
            "Iteration: 35700. Loss: nan.Correct:180. total:382. Accuracy: 47.12041884816754.\n",
            "Iteration: 35800. Loss: nan.Correct:180. total:382. Accuracy: 47.12041884816754.\n",
            "Iteration: 35900. Loss: nan.Correct:180. total:382. Accuracy: 47.12041884816754.\n",
            "Iteration: 36000. Loss: nan.Correct:180. total:382. Accuracy: 47.12041884816754.\n",
            "Iteration: 36100. Loss: nan.Correct:180. total:382. Accuracy: 47.12041884816754.\n",
            "Iteration: 36200. Loss: nan.Correct:180. total:382. Accuracy: 47.12041884816754.\n",
            "Iteration: 36300. Loss: nan.Correct:180. total:382. Accuracy: 47.12041884816754.\n",
            "Iteration: 36400. Loss: nan.Correct:180. total:382. Accuracy: 47.12041884816754.\n",
            "Iteration: 36500. Loss: nan.Correct:180. total:382. Accuracy: 47.12041884816754.\n",
            "Iteration: 36600. Loss: nan.Correct:180. total:382. Accuracy: 47.12041884816754.\n",
            "Iteration: 36700. Loss: nan.Correct:180. total:382. Accuracy: 47.12041884816754.\n",
            "Iteration: 36800. Loss: nan.Correct:180. total:382. Accuracy: 47.12041884816754.\n",
            "Iteration: 36900. Loss: nan.Correct:180. total:382. Accuracy: 47.12041884816754.\n",
            "Iteration: 37000. Loss: nan.Correct:180. total:382. Accuracy: 47.12041884816754.\n",
            "Iteration: 37100. Loss: nan.Correct:182. total:382. Accuracy: 47.64397905759162.\n",
            "Iteration: 37200. Loss: nan.Correct:182. total:382. Accuracy: 47.64397905759162.\n",
            "Iteration: 37300. Loss: nan.Correct:182. total:382. Accuracy: 47.64397905759162.\n",
            "Iteration: 37400. Loss: nan.Correct:182. total:382. Accuracy: 47.64397905759162.\n",
            "Iteration: 37500. Loss: nan.Correct:182. total:382. Accuracy: 47.64397905759162.\n",
            "Iteration: 37600. Loss: nan.Correct:182. total:382. Accuracy: 47.64397905759162.\n",
            "Iteration: 37700. Loss: nan.Correct:182. total:382. Accuracy: 47.64397905759162.\n",
            "Iteration: 37800. Loss: nan.Correct:183. total:382. Accuracy: 47.90575916230367.\n",
            "Iteration: 37900. Loss: nan.Correct:183. total:382. Accuracy: 47.90575916230367.\n",
            "Iteration: 38000. Loss: nan.Correct:183. total:382. Accuracy: 47.90575916230367.\n",
            "Iteration: 38100. Loss: nan.Correct:183. total:382. Accuracy: 47.90575916230367.\n",
            "Iteration: 38200. Loss: nan.Correct:183. total:382. Accuracy: 47.90575916230367.\n",
            "Iteration: 38300. Loss: nan.Correct:183. total:382. Accuracy: 47.90575916230367.\n",
            "Iteration: 38400. Loss: nan.Correct:183. total:382. Accuracy: 47.90575916230367.\n",
            "Iteration: 38500. Loss: nan.Correct:183. total:382. Accuracy: 47.90575916230367.\n",
            "Iteration: 38600. Loss: nan.Correct:184. total:382. Accuracy: 48.167539267015705.\n",
            "Iteration: 38700. Loss: nan.Correct:185. total:382. Accuracy: 48.42931937172775.\n",
            "Iteration: 38800. Loss: nan.Correct:185. total:382. Accuracy: 48.42931937172775.\n",
            "Iteration: 38900. Loss: nan.Correct:185. total:382. Accuracy: 48.42931937172775.\n",
            "Iteration: 39000. Loss: nan.Correct:185. total:382. Accuracy: 48.42931937172775.\n",
            "Iteration: 39100. Loss: nan.Correct:185. total:382. Accuracy: 48.42931937172775.\n",
            "Iteration: 39200. Loss: nan.Correct:185. total:382. Accuracy: 48.42931937172775.\n",
            "Iteration: 39300. Loss: nan.Correct:185. total:382. Accuracy: 48.42931937172775.\n",
            "Iteration: 39400. Loss: nan.Correct:185. total:382. Accuracy: 48.42931937172775.\n",
            "Iteration: 39500. Loss: nan.Correct:185. total:382. Accuracy: 48.42931937172775.\n",
            "Iteration: 39600. Loss: nan.Correct:185. total:382. Accuracy: 48.42931937172775.\n",
            "Iteration: 39700. Loss: nan.Correct:185. total:382. Accuracy: 48.42931937172775.\n",
            "Iteration: 39800. Loss: nan.Correct:185. total:382. Accuracy: 48.42931937172775.\n",
            "Iteration: 39900. Loss: nan.Correct:185. total:382. Accuracy: 48.42931937172775.\n",
            "Iteration: 40000. Loss: nan.Correct:185. total:382. Accuracy: 48.42931937172775.\n",
            "Iteration: 40100. Loss: nan.Correct:185. total:382. Accuracy: 48.42931937172775.\n",
            "Iteration: 40200. Loss: nan.Correct:187. total:382. Accuracy: 48.952879581151834.\n",
            "Iteration: 40300. Loss: nan.Correct:187. total:382. Accuracy: 48.952879581151834.\n",
            "Iteration: 40400. Loss: nan.Correct:186. total:382. Accuracy: 48.69109947643979.\n",
            "Iteration: 40500. Loss: nan.Correct:186. total:382. Accuracy: 48.69109947643979.\n",
            "Iteration: 40600. Loss: nan.Correct:187. total:382. Accuracy: 48.952879581151834.\n",
            "Iteration: 40700. Loss: nan.Correct:188. total:382. Accuracy: 49.21465968586387.\n",
            "Iteration: 40800. Loss: nan.Correct:188. total:382. Accuracy: 49.21465968586387.\n",
            "Iteration: 40900. Loss: nan.Correct:188. total:382. Accuracy: 49.21465968586387.\n",
            "Iteration: 41000. Loss: nan.Correct:188. total:382. Accuracy: 49.21465968586387.\n",
            "Iteration: 41100. Loss: nan.Correct:188. total:382. Accuracy: 49.21465968586387.\n",
            "Iteration: 41200. Loss: nan.Correct:188. total:382. Accuracy: 49.21465968586387.\n",
            "Iteration: 41300. Loss: nan.Correct:188. total:382. Accuracy: 49.21465968586387.\n",
            "Iteration: 41400. Loss: nan.Correct:188. total:382. Accuracy: 49.21465968586387.\n",
            "Iteration: 41500. Loss: nan.Correct:188. total:382. Accuracy: 49.21465968586387.\n",
            "Iteration: 41600. Loss: nan.Correct:188. total:382. Accuracy: 49.21465968586387.\n",
            "Iteration: 41700. Loss: nan.Correct:189. total:382. Accuracy: 49.47643979057592.\n",
            "Iteration: 41800. Loss: nan.Correct:189. total:382. Accuracy: 49.47643979057592.\n",
            "Iteration: 41900. Loss: nan.Correct:189. total:382. Accuracy: 49.47643979057592.\n",
            "Iteration: 42000. Loss: nan.Correct:189. total:382. Accuracy: 49.47643979057592.\n",
            "Iteration: 42100. Loss: nan.Correct:189. total:382. Accuracy: 49.47643979057592.\n",
            "Iteration: 42200. Loss: nan.Correct:189. total:382. Accuracy: 49.47643979057592.\n",
            "Iteration: 42300. Loss: nan.Correct:189. total:382. Accuracy: 49.47643979057592.\n",
            "Iteration: 42400. Loss: nan.Correct:189. total:382. Accuracy: 49.47643979057592.\n",
            "Iteration: 42500. Loss: nan.Correct:189. total:382. Accuracy: 49.47643979057592.\n",
            "Iteration: 42600. Loss: nan.Correct:189. total:382. Accuracy: 49.47643979057592.\n",
            "Iteration: 42700. Loss: nan.Correct:189. total:382. Accuracy: 49.47643979057592.\n",
            "Iteration: 42800. Loss: nan.Correct:189. total:382. Accuracy: 49.47643979057592.\n",
            "Iteration: 42900. Loss: nan.Correct:189. total:382. Accuracy: 49.47643979057592.\n",
            "Iteration: 43000. Loss: nan.Correct:189. total:382. Accuracy: 49.47643979057592.\n",
            "Iteration: 43100. Loss: nan.Correct:189. total:382. Accuracy: 49.47643979057592.\n",
            "Iteration: 43200. Loss: nan.Correct:189. total:382. Accuracy: 49.47643979057592.\n",
            "Iteration: 43300. Loss: nan.Correct:189. total:382. Accuracy: 49.47643979057592.\n",
            "Iteration: 43400. Loss: nan.Correct:189. total:382. Accuracy: 49.47643979057592.\n",
            "Iteration: 43500. Loss: nan.Correct:189. total:382. Accuracy: 49.47643979057592.\n",
            "Iteration: 43600. Loss: nan.Correct:189. total:382. Accuracy: 49.47643979057592.\n",
            "Iteration: 43700. Loss: nan.Correct:189. total:382. Accuracy: 49.47643979057592.\n",
            "Iteration: 43800. Loss: nan.Correct:189. total:382. Accuracy: 49.47643979057592.\n",
            "Iteration: 43900. Loss: nan.Correct:189. total:382. Accuracy: 49.47643979057592.\n",
            "Iteration: 44000. Loss: nan.Correct:189. total:382. Accuracy: 49.47643979057592.\n",
            "Iteration: 44100. Loss: nan.Correct:190. total:382. Accuracy: 49.738219895287955.\n",
            "Iteration: 44200. Loss: nan.Correct:190. total:382. Accuracy: 49.738219895287955.\n",
            "Iteration: 44300. Loss: nan.Correct:190. total:382. Accuracy: 49.738219895287955.\n",
            "Iteration: 44400. Loss: nan.Correct:190. total:382. Accuracy: 49.738219895287955.\n",
            "Iteration: 44500. Loss: nan.Correct:190. total:382. Accuracy: 49.738219895287955.\n",
            "Iteration: 44600. Loss: nan.Correct:190. total:382. Accuracy: 49.738219895287955.\n",
            "Iteration: 44700. Loss: nan.Correct:190. total:382. Accuracy: 49.738219895287955.\n",
            "Iteration: 44800. Loss: nan.Correct:190. total:382. Accuracy: 49.738219895287955.\n",
            "Iteration: 44900. Loss: nan.Correct:190. total:382. Accuracy: 49.738219895287955.\n",
            "Iteration: 45000. Loss: nan.Correct:190. total:382. Accuracy: 49.738219895287955.\n",
            "Iteration: 45100. Loss: nan.Correct:190. total:382. Accuracy: 49.738219895287955.\n",
            "Iteration: 45200. Loss: nan.Correct:190. total:382. Accuracy: 49.738219895287955.\n",
            "Iteration: 45300. Loss: nan.Correct:190. total:382. Accuracy: 49.738219895287955.\n",
            "Iteration: 45400. Loss: nan.Correct:190. total:382. Accuracy: 49.738219895287955.\n",
            "Iteration: 45500. Loss: nan.Correct:190. total:382. Accuracy: 49.738219895287955.\n",
            "Iteration: 45600. Loss: nan.Correct:190. total:382. Accuracy: 49.738219895287955.\n",
            "Iteration: 45700. Loss: nan.Correct:190. total:382. Accuracy: 49.738219895287955.\n",
            "Iteration: 45800. Loss: nan.Correct:190. total:382. Accuracy: 49.738219895287955.\n",
            "Iteration: 45900. Loss: nan.Correct:190. total:382. Accuracy: 49.738219895287955.\n",
            "Iteration: 46000. Loss: nan.Correct:190. total:382. Accuracy: 49.738219895287955.\n",
            "Iteration: 46100. Loss: nan.Correct:190. total:382. Accuracy: 49.738219895287955.\n",
            "Iteration: 46200. Loss: nan.Correct:190. total:382. Accuracy: 49.738219895287955.\n",
            "Iteration: 46300. Loss: nan.Correct:190. total:382. Accuracy: 49.738219895287955.\n",
            "Iteration: 46400. Loss: nan.Correct:190. total:382. Accuracy: 49.738219895287955.\n",
            "Iteration: 46500. Loss: nan.Correct:190. total:382. Accuracy: 49.738219895287955.\n",
            "Iteration: 46600. Loss: nan.Correct:190. total:382. Accuracy: 49.738219895287955.\n",
            "Iteration: 46700. Loss: nan.Correct:190. total:382. Accuracy: 49.738219895287955.\n",
            "Iteration: 46800. Loss: nan.Correct:190. total:382. Accuracy: 49.738219895287955.\n",
            "Iteration: 46900. Loss: nan.Correct:190. total:382. Accuracy: 49.738219895287955.\n",
            "Iteration: 47000. Loss: nan.Correct:190. total:382. Accuracy: 49.738219895287955.\n",
            "Iteration: 47100. Loss: nan.Correct:190. total:382. Accuracy: 49.738219895287955.\n",
            "Iteration: 47200. Loss: nan.Correct:190. total:382. Accuracy: 49.738219895287955.\n",
            "Iteration: 47300. Loss: nan.Correct:190. total:382. Accuracy: 49.738219895287955.\n",
            "Iteration: 47400. Loss: nan.Correct:190. total:382. Accuracy: 49.738219895287955.\n",
            "Iteration: 47500. Loss: nan.Correct:190. total:382. Accuracy: 49.738219895287955.\n",
            "Iteration: 47600. Loss: nan.Correct:190. total:382. Accuracy: 49.738219895287955.\n",
            "Iteration: 47700. Loss: nan.Correct:190. total:382. Accuracy: 49.738219895287955.\n",
            "Iteration: 47800. Loss: nan.Correct:190. total:382. Accuracy: 49.738219895287955.\n",
            "Iteration: 47900. Loss: nan.Correct:190. total:382. Accuracy: 49.738219895287955.\n",
            "Iteration: 48000. Loss: nan.Correct:190. total:382. Accuracy: 49.738219895287955.\n",
            "Iteration: 48100. Loss: nan.Correct:190. total:382. Accuracy: 49.738219895287955.\n",
            "Iteration: 48200. Loss: nan.Correct:190. total:382. Accuracy: 49.738219895287955.\n",
            "Iteration: 48300. Loss: nan.Correct:190. total:382. Accuracy: 49.738219895287955.\n",
            "Iteration: 48400. Loss: nan.Correct:191. total:382. Accuracy: 50.0.\n",
            "Iteration: 48500. Loss: nan.Correct:191. total:382. Accuracy: 50.0.\n",
            "Iteration: 48600. Loss: nan.Correct:192. total:382. Accuracy: 50.261780104712045.\n",
            "Iteration: 48700. Loss: nan.Correct:192. total:382. Accuracy: 50.261780104712045.\n",
            "Iteration: 48800. Loss: nan.Correct:192. total:382. Accuracy: 50.261780104712045.\n",
            "Iteration: 48900. Loss: nan.Correct:192. total:382. Accuracy: 50.261780104712045.\n",
            "Iteration: 49000. Loss: nan.Correct:191. total:382. Accuracy: 50.0.\n",
            "Iteration: 49100. Loss: nan.Correct:191. total:382. Accuracy: 50.0.\n",
            "Iteration: 49200. Loss: nan.Correct:192. total:382. Accuracy: 50.261780104712045.\n",
            "Iteration: 49300. Loss: nan.Correct:192. total:382. Accuracy: 50.261780104712045.\n",
            "Iteration: 49400. Loss: nan.Correct:192. total:382. Accuracy: 50.261780104712045.\n",
            "Iteration: 49500. Loss: nan.Correct:192. total:382. Accuracy: 50.261780104712045.\n",
            "Iteration: 49600. Loss: nan.Correct:192. total:382. Accuracy: 50.261780104712045.\n",
            "Iteration: 49700. Loss: nan.Correct:192. total:382. Accuracy: 50.261780104712045.\n",
            "Iteration: 49800. Loss: nan.Correct:192. total:382. Accuracy: 50.261780104712045.\n",
            "Iteration: 49900. Loss: nan.Correct:193. total:382. Accuracy: 50.52356020942408.\n",
            "Iteration: 50000. Loss: nan.Correct:193. total:382. Accuracy: 50.52356020942408.\n",
            "Iteration: 50100. Loss: nan.Correct:193. total:382. Accuracy: 50.52356020942408.\n",
            "Iteration: 50200. Loss: nan.Correct:193. total:382. Accuracy: 50.52356020942408.\n",
            "Iteration: 50300. Loss: nan.Correct:193. total:382. Accuracy: 50.52356020942408.\n",
            "Iteration: 50400. Loss: nan.Correct:193. total:382. Accuracy: 50.52356020942408.\n",
            "Iteration: 50500. Loss: nan.Correct:193. total:382. Accuracy: 50.52356020942408.\n",
            "Iteration: 50600. Loss: nan.Correct:193. total:382. Accuracy: 50.52356020942408.\n",
            "Iteration: 50700. Loss: nan.Correct:193. total:382. Accuracy: 50.52356020942408.\n",
            "Iteration: 50800. Loss: nan.Correct:194. total:382. Accuracy: 50.78534031413613.\n",
            "Iteration: 50900. Loss: nan.Correct:194. total:382. Accuracy: 50.78534031413613.\n",
            "Iteration: 51000. Loss: nan.Correct:194. total:382. Accuracy: 50.78534031413613.\n",
            "Iteration: 51100. Loss: nan.Correct:194. total:382. Accuracy: 50.78534031413613.\n",
            "Iteration: 51200. Loss: nan.Correct:194. total:382. Accuracy: 50.78534031413613.\n",
            "Iteration: 51300. Loss: nan.Correct:194. total:382. Accuracy: 50.78534031413613.\n",
            "Iteration: 51400. Loss: nan.Correct:194. total:382. Accuracy: 50.78534031413613.\n",
            "Iteration: 51500. Loss: nan.Correct:194. total:382. Accuracy: 50.78534031413613.\n",
            "Iteration: 51600. Loss: nan.Correct:194. total:382. Accuracy: 50.78534031413613.\n",
            "Iteration: 51700. Loss: nan.Correct:194. total:382. Accuracy: 50.78534031413613.\n",
            "Iteration: 51800. Loss: nan.Correct:194. total:382. Accuracy: 50.78534031413613.\n",
            "Iteration: 51900. Loss: nan.Correct:194. total:382. Accuracy: 50.78534031413613.\n",
            "Iteration: 52000. Loss: nan.Correct:194. total:382. Accuracy: 50.78534031413613.\n",
            "Iteration: 52100. Loss: nan.Correct:194. total:382. Accuracy: 50.78534031413613.\n",
            "Iteration: 52200. Loss: nan.Correct:194. total:382. Accuracy: 50.78534031413613.\n",
            "Iteration: 52300. Loss: nan.Correct:195. total:382. Accuracy: 51.047120418848166.\n",
            "Iteration: 52400. Loss: nan.Correct:195. total:382. Accuracy: 51.047120418848166.\n",
            "Iteration: 52500. Loss: nan.Correct:195. total:382. Accuracy: 51.047120418848166.\n",
            "Iteration: 52600. Loss: nan.Correct:195. total:382. Accuracy: 51.047120418848166.\n",
            "Iteration: 52700. Loss: nan.Correct:195. total:382. Accuracy: 51.047120418848166.\n",
            "Iteration: 52800. Loss: nan.Correct:195. total:382. Accuracy: 51.047120418848166.\n",
            "Iteration: 52900. Loss: nan.Correct:195. total:382. Accuracy: 51.047120418848166.\n",
            "Iteration: 53000. Loss: nan.Correct:195. total:382. Accuracy: 51.047120418848166.\n",
            "Iteration: 53100. Loss: nan.Correct:195. total:382. Accuracy: 51.047120418848166.\n",
            "Iteration: 53200. Loss: nan.Correct:195. total:382. Accuracy: 51.047120418848166.\n",
            "Iteration: 53300. Loss: nan.Correct:195. total:382. Accuracy: 51.047120418848166.\n",
            "Iteration: 53400. Loss: nan.Correct:195. total:382. Accuracy: 51.047120418848166.\n",
            "Iteration: 53500. Loss: nan.Correct:195. total:382. Accuracy: 51.047120418848166.\n",
            "Iteration: 53600. Loss: nan.Correct:195. total:382. Accuracy: 51.047120418848166.\n",
            "Iteration: 53700. Loss: nan.Correct:195. total:382. Accuracy: 51.047120418848166.\n",
            "Iteration: 53800. Loss: nan.Correct:195. total:382. Accuracy: 51.047120418848166.\n",
            "Iteration: 53900. Loss: nan.Correct:197. total:382. Accuracy: 51.57068062827225.\n",
            "Iteration: 54000. Loss: nan.Correct:197. total:382. Accuracy: 51.57068062827225.\n",
            "Iteration: 54100. Loss: nan.Correct:197. total:382. Accuracy: 51.57068062827225.\n",
            "Iteration: 54200. Loss: nan.Correct:197. total:382. Accuracy: 51.57068062827225.\n",
            "Iteration: 54300. Loss: nan.Correct:197. total:382. Accuracy: 51.57068062827225.\n",
            "Iteration: 54400. Loss: nan.Correct:197. total:382. Accuracy: 51.57068062827225.\n",
            "Iteration: 54500. Loss: nan.Correct:197. total:382. Accuracy: 51.57068062827225.\n",
            "Iteration: 54600. Loss: nan.Correct:197. total:382. Accuracy: 51.57068062827225.\n",
            "Iteration: 54700. Loss: nan.Correct:197. total:382. Accuracy: 51.57068062827225.\n",
            "Iteration: 54800. Loss: nan.Correct:197. total:382. Accuracy: 51.57068062827225.\n",
            "Iteration: 54900. Loss: nan.Correct:197. total:382. Accuracy: 51.57068062827225.\n",
            "Iteration: 55000. Loss: nan.Correct:197. total:382. Accuracy: 51.57068062827225.\n",
            "Iteration: 55100. Loss: nan.Correct:197. total:382. Accuracy: 51.57068062827225.\n",
            "Iteration: 55200. Loss: nan.Correct:197. total:382. Accuracy: 51.57068062827225.\n",
            "Iteration: 55300. Loss: nan.Correct:197. total:382. Accuracy: 51.57068062827225.\n",
            "Iteration: 55400. Loss: nan.Correct:197. total:382. Accuracy: 51.57068062827225.\n",
            "Iteration: 55500. Loss: nan.Correct:197. total:382. Accuracy: 51.57068062827225.\n",
            "Iteration: 55600. Loss: nan.Correct:197. total:382. Accuracy: 51.57068062827225.\n",
            "Iteration: 55700. Loss: nan.Correct:197. total:382. Accuracy: 51.57068062827225.\n",
            "Iteration: 55800. Loss: nan.Correct:197. total:382. Accuracy: 51.57068062827225.\n",
            "Iteration: 55900. Loss: nan.Correct:197. total:382. Accuracy: 51.57068062827225.\n",
            "Iteration: 56000. Loss: nan.Correct:197. total:382. Accuracy: 51.57068062827225.\n",
            "Iteration: 56100. Loss: nan.Correct:197. total:382. Accuracy: 51.57068062827225.\n",
            "Iteration: 56200. Loss: nan.Correct:197. total:382. Accuracy: 51.57068062827225.\n",
            "Iteration: 56300. Loss: nan.Correct:197. total:382. Accuracy: 51.57068062827225.\n",
            "Iteration: 56400. Loss: nan.Correct:197. total:382. Accuracy: 51.57068062827225.\n",
            "Iteration: 56500. Loss: nan.Correct:197. total:382. Accuracy: 51.57068062827225.\n",
            "Iteration: 56600. Loss: nan.Correct:197. total:382. Accuracy: 51.57068062827225.\n",
            "Iteration: 56700. Loss: nan.Correct:197. total:382. Accuracy: 51.57068062827225.\n",
            "Iteration: 56800. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 56900. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 57000. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 57100. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 57200. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 57300. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 57400. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 57500. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 57600. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 57700. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 57800. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 57900. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 58000. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 58100. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 58200. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 58300. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 58400. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 58500. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 58600. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 58700. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 58800. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 58900. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 59000. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 59100. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 59200. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 59300. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 59400. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 59500. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 59600. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 59700. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 59800. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 59900. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 60000. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 60100. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 60200. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 60300. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 60400. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 60500. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 60600. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 60700. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 60800. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 60900. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 61000. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 61100. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 61200. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 61300. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 61400. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 61500. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 61600. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 61700. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 61800. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 61900. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 62000. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 62100. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 62200. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 62300. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 62400. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 62500. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 62600. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 62700. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 62800. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 62900. Loss: nan.Correct:198. total:382. Accuracy: 51.832460732984295.\n",
            "Iteration: 63000. Loss: nan.Correct:199. total:382. Accuracy: 52.09424083769633.\n",
            "Iteration: 63100. Loss: nan.Correct:199. total:382. Accuracy: 52.09424083769633.\n",
            "Iteration: 63200. Loss: nan.Correct:199. total:382. Accuracy: 52.09424083769633.\n",
            "Iteration: 63300. Loss: nan.Correct:199. total:382. Accuracy: 52.09424083769633.\n",
            "Iteration: 63400. Loss: nan.Correct:199. total:382. Accuracy: 52.09424083769633.\n",
            "Iteration: 63500. Loss: nan.Correct:199. total:382. Accuracy: 52.09424083769633.\n",
            "Iteration: 63600. Loss: nan.Correct:199. total:382. Accuracy: 52.09424083769633.\n",
            "Iteration: 63700. Loss: nan.Correct:199. total:382. Accuracy: 52.09424083769633.\n",
            "Iteration: 63800. Loss: nan.Correct:199. total:382. Accuracy: 52.09424083769633.\n",
            "Iteration: 63900. Loss: nan.Correct:199. total:382. Accuracy: 52.09424083769633.\n",
            "Iteration: 64000. Loss: nan.Correct:199. total:382. Accuracy: 52.09424083769633.\n",
            "Iteration: 64100. Loss: nan.Correct:199. total:382. Accuracy: 52.09424083769633.\n",
            "Iteration: 64200. Loss: nan.Correct:199. total:382. Accuracy: 52.09424083769633.\n",
            "Iteration: 64300. Loss: nan.Correct:199. total:382. Accuracy: 52.09424083769633.\n",
            "Iteration: 64400. Loss: nan.Correct:199. total:382. Accuracy: 52.09424083769633.\n",
            "Iteration: 64500. Loss: nan.Correct:199. total:382. Accuracy: 52.09424083769633.\n",
            "Iteration: 64600. Loss: nan.Correct:199. total:382. Accuracy: 52.09424083769633.\n",
            "Iteration: 64700. Loss: nan.Correct:199. total:382. Accuracy: 52.09424083769633.\n",
            "Iteration: 64800. Loss: nan.Correct:199. total:382. Accuracy: 52.09424083769633.\n",
            "Iteration: 64900. Loss: nan.Correct:199. total:382. Accuracy: 52.09424083769633.\n",
            "Iteration: 65000. Loss: nan.Correct:199. total:382. Accuracy: 52.09424083769633.\n",
            "Iteration: 65100. Loss: nan.Correct:199. total:382. Accuracy: 52.09424083769633.\n",
            "Iteration: 65200. Loss: nan.Correct:199. total:382. Accuracy: 52.09424083769633.\n",
            "Iteration: 65300. Loss: nan.Correct:199. total:382. Accuracy: 52.09424083769633.\n",
            "Iteration: 65400. Loss: nan.Correct:199. total:382. Accuracy: 52.09424083769633.\n",
            "Iteration: 65500. Loss: nan.Correct:199. total:382. Accuracy: 52.09424083769633.\n",
            "Iteration: 65600. Loss: nan.Correct:199. total:382. Accuracy: 52.09424083769633.\n",
            "Iteration: 65700. Loss: nan.Correct:199. total:382. Accuracy: 52.09424083769633.\n",
            "Iteration: 65800. Loss: nan.Correct:199. total:382. Accuracy: 52.09424083769633.\n",
            "Iteration: 65900. Loss: nan.Correct:199. total:382. Accuracy: 52.09424083769633.\n",
            "Iteration: 66000. Loss: nan.Correct:199. total:382. Accuracy: 52.09424083769633.\n",
            "Iteration: 66100. Loss: nan.Correct:199. total:382. Accuracy: 52.09424083769633.\n",
            "Iteration: 66200. Loss: nan.Correct:199. total:382. Accuracy: 52.09424083769633.\n",
            "Iteration: 66300. Loss: nan.Correct:199. total:382. Accuracy: 52.09424083769633.\n",
            "Iteration: 66400. Loss: nan.Correct:199. total:382. Accuracy: 52.09424083769633.\n",
            "Iteration: 66500. Loss: nan.Correct:199. total:382. Accuracy: 52.09424083769633.\n",
            "Iteration: 66600. Loss: nan.Correct:199. total:382. Accuracy: 52.09424083769633.\n",
            "Iteration: 66700. Loss: nan.Correct:199. total:382. Accuracy: 52.09424083769633.\n",
            "Iteration: 66800. Loss: nan.Correct:199. total:382. Accuracy: 52.09424083769633.\n",
            "Iteration: 66900. Loss: nan.Correct:199. total:382. Accuracy: 52.09424083769633.\n",
            "Iteration: 67000. Loss: nan.Correct:200. total:382. Accuracy: 52.35602094240838.\n",
            "Iteration: 67100. Loss: nan.Correct:199. total:382. Accuracy: 52.09424083769633.\n",
            "Iteration: 67200. Loss: nan.Correct:199. total:382. Accuracy: 52.09424083769633.\n",
            "Iteration: 67300. Loss: nan.Correct:199. total:382. Accuracy: 52.09424083769633.\n",
            "Iteration: 67400. Loss: nan.Correct:199. total:382. Accuracy: 52.09424083769633.\n",
            "Iteration: 67500. Loss: nan.Correct:200. total:382. Accuracy: 52.35602094240838.\n",
            "Iteration: 67600. Loss: nan.Correct:200. total:382. Accuracy: 52.35602094240838.\n",
            "Iteration: 67700. Loss: nan.Correct:201. total:382. Accuracy: 52.617801047120416.\n",
            "Iteration: 67800. Loss: nan.Correct:201. total:382. Accuracy: 52.617801047120416.\n",
            "Iteration: 67900. Loss: nan.Correct:201. total:382. Accuracy: 52.617801047120416.\n",
            "Iteration: 68000. Loss: nan.Correct:201. total:382. Accuracy: 52.617801047120416.\n",
            "Iteration: 68100. Loss: nan.Correct:201. total:382. Accuracy: 52.617801047120416.\n",
            "Iteration: 68200. Loss: nan.Correct:202. total:382. Accuracy: 52.87958115183246.\n",
            "Iteration: 68300. Loss: nan.Correct:202. total:382. Accuracy: 52.87958115183246.\n",
            "Iteration: 68400. Loss: nan.Correct:203. total:382. Accuracy: 53.1413612565445.\n",
            "Iteration: 68500. Loss: nan.Correct:203. total:382. Accuracy: 53.1413612565445.\n",
            "Iteration: 68600. Loss: nan.Correct:203. total:382. Accuracy: 53.1413612565445.\n",
            "Iteration: 68700. Loss: nan.Correct:203. total:382. Accuracy: 53.1413612565445.\n",
            "Iteration: 68800. Loss: nan.Correct:203. total:382. Accuracy: 53.1413612565445.\n",
            "Iteration: 68900. Loss: nan.Correct:203. total:382. Accuracy: 53.1413612565445.\n",
            "Iteration: 69000. Loss: nan.Correct:203. total:382. Accuracy: 53.1413612565445.\n",
            "Iteration: 69100. Loss: nan.Correct:203. total:382. Accuracy: 53.1413612565445.\n",
            "Iteration: 69200. Loss: nan.Correct:203. total:382. Accuracy: 53.1413612565445.\n",
            "Iteration: 69300. Loss: nan.Correct:203. total:382. Accuracy: 53.1413612565445.\n",
            "Iteration: 69400. Loss: nan.Correct:203. total:382. Accuracy: 53.1413612565445.\n",
            "Iteration: 69500. Loss: nan.Correct:203. total:382. Accuracy: 53.1413612565445.\n",
            "Iteration: 69600. Loss: nan.Correct:203. total:382. Accuracy: 53.1413612565445.\n",
            "Iteration: 69700. Loss: nan.Correct:205. total:382. Accuracy: 53.66492146596859.\n",
            "Iteration: 69800. Loss: nan.Correct:205. total:382. Accuracy: 53.66492146596859.\n",
            "Iteration: 69900. Loss: nan.Correct:205. total:382. Accuracy: 53.66492146596859.\n",
            "Iteration: 70000. Loss: nan.Correct:205. total:382. Accuracy: 53.66492146596859.\n",
            "Iteration: 70100. Loss: nan.Correct:205. total:382. Accuracy: 53.66492146596859.\n",
            "Iteration: 70200. Loss: nan.Correct:205. total:382. Accuracy: 53.66492146596859.\n",
            "Iteration: 70300. Loss: nan.Correct:205. total:382. Accuracy: 53.66492146596859.\n",
            "Iteration: 70400. Loss: nan.Correct:205. total:382. Accuracy: 53.66492146596859.\n",
            "Iteration: 70500. Loss: nan.Correct:205. total:382. Accuracy: 53.66492146596859.\n",
            "Iteration: 70600. Loss: nan.Correct:205. total:382. Accuracy: 53.66492146596859.\n",
            "Iteration: 70700. Loss: nan.Correct:205. total:382. Accuracy: 53.66492146596859.\n",
            "Iteration: 70800. Loss: nan.Correct:205. total:382. Accuracy: 53.66492146596859.\n",
            "Iteration: 70900. Loss: nan.Correct:205. total:382. Accuracy: 53.66492146596859.\n",
            "Iteration: 71000. Loss: nan.Correct:205. total:382. Accuracy: 53.66492146596859.\n",
            "Iteration: 71100. Loss: nan.Correct:205. total:382. Accuracy: 53.66492146596859.\n",
            "Iteration: 71200. Loss: nan.Correct:205. total:382. Accuracy: 53.66492146596859.\n",
            "Iteration: 71300. Loss: nan.Correct:205. total:382. Accuracy: 53.66492146596859.\n",
            "Iteration: 71400. Loss: nan.Correct:205. total:382. Accuracy: 53.66492146596859.\n",
            "Iteration: 71500. Loss: nan.Correct:205. total:382. Accuracy: 53.66492146596859.\n",
            "Iteration: 71600. Loss: nan.Correct:205. total:382. Accuracy: 53.66492146596859.\n",
            "Iteration: 71700. Loss: nan.Correct:205. total:382. Accuracy: 53.66492146596859.\n",
            "Iteration: 71800. Loss: nan.Correct:205. total:382. Accuracy: 53.66492146596859.\n",
            "Iteration: 71900. Loss: nan.Correct:205. total:382. Accuracy: 53.66492146596859.\n",
            "Iteration: 72000. Loss: nan.Correct:205. total:382. Accuracy: 53.66492146596859.\n",
            "Iteration: 72100. Loss: nan.Correct:205. total:382. Accuracy: 53.66492146596859.\n",
            "Iteration: 72200. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 72300. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 72400. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 72500. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 72600. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 72700. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 72800. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 72900. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 73000. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 73100. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 73200. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 73300. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 73400. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 73500. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 73600. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 73700. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 73800. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 73900. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 74000. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 74100. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 74200. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 74300. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 74400. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 74500. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 74600. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 74700. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 74800. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 74900. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 75000. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 75100. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 75200. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 75300. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 75400. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 75500. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 75600. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 75700. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 75800. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 75900. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 76000. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 76100. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 76200. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 76300. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 76400. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 76500. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 76600. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 76700. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 76800. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 76900. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 77000. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 77100. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 77200. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 77300. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 77400. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 77500. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 77600. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 77700. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 77800. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 77900. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 78000. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 78100. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 78200. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 78300. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 78400. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 78500. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 78600. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 78700. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 78800. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 78900. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 79000. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 79100. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 79200. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 79300. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 79400. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 79500. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 79600. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 79700. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 79800. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 79900. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 80000. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 80100. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 80200. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 80300. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 80400. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 80500. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 80600. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 80700. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 80800. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 80900. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 81000. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 81100. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 81200. Loss: nan.Correct:206. total:382. Accuracy: 53.92670157068063.\n",
            "Iteration: 81300. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 81400. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 81500. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 81600. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 81700. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 81800. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 81900. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 82000. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 82100. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 82200. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 82300. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 82400. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 82500. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 82600. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 82700. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 82800. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 82900. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 83000. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 83100. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 83200. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 83300. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 83400. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 83500. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 83600. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 83700. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 83800. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 83900. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 84000. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 84100. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 84200. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 84300. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 84400. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 84500. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 84600. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 84700. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 84800. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 84900. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 85000. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 85100. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 85200. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 85300. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 85400. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 85500. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 85600. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 85700. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 85800. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 85900. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 86000. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 86100. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 86200. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 86300. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 86400. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 86500. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 86600. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 86700. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 86800. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 86900. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 87000. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 87100. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 87200. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 87300. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 87400. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 87500. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 87600. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 87700. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 87800. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 87900. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 88000. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 88100. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 88200. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 88300. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 88400. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 88500. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 88600. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 88700. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 88800. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 88900. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 89000. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 89100. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 89200. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 89300. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 89400. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 89500. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 89600. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 89700. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 89800. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 89900. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 90000. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 90100. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 90200. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 90300. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 90400. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 90500. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 90600. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 90700. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 90800. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 90900. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 91000. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 91100. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 91200. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 91300. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 91400. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 91500. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 91600. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 91700. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 91800. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 91900. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 92000. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 92100. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 92200. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 92300. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 92400. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 92500. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 92600. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 92700. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 92800. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 92900. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 93000. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 93100. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 93200. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 93300. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 93400. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 93500. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 93600. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 93700. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 93800. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 93900. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 94000. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 94100. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 94200. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 94300. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 94400. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 94500. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 94600. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 94700. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 94800. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 94900. Loss: nan.Correct:207. total:382. Accuracy: 54.18848167539267.\n",
            "Iteration: 95000. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 95100. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 95200. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 95300. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 95400. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 95500. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 95600. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 95700. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 95800. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 95900. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 96000. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 96100. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 96200. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 96300. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 96400. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 96500. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 96600. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 96700. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 96800. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 96900. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 97000. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 97100. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 97200. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 97300. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 97400. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 97500. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 97600. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 97700. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 97800. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 97900. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 98000. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 98100. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 98200. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 98300. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 98400. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 98500. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 98600. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 98700. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 98800. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 98900. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 99000. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 99100. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 99200. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 99300. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 99400. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 99500. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 99600. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 99700. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 99800. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 99900. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 100000. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 100100. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 100200. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 100300. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 100400. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 100500. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 100600. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 100700. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 100800. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 100900. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 101000. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 101100. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 101200. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 101300. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 101400. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 101500. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 101600. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 101700. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 101800. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 101900. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 102000. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 102100. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 102200. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 102300. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 102400. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 102500. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 102600. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 102700. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 102800. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 102900. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 103000. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 103100. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 103200. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 103300. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 103400. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 103500. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 103600. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 103700. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 103800. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 103900. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 104000. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 104100. Loss: nan.Correct:208. total:382. Accuracy: 54.45026178010471.\n",
            "Iteration: 104200. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 104300. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 104400. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 104500. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 104600. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 104700. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 104800. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 104900. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 105000. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 105100. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 105200. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 105300. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 105400. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 105500. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 105600. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 105700. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 105800. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 105900. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 106000. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 106100. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 106200. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 106300. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 106400. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 106500. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 106600. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 106700. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 106800. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 106900. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 107000. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 107100. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 107200. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 107300. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 107400. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 107500. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 107600. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 107700. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 107800. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 107900. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 108000. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 108100. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 108200. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 108300. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 108400. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 108500. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 108600. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 108700. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 108800. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 108900. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 109000. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 109100. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 109200. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 109300. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 109400. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 109500. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 109600. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 109700. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 109800. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 109900. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 110000. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 110100. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 110200. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 110300. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 110400. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 110500. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 110600. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 110700. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 110800. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 110900. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 111000. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 111100. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 111200. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 111300. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 111400. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 111500. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 111600. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 111700. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 111800. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 111900. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 112000. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 112100. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 112200. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 112300. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 112400. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 112500. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 112600. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 112700. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 112800. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 112900. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 113000. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 113100. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 113200. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 113300. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 113400. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 113500. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 113600. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 113700. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 113800. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 113900. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 114000. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 114100. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 114200. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 114300. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 114400. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 114500. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 114600. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 114700. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 114800. Loss: nan.Correct:209. total:382. Accuracy: 54.712041884816756.\n",
            "Iteration: 114900. Loss: nan.Correct:210. total:382. Accuracy: 54.973821989528794.\n",
            "Iteration: 115000. Loss: nan.Correct:210. total:382. Accuracy: 54.973821989528794.\n",
            "Iteration: 115100. Loss: nan.Correct:210. total:382. Accuracy: 54.973821989528794.\n",
            "Iteration: 115200. Loss: nan.Correct:210. total:382. Accuracy: 54.973821989528794.\n",
            "Iteration: 115300. Loss: nan.Correct:210. total:382. Accuracy: 54.973821989528794.\n",
            "Iteration: 115400. Loss: nan.Correct:210. total:382. Accuracy: 54.973821989528794.\n",
            "Iteration: 115500. Loss: nan.Correct:210. total:382. Accuracy: 54.973821989528794.\n",
            "Iteration: 115600. Loss: nan.Correct:210. total:382. Accuracy: 54.973821989528794.\n",
            "Iteration: 115700. Loss: nan.Correct:210. total:382. Accuracy: 54.973821989528794.\n",
            "Iteration: 115800. Loss: nan.Correct:210. total:382. Accuracy: 54.973821989528794.\n",
            "Iteration: 115900. Loss: nan.Correct:210. total:382. Accuracy: 54.973821989528794.\n",
            "Iteration: 116000. Loss: nan.Correct:210. total:382. Accuracy: 54.973821989528794.\n",
            "Iteration: 116100. Loss: nan.Correct:210. total:382. Accuracy: 54.973821989528794.\n",
            "Iteration: 116200. Loss: nan.Correct:210. total:382. Accuracy: 54.973821989528794.\n",
            "Iteration: 116300. Loss: nan.Correct:210. total:382. Accuracy: 54.973821989528794.\n",
            "Iteration: 116400. Loss: nan.Correct:211. total:382. Accuracy: 55.23560209424084.\n",
            "Iteration: 116500. Loss: nan.Correct:210. total:382. Accuracy: 54.973821989528794.\n",
            "Iteration: 116600. Loss: nan.Correct:210. total:382. Accuracy: 54.973821989528794.\n",
            "Iteration: 116700. Loss: nan.Correct:210. total:382. Accuracy: 54.973821989528794.\n",
            "Iteration: 116800. Loss: nan.Correct:210. total:382. Accuracy: 54.973821989528794.\n",
            "Iteration: 116900. Loss: nan.Correct:211. total:382. Accuracy: 55.23560209424084.\n",
            "Iteration: 117000. Loss: nan.Correct:211. total:382. Accuracy: 55.23560209424084.\n",
            "Iteration: 117100. Loss: nan.Correct:211. total:382. Accuracy: 55.23560209424084.\n",
            "Iteration: 117200. Loss: nan.Correct:211. total:382. Accuracy: 55.23560209424084.\n",
            "Iteration: 117300. Loss: nan.Correct:211. total:382. Accuracy: 55.23560209424084.\n",
            "Iteration: 117400. Loss: nan.Correct:211. total:382. Accuracy: 55.23560209424084.\n",
            "Iteration: 117500. Loss: nan.Correct:211. total:382. Accuracy: 55.23560209424084.\n",
            "Iteration: 117600. Loss: nan.Correct:210. total:382. Accuracy: 54.973821989528794.\n",
            "Iteration: 117700. Loss: nan.Correct:210. total:382. Accuracy: 54.973821989528794.\n",
            "Iteration: 117800. Loss: nan.Correct:211. total:382. Accuracy: 55.23560209424084.\n",
            "Iteration: 117900. Loss: nan.Correct:211. total:382. Accuracy: 55.23560209424084.\n",
            "Iteration: 118000. Loss: nan.Correct:211. total:382. Accuracy: 55.23560209424084.\n",
            "Iteration: 118100. Loss: nan.Correct:211. total:382. Accuracy: 55.23560209424084.\n",
            "Iteration: 118200. Loss: nan.Correct:211. total:382. Accuracy: 55.23560209424084.\n",
            "Iteration: 118300. Loss: nan.Correct:211. total:382. Accuracy: 55.23560209424084.\n",
            "Iteration: 118400. Loss: nan.Correct:211. total:382. Accuracy: 55.23560209424084.\n",
            "Iteration: 118500. Loss: nan.Correct:211. total:382. Accuracy: 55.23560209424084.\n",
            "Iteration: 118600. Loss: nan.Correct:211. total:382. Accuracy: 55.23560209424084.\n",
            "Iteration: 118700. Loss: nan.Correct:211. total:382. Accuracy: 55.23560209424084.\n",
            "Iteration: 118800. Loss: nan.Correct:211. total:382. Accuracy: 55.23560209424084.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}